{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started tutorial In this tutorial, you: Create your first database Import data to Firebolt Configure an aggregating index {% hint style=\"info\" %} In order to perform this tutorial, you need to have an active Firebolt account.\\ Contact us via hello@firebolt.io in order to create one and see Firebolt in action. {% endhint %} Create your first database To start working with your data, you will need to create a new database and engine. {% hint style=\"info\" %} Note A Firebolt engine represents the compute resources that are attached to a database for a certain workload. You can choose different engine types based on the workflow you plan on managing. Engines can be scaled up and down even after you\u2019ve set up the initial configuration. {% endhint %} Step 1: Create a new database and engine From the Databases page, click + New DataBase . Name your database Tutorial . Select us-east-1 as the Database region. Firebolt provides you by default with a general purpose engine optimized for data ingestion and analytic queries. You can edit the specifications of this engine. For this tutorial, we keep the provided engine as it is. Click on Create Database to create both the database and engine. Step 2: Start the engine From the Database page, locate the Tutorial_general_purpose engine next to your Tutorial database, and click on Start to start the engine. Once your engine is ready, the status changes to On . Step 3: Query the database Go to the SQL Workspace page. You will be asked to choose a database: Choose the Tutorial database from the list. The Tutorial_general_purpose engine will be used to run the queries on your database. You can always see the engine being used to run your workload in the SQL workspace: Click Run Script in order to run the following SQL command: SHOW DATABASES; Firebolt returns a list of databases. Import data into Firebolt To work with Firebolt, you need to connect to your data sources and ingest that data.\\ Follow these steps to ingest your data into Firebolt: Step 1: Create an external table You will now connect to a public S3 bucket (data source) that contains your parquet files. As part of this tutorial, you will use the Firebolt demo bucket, which contains tables from the TPC-H benchmark. First, you will create an external table. An external table is a virtual table that directly connects to an external data source from your data lake, such as an S3 bucket, without having to load the data into a Firebolt table. Connecting to an external source to view this table is the first building block in the data load ( ingestion ) process. Once you connect to an external table from Firebolt and create fact and dimension tables , you can then copy the external table data to those fact and dimension tables within the database in order to work with the data. To do this, copy and paste the following command to the script editor in the SQL Workspace page (make sure the Tutorial database is selected and you run your workload on the Tutorial_general_purpose engine): CREATE EXTERNAL TABLE IF NOT EXISTS ex_lineitem ( l_orderkey LONG, l_partkey LONG, l_suppkey LONG, l_linenumber INT, l_quantity LONG, l_extendedprice LONG, l_discount LONG, l_tax LONG, l_returnflag TEXT, l_linestatus TEXT, l_shipdate TEXT, l_commitdate TEXT, l_receiptdate TEXT, l_shipinstruct TEXT, l_shipmode TEXT, l_comment TEXT ) URL = 's3://firebolt-publishing-public/samples/tpc-h/parquet/lineitem/' -- CREDENTIALS = ( AWS_KEY_ID = '******' AWS_SECRET_KEY = '******' ) OBJECT_PATTERN = '*.parquet' TYPE = (PARQUET); {% hint style=\"info\" %} Note The Firebolt demo S3 bucket is configured to be accessed publicly, so we do not need to use the CREDENTIALS parameter. When you are accessing private data, you will need to use the relevant credentials. {% endhint %} The external table ex_lineitem appears on the object panel of the database. Step 2: Import data into Firebolt In this step, you will create a Firebolt fact table called lineitem and then load it with data ( data ingestion ) in order to work with the data from the ex_lineitem external table . {% hint style=\"info\" %} Notice In Firebolt, data is stored in either fact or dimension tables. Therefore, to make imported data available for querying, one of these tables must first be created. {% endhint %} For the fact table, as part of the CREATE script, we will declare ( l_orderkey, l_linenumber ) as its primary index . This tells Firebolt that the data should be sorted and indexed according to the ( l_orderkey, l_linenumber ) combination of fields. This optimizes range queries with the l_orderkey, l_linenumber columns . Primary indexes are mandatory for fact tables. Read our guide on tables for more help on creating primary indexes. To do this, run the following command: CREATE FACT TABLE IF NOT EXISTS lineitem ( l_orderkey LONG, l_partkey LONG, l_suppkey LONG, l_linenumber INT, l_quantity LONG, l_extendedprice LONG, l_discount LONG, l_tax LONG, l_returnflag TEXT, l_linestatus TEXT, l_shipdate TEXT, l_commitdate TEXT, l_receiptdate TEXT, l_shipinstruct TEXT, l_shipmode TEXT, l_comment TEXT ) PRIMARY INDEX l_orderkey, l_linenumber; When successful, the table appears in the object panel of the database, similar to the following: You can now use the INSERT INTO command to copy the data from the external table into the fact table as follows: INSERT INTO lineitem SELECT * FROM ex_lineitem; Following is how it should look when you run the query: When the import is completed, the Status column changes to success , as follows: Now, query the data in the lineitem table to verify that the fact table was created successfully: SELECT * FROM lineitem LIMIT 1000; Following is how it should look when you run the query: Configure an aggregating index For this portion of the tutorial, you will create an aggregating index . {% hint style=\"info\" %} Note The aggregating index enables you to take a subset of a table's columns and configure aggregations on top of those columns. Many aggregations are supported from the simple sum, max, min to more complex ones such as count and count (distinct). The index is automatically updated and aggregating as new data streams into the table without having to scan the entire table every time since the index is stateful and consistent. The index is configured per table so when the table is queried, Firebolt's query optimizer searches the table's indexes for the index (or indexes) which has the potential for providing the most optimized query response time. When using the index - Instead of calculating the aggregation on the entire table and scanning all the rows, the aggregation is already pre-calculated in the aggregating index. No need to scan the entire table to perform the calculation. {% endhint %} From the lineitem fact table you created in the previous step, let's assume you typically run queries to look at the SUM(l_quantity) , SUM(l_extendedprice) , and AVG(l_discount) , grouped by different combinations of l_suppkey and l_partkey . To help you speed up your queries on this database, you will create a relevant aggregating index. Copy the following script: sql CREATE AGGREGATING INDEX agg_lineitem ON lineitem ( l_suppkey, l_partkey, SUM(l_quantity), SUM(l_extendedprice), AVG(l_discount) ); 2. Paste it to the script editor in the SQL Workspace page (make sure the Tutorial database is selected and you run your workload on the Tutorial_general_purpose engine). When successful, a confirmation message appears below the editor. 3. Now populate the index with values by running the GENERATE command. Use the following script: GENERATE AGGREGATING INDEX agg_lineitem; When successful, a confirmation message appears below the editor. From now on, every query for data from the lineitem table that combines any of these fields and aggregations will now use the index you just created instead of the full table in order to run the query and retrieve a response. Read more about aggregating indexes here . Congratulations! You can now use Firebolt to run queries on the demo database!","title":"Getting started"},{"location":"#getting-started-tutorial","text":"In this tutorial, you: Create your first database Import data to Firebolt Configure an aggregating index {% hint style=\"info\" %} In order to perform this tutorial, you need to have an active Firebolt account.\\ Contact us via hello@firebolt.io in order to create one and see Firebolt in action. {% endhint %}","title":"Getting started tutorial"},{"location":"#create-your-first-database","text":"To start working with your data, you will need to create a new database and engine. {% hint style=\"info\" %} Note A Firebolt engine represents the compute resources that are attached to a database for a certain workload. You can choose different engine types based on the workflow you plan on managing. Engines can be scaled up and down even after you\u2019ve set up the initial configuration. {% endhint %}","title":"Create your first database"},{"location":"#step-1-create-a-new-database-and-engine","text":"From the Databases page, click + New DataBase . Name your database Tutorial . Select us-east-1 as the Database region. Firebolt provides you by default with a general purpose engine optimized for data ingestion and analytic queries. You can edit the specifications of this engine. For this tutorial, we keep the provided engine as it is. Click on Create Database to create both the database and engine.","title":"Step 1: Create a new database and engine"},{"location":"#step-2-start-the-engine","text":"From the Database page, locate the Tutorial_general_purpose engine next to your Tutorial database, and click on Start to start the engine. Once your engine is ready, the status changes to On .","title":"Step 2: Start the engine"},{"location":"#step-3-query-the-database","text":"Go to the SQL Workspace page. You will be asked to choose a database: Choose the Tutorial database from the list. The Tutorial_general_purpose engine will be used to run the queries on your database. You can always see the engine being used to run your workload in the SQL workspace: Click Run Script in order to run the following SQL command: SHOW DATABASES; Firebolt returns a list of databases.","title":"Step 3: Query the database"},{"location":"#import-data-into-firebolt","text":"To work with Firebolt, you need to connect to your data sources and ingest that data.\\ Follow these steps to ingest your data into Firebolt:","title":"Import data into Firebolt"},{"location":"#step-1-create-an-external-table","text":"You will now connect to a public S3 bucket (data source) that contains your parquet files. As part of this tutorial, you will use the Firebolt demo bucket, which contains tables from the TPC-H benchmark. First, you will create an external table. An external table is a virtual table that directly connects to an external data source from your data lake, such as an S3 bucket, without having to load the data into a Firebolt table. Connecting to an external source to view this table is the first building block in the data load ( ingestion ) process. Once you connect to an external table from Firebolt and create fact and dimension tables , you can then copy the external table data to those fact and dimension tables within the database in order to work with the data. To do this, copy and paste the following command to the script editor in the SQL Workspace page (make sure the Tutorial database is selected and you run your workload on the Tutorial_general_purpose engine): CREATE EXTERNAL TABLE IF NOT EXISTS ex_lineitem ( l_orderkey LONG, l_partkey LONG, l_suppkey LONG, l_linenumber INT, l_quantity LONG, l_extendedprice LONG, l_discount LONG, l_tax LONG, l_returnflag TEXT, l_linestatus TEXT, l_shipdate TEXT, l_commitdate TEXT, l_receiptdate TEXT, l_shipinstruct TEXT, l_shipmode TEXT, l_comment TEXT ) URL = 's3://firebolt-publishing-public/samples/tpc-h/parquet/lineitem/' -- CREDENTIALS = ( AWS_KEY_ID = '******' AWS_SECRET_KEY = '******' ) OBJECT_PATTERN = '*.parquet' TYPE = (PARQUET); {% hint style=\"info\" %} Note The Firebolt demo S3 bucket is configured to be accessed publicly, so we do not need to use the CREDENTIALS parameter. When you are accessing private data, you will need to use the relevant credentials. {% endhint %} The external table ex_lineitem appears on the object panel of the database.","title":"Step 1: Create an external table"},{"location":"#step-2-import-data-into-firebolt","text":"In this step, you will create a Firebolt fact table called lineitem and then load it with data ( data ingestion ) in order to work with the data from the ex_lineitem external table . {% hint style=\"info\" %} Notice In Firebolt, data is stored in either fact or dimension tables. Therefore, to make imported data available for querying, one of these tables must first be created. {% endhint %} For the fact table, as part of the CREATE script, we will declare ( l_orderkey, l_linenumber ) as its primary index . This tells Firebolt that the data should be sorted and indexed according to the ( l_orderkey, l_linenumber ) combination of fields. This optimizes range queries with the l_orderkey, l_linenumber columns . Primary indexes are mandatory for fact tables. Read our guide on tables for more help on creating primary indexes. To do this, run the following command: CREATE FACT TABLE IF NOT EXISTS lineitem ( l_orderkey LONG, l_partkey LONG, l_suppkey LONG, l_linenumber INT, l_quantity LONG, l_extendedprice LONG, l_discount LONG, l_tax LONG, l_returnflag TEXT, l_linestatus TEXT, l_shipdate TEXT, l_commitdate TEXT, l_receiptdate TEXT, l_shipinstruct TEXT, l_shipmode TEXT, l_comment TEXT ) PRIMARY INDEX l_orderkey, l_linenumber; When successful, the table appears in the object panel of the database, similar to the following: You can now use the INSERT INTO command to copy the data from the external table into the fact table as follows: INSERT INTO lineitem SELECT * FROM ex_lineitem; Following is how it should look when you run the query: When the import is completed, the Status column changes to success , as follows: Now, query the data in the lineitem table to verify that the fact table was created successfully: SELECT * FROM lineitem LIMIT 1000; Following is how it should look when you run the query:","title":"Step 2: Import data into Firebolt"},{"location":"#configure-an-aggregating-index","text":"For this portion of the tutorial, you will create an aggregating index . {% hint style=\"info\" %}","title":"Configure an aggregating index"},{"location":"#note","text":"The aggregating index enables you to take a subset of a table's columns and configure aggregations on top of those columns. Many aggregations are supported from the simple sum, max, min to more complex ones such as count and count (distinct). The index is automatically updated and aggregating as new data streams into the table without having to scan the entire table every time since the index is stateful and consistent. The index is configured per table so when the table is queried, Firebolt's query optimizer searches the table's indexes for the index (or indexes) which has the potential for providing the most optimized query response time. When using the index - Instead of calculating the aggregation on the entire table and scanning all the rows, the aggregation is already pre-calculated in the aggregating index. No need to scan the entire table to perform the calculation. {% endhint %} From the lineitem fact table you created in the previous step, let's assume you typically run queries to look at the SUM(l_quantity) , SUM(l_extendedprice) , and AVG(l_discount) , grouped by different combinations of l_suppkey and l_partkey . To help you speed up your queries on this database, you will create a relevant aggregating index. Copy the following script: sql CREATE AGGREGATING INDEX agg_lineitem ON lineitem ( l_suppkey, l_partkey, SUM(l_quantity), SUM(l_extendedprice), AVG(l_discount) ); 2. Paste it to the script editor in the SQL Workspace page (make sure the Tutorial database is selected and you run your workload on the Tutorial_general_purpose engine). When successful, a confirmation message appears below the editor. 3. Now populate the index with values by running the GENERATE command. Use the following script: GENERATE AGGREGATING INDEX agg_lineitem; When successful, a confirmation message appears below the editor. From now on, every query for data from the lineitem table that combines any of these fields and aggregations will now use the index you just created instead of the full table in order to run the query and retrieve a response. Read more about aggregating indexes here . Congratulations! You can now use Firebolt to run queries on the demo database!","title":"Note"},{"location":"architecture-overview/","text":"Architecture overview The diagram below depicts Firebolt\u2019s high-level architecture, which comprises a services layer and decoupled storage and compute layers. This topic describes each layer. Services Layer The services layer is multi-tenant. It accepts all incoming requests to Firebolt. Its most important functions are: Administration - Handles account information, user management, and permissions. Metadata - Contains all metadata of databases, engines, tables, indexes, etc. Security - Handles authentication. Isolated Tenancy Unlike the multi-tenant services layer, the compute and storage layers in Firebolt run on isolated tenants. A dedicated and isolated AWS sub-account is created for each Firebolt customer, within which Firebolt manages the storage and compute layers. Each tenant runs within Firebolt\u2019s master account and outside their own VPC. This ensures complete cross-customer isolation for data and query execution. Compute Layer The compute layer runs Firebolt engines . Engines are compute clusters that run database workloads. Each engine is an isolated cluster. Within each cluster, engine instances store data and indexes in the local solid state drive (SSD), which acts as the local cache. The engine loads data from the storage layer into SSD at query runtime based on the query configuration. A benefit of the decoupled storage and compute architecture is that multiple engines can be assigned to the same database. This allows for granular control over which hardware is assigned to which tasks. Each engine can have a different configuration and size depending on the workloads. Engines can work in parallel or separately, and you can share them with different people in your organization. Storage Layer The storage layer within Firebolt runs on Amazon S3. After you ingest data into Firebolt, this is where the data and indexes associated with a database are saved. When you ingest data, you use a Firebolt general purpose engine, which stores the data in the proprietary Firebolt File Format (F3). The data is sorted, compressed, and indexed to support highly efficient pruning for query acceleration. F3 works together with other proprietary Firebolt technologies to deliver exceptional performance at query runtime.","title":"Architecture overview"},{"location":"architecture-overview/#architecture-overview","text":"The diagram below depicts Firebolt\u2019s high-level architecture, which comprises a services layer and decoupled storage and compute layers. This topic describes each layer.","title":"Architecture overview"},{"location":"architecture-overview/#services-layer","text":"The services layer is multi-tenant. It accepts all incoming requests to Firebolt. Its most important functions are: Administration - Handles account information, user management, and permissions. Metadata - Contains all metadata of databases, engines, tables, indexes, etc. Security - Handles authentication.","title":"Services Layer"},{"location":"architecture-overview/#isolated-tenancy","text":"Unlike the multi-tenant services layer, the compute and storage layers in Firebolt run on isolated tenants. A dedicated and isolated AWS sub-account is created for each Firebolt customer, within which Firebolt manages the storage and compute layers. Each tenant runs within Firebolt\u2019s master account and outside their own VPC. This ensures complete cross-customer isolation for data and query execution.","title":"Isolated Tenancy"},{"location":"architecture-overview/#compute-layer","text":"The compute layer runs Firebolt engines . Engines are compute clusters that run database workloads. Each engine is an isolated cluster. Within each cluster, engine instances store data and indexes in the local solid state drive (SSD), which acts as the local cache. The engine loads data from the storage layer into SSD at query runtime based on the query configuration. A benefit of the decoupled storage and compute architecture is that multiple engines can be assigned to the same database. This allows for granular control over which hardware is assigned to which tasks. Each engine can have a different configuration and size depending on the workloads. Engines can work in parallel or separately, and you can share them with different people in your organization.","title":"Compute Layer"},{"location":"architecture-overview/#storage-layer","text":"The storage layer within Firebolt runs on Amazon S3. After you ingest data into Firebolt, this is where the data and indexes associated with a database are saved. When you ingest data, you use a Firebolt general purpose engine, which stores the data in the proprietary Firebolt File Format (F3). The data is sorted, compressed, and indexed to support highly efficient pruning for query acceleration. F3 works together with other proprietary Firebolt technologies to deliver exceptional performance at query runtime.","title":"Storage Layer"},{"location":"concepts/","text":"Concepts","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/get-instant-query-response-time/","text":"Using indexes for faster queries Firebolt incorporates several building blocks that enable accelerate query response times. Those building blocks are called indexes. Using them wisely not only guarantees fast query response times but will also reduce down your cloud bill. In this topic: Primary indexes as the first line of performance accelerator Get sub-second query response time using aggregating indexes Accelerate queries using join indexes Primary indexes The primary indexes are the main sort key of the table. They will be built with the table using the CREATE FACT/DIMENSION TABLE syntax and will be used as the main tool for data pruning and data distribution. By using the primary index, Firebolt will read ranges of data from each F3 file. This will reduce the I/O needed to be read, and increase performance dramatically. How to choose your Primary index The columns we set in the primary index and their order is a key factor for the index success. Make sure to put the main queries filter columns, in the order that filter the most. In partitioned tables, the partition key column will be the first line of data distribution. The primary index will be used in each partition as the second level of sort/distribution. If you have a use case of a star schema, and you use join indexes on your dimension tables, we recommend adding the dimension join key as the first column in the primary index. Get sub-second query response time using aggregating indexes Firebolt incorporates many building blocks to guarantee fast query response times. One of these building blocks is a type of index called an aggregating index . The aggregating index enables you to take a subset of a table's columns and configure aggregations on top of those columns. Many aggregations are supported from the simple sum , max , min to more complex ones such as count and count (distinct) . The index is automatically updated and aggregating as new data streams into the table without having to scan the entire table every time since the index is stateful and consistent. The index is configured per table so when the table is queried, Firebolt's query optimizer searches the table's indexes for the index (or indexes) which has the potential for providing the most optimized query response time. When using the index - Instead of calculating the aggregation on the entire table and scanning all the rows, the aggregation is already pre-calculated in the aggregating index. No need to scan the entire table to perform the calculation. When to use aggregating indexes You should consider using aggregating indexes as a best practice in Firebolt to speed up dashboards, reports, or any type of workload that is highly repetitive in your queries and contains aggregations. Not only does it speed up queries dramatically, but it also helps reduce compute costs because the CPU doesn\u2019t need to work as hard to scan as much data. This means that once indexes are set up, you can often reduce your cluster size (and thus pay less on compute), while still enjoying fast query performance. How many aggregating indexes can I create? You can create as many aggregating indexes as you like. Since the indexes are compressed and relatively small compared to the table they are configured on - you have nothing to worry about increasing your cloud bill. In fact, when using aggregating indexes your cloud bill will drop significantly since you will probably be able to reduce your cluster size and maybe also use an engine with a cheaper spec. Prerequisites To configure an aggregating index, first, you need to create a Fact table . Create and generate an aggregating index Use the CREATE & GENERATE AGGERGATING INDEX syntax to create and generate an aggregating index that includes the keys and the functions per parameter that you usually query. Example: Implementing an aggregating index for common queries To create an index, we need to first create a fact table. We'll call ours fact_orders . CREATE FACT TABLE fact_orders ( order_id LONG, product_id LONG, store_id LONG, client_id LONG, order_date DATE, order_total DOUBLE, order_item_count INT ) PRIMARY INDEX store_id, product_id, order_id; From this table, let's assume we typically run queries that calculate the SUM(order_total), SUM(order_item_count) , AVG(order_item_count) , and COUNT(DISTINCT client_id), grouped by different combinations of the store_id and product_id columns. To help us accelerate our queries, we'll create an aggregating index and populate it as follows: CREATE AND GENERATE AGGREGATING INDEX agg_fact_orders ON fact_orders ( store_id, product_id, SUM(order_total), SUM(order_item_count), AVG(order_item_count), COUNT(DISTINCT client_id) ); From now on, every query for data from the fact_orders table that combines any of these fields and aggregations will now benefit from the aggregating index. Instead of performing a full query scan and perform each aggregation - the engine uses the aggregating index which already has the required aggregating pre-calculated providing a sub-second query response time. The order of the fields is important. Firebolt will create the primary index on the aggregating index as an optimization to filter it best. The primary index will be built base on the index fields, in the order as they were written. In our example, the primary index will be in the following order: store_id , product_id . Make sure to write first those you mostly filter by in your query WHERE clause . Accelerate joins using join indexes Firebolt supports accelerating your joins by creating join indexes. Queries with joins might be resource-consuming and if not done efficiently - joins can take a significant amount of time to complete which makes them unusable to the user. Using Firebolt\u2019s join indexing saves time searching data in the disk and loading it into memory. It\u2019s already there, indexed by the required join key, and waits to be queried. When to use join index You should consider implementing join indexes as best practice in Firebolt to speed up any query which performs a join with a dimension table. This reduces the additional overhead of performing the join to the minimum with the benefit of fast query response times while keeping your cloud bill low. Create join index Configuring the join index is being done using the CREATE JOIN INDEX syntax . Example: Assuming we have the following tables: A fact table my_fact with a column named id and some additional columns. A dimension table my_dimension with a column named id and some additional columns. We want to run the following query: select * from my_fact left join my_dimension_table on (my_fact.id = my_dimension_table.id) Let's create the following join index: CREATE JOIN INDEX IF NOT EXISTS my_join_index ON my_dimension_table ( id ) The query optimizer uses my_join_index instead of performing the join in runtime. This instantly reduces both CPU and memory overhead while achieving super-fast query response time.","title":"Using indexes for faster queries"},{"location":"concepts/get-instant-query-response-time/#using-indexes-for-faster-queries","text":"Firebolt incorporates several building blocks that enable accelerate query response times. Those building blocks are called indexes. Using them wisely not only guarantees fast query response times but will also reduce down your cloud bill. In this topic: Primary indexes as the first line of performance accelerator Get sub-second query response time using aggregating indexes Accelerate queries using join indexes","title":"Using indexes for faster queries"},{"location":"concepts/get-instant-query-response-time/#primary-indexes","text":"The primary indexes are the main sort key of the table. They will be built with the table using the CREATE FACT/DIMENSION TABLE syntax and will be used as the main tool for data pruning and data distribution. By using the primary index, Firebolt will read ranges of data from each F3 file. This will reduce the I/O needed to be read, and increase performance dramatically.","title":"Primary indexes"},{"location":"concepts/get-instant-query-response-time/#how-to-choose-your-primary-index","text":"The columns we set in the primary index and their order is a key factor for the index success. Make sure to put the main queries filter columns, in the order that filter the most. In partitioned tables, the partition key column will be the first line of data distribution. The primary index will be used in each partition as the second level of sort/distribution. If you have a use case of a star schema, and you use join indexes on your dimension tables, we recommend adding the dimension join key as the first column in the primary index.","title":"How to choose your Primary index"},{"location":"concepts/get-instant-query-response-time/#get-sub-second-query-response-time-using-aggregating-indexes","text":"Firebolt incorporates many building blocks to guarantee fast query response times. One of these building blocks is a type of index called an aggregating index . The aggregating index enables you to take a subset of a table's columns and configure aggregations on top of those columns. Many aggregations are supported from the simple sum , max , min to more complex ones such as count and count (distinct) . The index is automatically updated and aggregating as new data streams into the table without having to scan the entire table every time since the index is stateful and consistent. The index is configured per table so when the table is queried, Firebolt's query optimizer searches the table's indexes for the index (or indexes) which has the potential for providing the most optimized query response time. When using the index - Instead of calculating the aggregation on the entire table and scanning all the rows, the aggregation is already pre-calculated in the aggregating index. No need to scan the entire table to perform the calculation.","title":"Get sub-second query response time using aggregating indexes"},{"location":"concepts/get-instant-query-response-time/#when-to-use-aggregating-indexes","text":"You should consider using aggregating indexes as a best practice in Firebolt to speed up dashboards, reports, or any type of workload that is highly repetitive in your queries and contains aggregations. Not only does it speed up queries dramatically, but it also helps reduce compute costs because the CPU doesn\u2019t need to work as hard to scan as much data. This means that once indexes are set up, you can often reduce your cluster size (and thus pay less on compute), while still enjoying fast query performance.","title":"When to use aggregating indexes"},{"location":"concepts/get-instant-query-response-time/#how-many-aggregating-indexes-can-i-create","text":"You can create as many aggregating indexes as you like. Since the indexes are compressed and relatively small compared to the table they are configured on - you have nothing to worry about increasing your cloud bill. In fact, when using aggregating indexes your cloud bill will drop significantly since you will probably be able to reduce your cluster size and maybe also use an engine with a cheaper spec.","title":"How many aggregating indexes can I create?"},{"location":"concepts/get-instant-query-response-time/#prerequisites","text":"To configure an aggregating index, first, you need to create a Fact table .","title":"Prerequisites"},{"location":"concepts/get-instant-query-response-time/#create-and-generate-an-aggregating-index","text":"Use the CREATE & GENERATE AGGERGATING INDEX syntax to create and generate an aggregating index that includes the keys and the functions per parameter that you usually query. Example: Implementing an aggregating index for common queries To create an index, we need to first create a fact table. We'll call ours fact_orders . CREATE FACT TABLE fact_orders ( order_id LONG, product_id LONG, store_id LONG, client_id LONG, order_date DATE, order_total DOUBLE, order_item_count INT ) PRIMARY INDEX store_id, product_id, order_id; From this table, let's assume we typically run queries that calculate the SUM(order_total), SUM(order_item_count) , AVG(order_item_count) , and COUNT(DISTINCT client_id), grouped by different combinations of the store_id and product_id columns. To help us accelerate our queries, we'll create an aggregating index and populate it as follows: CREATE AND GENERATE AGGREGATING INDEX agg_fact_orders ON fact_orders ( store_id, product_id, SUM(order_total), SUM(order_item_count), AVG(order_item_count), COUNT(DISTINCT client_id) ); From now on, every query for data from the fact_orders table that combines any of these fields and aggregations will now benefit from the aggregating index. Instead of performing a full query scan and perform each aggregation - the engine uses the aggregating index which already has the required aggregating pre-calculated providing a sub-second query response time. The order of the fields is important. Firebolt will create the primary index on the aggregating index as an optimization to filter it best. The primary index will be built base on the index fields, in the order as they were written. In our example, the primary index will be in the following order: store_id , product_id . Make sure to write first those you mostly filter by in your query WHERE clause .","title":"Create and generate an aggregating index"},{"location":"concepts/get-instant-query-response-time/#accelerate-joins-using-join-indexes","text":"Firebolt supports accelerating your joins by creating join indexes. Queries with joins might be resource-consuming and if not done efficiently - joins can take a significant amount of time to complete which makes them unusable to the user. Using Firebolt\u2019s join indexing saves time searching data in the disk and loading it into memory. It\u2019s already there, indexed by the required join key, and waits to be queried.","title":"Accelerate joins using join indexes"},{"location":"concepts/get-instant-query-response-time/#when-to-use-join-index","text":"You should consider implementing join indexes as best practice in Firebolt to speed up any query which performs a join with a dimension table. This reduces the additional overhead of performing the join to the minimum with the benefit of fast query response times while keeping your cloud bill low.","title":"When to use join index"},{"location":"concepts/get-instant-query-response-time/#create-join-index","text":"Configuring the join index is being done using the CREATE JOIN INDEX syntax . Example: Assuming we have the following tables: A fact table my_fact with a column named id and some additional columns. A dimension table my_dimension with a column named id and some additional columns. We want to run the following query: select * from my_fact left join my_dimension_table on (my_fact.id = my_dimension_table.id) Let's create the following join index: CREATE JOIN INDEX IF NOT EXISTS my_join_index ON my_dimension_table ( id ) The query optimizer uses my_join_index instead of performing the join in runtime. This instantly reduces both CPU and memory overhead while achieving super-fast query response time.","title":"Create join index"},{"location":"concepts/working-with-partitions/","text":"Working with partitions Partitions are multiple smaller physical parts of large tables created for data maintenance and performance. They are defined as part of the table's DDL and maintained as part of the table's internal structure. You can partition your data by any column in your table. When to use partitions Partitions should be used on large fact tables in one of the following scenarios: Delete or update data: Partitions are useful if you need to perform update or delete operations on large portions of your data. Data can be deleted from a specified partition by using the ALTER TABLE ... DROP PARTITION command. After deleting data from a partition, it can be updated with new data by using an INSERT command. Boost performance: If your fact table is large enough (contains more than 100 million rows), consider using partitions to better prune data at query time. When the partition key appears in the WHERE clause of your queries, it will extend the Primary Index's functionality and reduce the required I/O so that you can benefit from even faster query response times. Note that the partition key column doesn't have to be part of a query's WHERE predicate. In such cases, Firebolt utilizes the Primary Index per partition and reads them all. Considerations and limitations When using partitions, pay attention to the following: Plan carefully - partitions definition takes place during table creation. Check the queries you plan to run on your data - in particular - the WHERE clauses in them. Make sure that either the table primary index or the table partition key covers those to enjoy maximum data pruning. Read more here on how to configure the partition key here . The partition key cannot be composed of nullable columns. Choosing the partition key If you choose to use partitions in order to delete or update data by key, then your partition key should be the column by which you intend to update/delete. For example, if you need to store 1 month of data, set your partition key based on your date column, so you can drop partitions that are older than 1 month. If you choose to use partitions for boosting performance, set your partition key as the main predicate in your WHERE clause, so that it will prune as much data as possible for each query. For example, if your main query's predicate is the product_type column, set the partition key by this column. In both cases, the recommendation is not to use long text columns as your partition key. Partitioning your table is done as part of the CREATE FACT TABLE DDL using the PARTITION BY specifier. Working with partitions Once the table is created with the required partition key, Firebolt arranges the table's data in the relevant partitions. New rows will be stored, each on the relevant partition and queries will prune and read from the relevant partition. New partitions will be created automatically during ingest, based on the partition key. Drop partitions, if requested, should be done manually by the DB admin. Configuring the partition key The partition key can be either a column name or a result of a function applied on a column: PARTITION BY date_column; PARTITION BY product_type; PARTITION BY EXTRACT(MONTH FROM date_column); PARTITION BY EXTRACT(MONTH FROM date_column), product_type; The following functions are supported for defining the partition key: DATE_FORMAT EXTRACT Drop partition In order to delete data from your table, use the ALTER TABLE...DROP PARTITION command. For example: With the above extract(month from date_column),product_type partition key, we have a partition key composed of multiple columns to represent the month of the date_column and the product type. In order to drop the partition of December (12) and product type 34 , use the following command: ALTER TABLE <table_name> DROP PARTITION 12,34; When using multiple columns for the partition key, Firebolt creates each partition with all the column boundaries, and when using the ALTER TABLE ... DROP PARTITION command - the full partition key should be specified. Providing a partial partition key value is not supported. Examples Example 1: Partition by a single column Create a fact table and partition it by year (assume data was loaded to that table): CREATE FACT TABLE transactions ( transaction_id BIGINT, transaction_date DATETIME, store_id INT, product_id INT, units_sold INT ) PRIMARY INDEX store_id, product_id PARTITION BY EXTRACT(YEAR from transaction_date); Drop all the transactions which took place in the year of 2020 : ALTER TABLE transactions DROP PARTITION 2020; Example 2: Partition by multiple columns Create a table with a partition key composed of multiple columns: CREATE FACT TABLE transactions ( transaction_id BIGINT, transaction_date DATETIME, store_id INT, product_id INT, units_sold INT ) PRIMARY INDEX store_id, product_id PARTITION BY store_id,EXTRACT(YEAR FROM transaction_date); Drop the transactions that have the store_id 982 and took place in 2020 : ALTER TABLE transactions DROP PARTITION 982,2020;","title":"Working with partitions"},{"location":"concepts/working-with-partitions/#working-with-partitions","text":"Partitions are multiple smaller physical parts of large tables created for data maintenance and performance. They are defined as part of the table's DDL and maintained as part of the table's internal structure. You can partition your data by any column in your table.","title":"Working with partitions"},{"location":"concepts/working-with-partitions/#when-to-use-partitions","text":"Partitions should be used on large fact tables in one of the following scenarios: Delete or update data: Partitions are useful if you need to perform update or delete operations on large portions of your data. Data can be deleted from a specified partition by using the ALTER TABLE ... DROP PARTITION command. After deleting data from a partition, it can be updated with new data by using an INSERT command. Boost performance: If your fact table is large enough (contains more than 100 million rows), consider using partitions to better prune data at query time. When the partition key appears in the WHERE clause of your queries, it will extend the Primary Index's functionality and reduce the required I/O so that you can benefit from even faster query response times. Note that the partition key column doesn't have to be part of a query's WHERE predicate. In such cases, Firebolt utilizes the Primary Index per partition and reads them all.","title":"When to use partitions"},{"location":"concepts/working-with-partitions/#considerations-and-limitations","text":"When using partitions, pay attention to the following: Plan carefully - partitions definition takes place during table creation. Check the queries you plan to run on your data - in particular - the WHERE clauses in them. Make sure that either the table primary index or the table partition key covers those to enjoy maximum data pruning. Read more here on how to configure the partition key here . The partition key cannot be composed of nullable columns.","title":"Considerations and limitations"},{"location":"concepts/working-with-partitions/#choosing-the-partition-key","text":"If you choose to use partitions in order to delete or update data by key, then your partition key should be the column by which you intend to update/delete. For example, if you need to store 1 month of data, set your partition key based on your date column, so you can drop partitions that are older than 1 month. If you choose to use partitions for boosting performance, set your partition key as the main predicate in your WHERE clause, so that it will prune as much data as possible for each query. For example, if your main query's predicate is the product_type column, set the partition key by this column. In both cases, the recommendation is not to use long text columns as your partition key. Partitioning your table is done as part of the CREATE FACT TABLE DDL using the PARTITION BY specifier.","title":"Choosing the partition key"},{"location":"concepts/working-with-partitions/#working-with-partitions_1","text":"Once the table is created with the required partition key, Firebolt arranges the table's data in the relevant partitions. New rows will be stored, each on the relevant partition and queries will prune and read from the relevant partition. New partitions will be created automatically during ingest, based on the partition key. Drop partitions, if requested, should be done manually by the DB admin.","title":"Working with partitions"},{"location":"concepts/working-with-partitions/#configuring-the-partition-key","text":"The partition key can be either a column name or a result of a function applied on a column: PARTITION BY date_column; PARTITION BY product_type; PARTITION BY EXTRACT(MONTH FROM date_column); PARTITION BY EXTRACT(MONTH FROM date_column), product_type; The following functions are supported for defining the partition key: DATE_FORMAT EXTRACT","title":"Configuring the partition key"},{"location":"concepts/working-with-partitions/#drop-partition","text":"In order to delete data from your table, use the ALTER TABLE...DROP PARTITION command. For example: With the above extract(month from date_column),product_type partition key, we have a partition key composed of multiple columns to represent the month of the date_column and the product type. In order to drop the partition of December (12) and product type 34 , use the following command: ALTER TABLE <table_name> DROP PARTITION 12,34; When using multiple columns for the partition key, Firebolt creates each partition with all the column boundaries, and when using the ALTER TABLE ... DROP PARTITION command - the full partition key should be specified. Providing a partial partition key value is not supported.","title":"Drop partition"},{"location":"concepts/working-with-partitions/#examples","text":"","title":"Examples"},{"location":"concepts/working-with-partitions/#example-1-partition-by-a-single-column","text":"Create a fact table and partition it by year (assume data was loaded to that table): CREATE FACT TABLE transactions ( transaction_id BIGINT, transaction_date DATETIME, store_id INT, product_id INT, units_sold INT ) PRIMARY INDEX store_id, product_id PARTITION BY EXTRACT(YEAR from transaction_date); Drop all the transactions which took place in the year of 2020 : ALTER TABLE transactions DROP PARTITION 2020;","title":"Example 1: Partition by a single column"},{"location":"concepts/working-with-partitions/#example-2-partition-by-multiple-columns","text":"Create a table with a partition key composed of multiple columns: CREATE FACT TABLE transactions ( transaction_id BIGINT, transaction_date DATETIME, store_id INT, product_id INT, units_sold INT ) PRIMARY INDEX store_id, product_id PARTITION BY store_id,EXTRACT(YEAR FROM transaction_date); Drop the transactions that have the store_id 982 and took place in 2020 : ALTER TABLE transactions DROP PARTITION 982,2020;","title":"Example 2: Partition by multiple columns"},{"location":"concepts/working-with-tables/","text":"Working with tables Tables in Firebolt have a few unique characteristics that are designed to optimize performance. This page will help you understand these concepts and how to work with Firebolt tables. FACT and DIMENSION tables When you create a table in Firebolt, you must also specify whether it\u2019s a FACT or a DIMENSION table. The two different types of tables are handled differently in order to optimize the performance of queries in general and joins in particular. FACT tables are always sharded across the nodes of the cluster DIMENSION tables are replicated across the nodes of the cluster When performing joins, the local shard of a fact table on each node is joined with the local copy of the dimension table FACT tables should be used for your traditional fact tables - usually your larger and most granular (transaction) tables. DIMENSION tables should be used for the smaller tables that are typically more descriptive in nature and are joined with the FACT tables. If your table does not fit in either of the traditional fact/dimension definition, then it is recommended to define very large tables as FACT, and smaller tables as DIMENSION. PRIMARY INDEX Firebolt tables are persisted in S3 in a proprietary file format called TripleF, aimed to optimize speed and efficiency. One of the unique characteristics of the TripleF format is that it is sorted, compressed, and indexed. What defines the sort order of the files is the PRIMARY INDEX defined on the table, which can include one or many fields. Primary indexes are mandatory for fact tables and optional for dimension tables. How to choose a primary index Because the primary index determines the physical sort order of the file, it significantly affects performance. For optimal performance, the primary index of fact tables should contain the fields that are most typically filtered or grouped by. This enables most queries to scan physically adjacent data and thus improve query performance. The primary index does not have to be identical to the field/s by which data is partitioned at the source. In dimension tables , the primary key should include the field/s that are used to join the dimension table to the fact table. Note A primary index should not be confused with a primary key in traditional database design. Unlike a primary key, the primary index is not unique. Example: creating fact and dimension tables Below is a simple example of creating a fact table and two dimension tables. CREATE FACT TABLE transactions ( transaction_id BIGINT, sale_date DATETIME, store_id INT, product_id INT, units_sold INT ) PRIMARY INDEX store_id, product_id; CREATE DIMENSION TABLE dim_store ( store_id INT, store_nunber INT, state TEXT, country TEXT ); CREATE DIMENSION TABLE dim_product ( product_id INT, product_name TEXT, product_category TEXT, brand TEXT ); Click here for the full CREATE TABLE reference.","title":"Working with tables"},{"location":"concepts/working-with-tables/#working-with-tables","text":"Tables in Firebolt have a few unique characteristics that are designed to optimize performance. This page will help you understand these concepts and how to work with Firebolt tables.","title":"Working with tables"},{"location":"concepts/working-with-tables/#fact-and-dimension-tables","text":"When you create a table in Firebolt, you must also specify whether it\u2019s a FACT or a DIMENSION table. The two different types of tables are handled differently in order to optimize the performance of queries in general and joins in particular. FACT tables are always sharded across the nodes of the cluster DIMENSION tables are replicated across the nodes of the cluster When performing joins, the local shard of a fact table on each node is joined with the local copy of the dimension table FACT tables should be used for your traditional fact tables - usually your larger and most granular (transaction) tables. DIMENSION tables should be used for the smaller tables that are typically more descriptive in nature and are joined with the FACT tables. If your table does not fit in either of the traditional fact/dimension definition, then it is recommended to define very large tables as FACT, and smaller tables as DIMENSION.","title":"FACT and DIMENSION tables"},{"location":"concepts/working-with-tables/#primary-index","text":"Firebolt tables are persisted in S3 in a proprietary file format called TripleF, aimed to optimize speed and efficiency. One of the unique characteristics of the TripleF format is that it is sorted, compressed, and indexed. What defines the sort order of the files is the PRIMARY INDEX defined on the table, which can include one or many fields. Primary indexes are mandatory for fact tables and optional for dimension tables.","title":"PRIMARY INDEX"},{"location":"concepts/working-with-tables/#how-to-choose-a-primary-index","text":"Because the primary index determines the physical sort order of the file, it significantly affects performance. For optimal performance, the primary index of fact tables should contain the fields that are most typically filtered or grouped by. This enables most queries to scan physically adjacent data and thus improve query performance. The primary index does not have to be identical to the field/s by which data is partitioned at the source. In dimension tables , the primary key should include the field/s that are used to join the dimension table to the fact table. Note A primary index should not be confused with a primary key in traditional database design. Unlike a primary key, the primary index is not unique.","title":"How to choose a primary index"},{"location":"concepts/working-with-tables/#example-creating-fact-and-dimension-tables","text":"Below is a simple example of creating a fact table and two dimension tables. CREATE FACT TABLE transactions ( transaction_id BIGINT, sale_date DATETIME, store_id INT, product_id INT, units_sold INT ) PRIMARY INDEX store_id, product_id; CREATE DIMENSION TABLE dim_store ( store_id INT, store_nunber INT, state TEXT, country TEXT ); CREATE DIMENSION TABLE dim_product ( product_id INT, product_name TEXT, product_category TEXT, brand TEXT ); Click here for the full CREATE TABLE reference.","title":"Example: creating fact and dimension tables"},{"location":"concepts/working-with-semi-structured-data/","text":"Working with semi-structured data Background Semi-structured data is any data that does not adhere to a strict tabular schema and/or some of its field types is not of standard SQL type. Such data usually has a nested structure and supports complex data types like arrays, maps, and structs (compound types). The prototypical example of such a data format is JSON, but many other serialization formats such as Avro, Parquet, and ORC support similar features. For reference see semi-structured data functions Arrays are the building blocks of how Firebolt represent semi-structured data, among others, they are used to represent: Arrays of varying lengths in the input, unknown at the creation of the table. These arrays can have arbitrary nesting levels, however, the nesting level should be the same for a given column and known at the creation time of the table. Maps - using two coordinated arrays - one for keys, the other for values. This is especially useful for JSON like semi-structured data sources in which each object can have different keys - so a fixed schema cannot handle such data properly In some cases, when the JSON adheres to a fixed schema, that is, each object has a known set of keys, and nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. This page will introduce the correspondence between semi-structured constructs to firebolt arrays. The subsection will elaborate on the following topics: Working with arrays Ingesting semi-structured data Representing semi-structured data in Firebolt Source Data Throughout this page and its subsections, we will use a set of JSON records that can result from a website's logs/web-analytics platform. We will start with a simple example that will become more involved and realistic as we present new concepts. Each record in the source data represents a \"visit\" or \"session\" on the website. The records will usually be stored in an \"Object per line\" file, that is, each line is a JSON object, although the file as a whole is not a valid JSON. This test file will usually be stored in the Data Lake in a compressed form. More about it in the ingestion section . Assume we have the following two records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Here are some important points to pay attention to: For each record, there are mandatory scalar fields: \"id\", \"StartTime\", and \"Duration\". There is an array of arbitrary length (potentially empty) of \"tags\". There is a \"map\" of user agent properties. Those properties can change from record to record and the full set of potential properties is not known at the creation of the table. Representation as a Firebolt table Let's see how we represent the above semi-structured \"schema\" in a Firebolt table. For the mandatory scalar fields, we will have regular columns. For the \"tags\" array we will define a column whose type will be ARRAY(TEXT) . For the user agent properties map, we will define two columns: one for the keys and one for the values - this is a pattern used to represent maps (or dictionaries) in Firebolt. We will encounter it in many examples. Combining the above will result in the following table - note that the headings specify the column name and type id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] The DDL statement creating the above table will be: CREATE [FACT|DIMENSION] TABLE visits ( id INT, StartTime DATETIME, tags ARRAY(TEXT), agent_props_keys ARRAY(TEXT), agent_props_vals ARRAY(TEXT) ) PRIMARY INDEX ... In the next sections, we will see how to query and manipulate the resultant table ( working with arrays ) and how to ingest and transform the semi-structured data into a Firebolt table ( ingesting semi-structured data ).","title":"Working with semi-structured data"},{"location":"concepts/working-with-semi-structured-data/#working-with-semi-structured-data","text":"","title":"Working with semi-structured data"},{"location":"concepts/working-with-semi-structured-data/#background","text":"Semi-structured data is any data that does not adhere to a strict tabular schema and/or some of its field types is not of standard SQL type. Such data usually has a nested structure and supports complex data types like arrays, maps, and structs (compound types). The prototypical example of such a data format is JSON, but many other serialization formats such as Avro, Parquet, and ORC support similar features. For reference see semi-structured data functions Arrays are the building blocks of how Firebolt represent semi-structured data, among others, they are used to represent: Arrays of varying lengths in the input, unknown at the creation of the table. These arrays can have arbitrary nesting levels, however, the nesting level should be the same for a given column and known at the creation time of the table. Maps - using two coordinated arrays - one for keys, the other for values. This is especially useful for JSON like semi-structured data sources in which each object can have different keys - so a fixed schema cannot handle such data properly In some cases, when the JSON adheres to a fixed schema, that is, each object has a known set of keys, and nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. This page will introduce the correspondence between semi-structured constructs to firebolt arrays. The subsection will elaborate on the following topics: Working with arrays Ingesting semi-structured data","title":"Background"},{"location":"concepts/working-with-semi-structured-data/#representing-semi-structured-data-in-firebolt","text":"","title":"Representing semi-structured data in Firebolt"},{"location":"concepts/working-with-semi-structured-data/#source-data","text":"Throughout this page and its subsections, we will use a set of JSON records that can result from a website's logs/web-analytics platform. We will start with a simple example that will become more involved and realistic as we present new concepts. Each record in the source data represents a \"visit\" or \"session\" on the website. The records will usually be stored in an \"Object per line\" file, that is, each line is a JSON object, although the file as a whole is not a valid JSON. This test file will usually be stored in the Data Lake in a compressed form. More about it in the ingestion section . Assume we have the following two records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Here are some important points to pay attention to: For each record, there are mandatory scalar fields: \"id\", \"StartTime\", and \"Duration\". There is an array of arbitrary length (potentially empty) of \"tags\". There is a \"map\" of user agent properties. Those properties can change from record to record and the full set of potential properties is not known at the creation of the table.","title":"Source Data"},{"location":"concepts/working-with-semi-structured-data/#representation-as-a-firebolt-table","text":"Let's see how we represent the above semi-structured \"schema\" in a Firebolt table. For the mandatory scalar fields, we will have regular columns. For the \"tags\" array we will define a column whose type will be ARRAY(TEXT) . For the user agent properties map, we will define two columns: one for the keys and one for the values - this is a pattern used to represent maps (or dictionaries) in Firebolt. We will encounter it in many examples. Combining the above will result in the following table - note that the headings specify the column name and type id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] The DDL statement creating the above table will be: CREATE [FACT|DIMENSION] TABLE visits ( id INT, StartTime DATETIME, tags ARRAY(TEXT), agent_props_keys ARRAY(TEXT), agent_props_vals ARRAY(TEXT) ) PRIMARY INDEX ... In the next sections, we will see how to query and manipulate the resultant table ( working with arrays ) and how to ingest and transform the semi-structured data into a Firebolt table ( ingesting semi-structured data ).","title":"Representation as a Firebolt table"},{"location":"concepts/working-with-semi-structured-data/ingesting-semi-structured-data/","text":"Ingesting semi-structured data There are three major approaches to ingest and handle semi-structured data: Transforming the input using JSON and ARRAY functions to fit the target schema during ingestion. Ingesting the JSON object as raw TEXT rows, and later using JSON and ARRAY functions to query and manipulate them When the input JSON adheres to a fixed schema, that is, each object has a known set of keys, and the nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. Omitted keys can be handled by specifying default values for the respective columns, but keys that are defined at table creation time will be ignored. All options can be combined depending on the use case: the nature of the input data and the queries to be performed. The 3rd approach is not very common with true semi-structured data sources, but usually is the result of an export from table-oriented storage, and therefore will be discussed in a separate section. We will continue with our source JSON example. Assume that each JSON record is stored as plain text in the column raw_json of a (potentially external) table named source_json As a reminder here are the JSON records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Recall that the target table named \"Visits\" should look as follows: id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] Extracting top-level scalars and arrays For the top-level keys: \"id\", \"Duration\", and \"tags\" the task is straightforward using JSON_EXTRACT function. This function accepts three parameters: Although \"StartTime\" is also a scalar field, since there is no native DATETIME type in JSON type system it will require an additional step An expression containing a JSON string A JSON Pointer specifying the location in the JSON object from where the value will be extracted, A type specifier indicating Firebolt's SQL type that will be returned from the function. This type should correspond to the JSON type found under the key pointed by the JSON Pointer. For a detailed discussion of JSON to SQL type mapping see the Type Parameters section in the JSON Functions reference. Note that our native support for arrays makes the extraction of tags as simple as other scalar types. Putting those concepts in action will result in the following query that will return the expected tabular representation: SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags FROM source_json Result: id duration tags 1 450 [\"summer-sale\",\"sports\"] 2 959 [\"gadgets\",\"audio\"] Since we want to store \"StartTime\" as a DATETIME SQL typed column, which will allow many optimizations, correct ordering, and other benefits, and since JSON type system lacks such type we will have to cast the result of JSON_EXTRACT of this field: SELECT -- ... other fields CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) FROM source_json Extracting sub-object keys and values Now we have to perform a non-trivial transformation to the input data. We need to take the JSON keys of the sub-object user_agent , and their corresponding values and reshape them as two coordinated arrays. The functions JSON_EXTRACT_KEYS and JSON_EXTRACT_VALUES do exactly this. The first will return the keys under the sub-objects pointed by the JSON pointer provided as the first parameter, and the second will return the values of this sub-object as strings . That means that if under a certain key there is an arbitrarily nested sub-object - the whole object will be returned as a single TEXT element in the resulting array. Putting it all together The following statement takes the raw JSON input and transforms it into our target schema. The result is provided as an illustration, since an INSERT INTO ... return only the number of affected rows. INSERT INTO Visits SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) as StartTime, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags, JSON_EXTRACT_KEYS(raw_json,'/user_agent') as agent_props_keys, JSON_EXTRACT_VALUES(raw_json,'/user_agent') as agent_props_vals FROM doc_visits_source Result (if the script whould have been excecuted without the INSERT INTO clause): id StartTime duration tags agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Ingesting semi-structured data"},{"location":"concepts/working-with-semi-structured-data/ingesting-semi-structured-data/#ingesting-semi-structured-data","text":"There are three major approaches to ingest and handle semi-structured data: Transforming the input using JSON and ARRAY functions to fit the target schema during ingestion. Ingesting the JSON object as raw TEXT rows, and later using JSON and ARRAY functions to query and manipulate them When the input JSON adheres to a fixed schema, that is, each object has a known set of keys, and the nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. Omitted keys can be handled by specifying default values for the respective columns, but keys that are defined at table creation time will be ignored. All options can be combined depending on the use case: the nature of the input data and the queries to be performed. The 3rd approach is not very common with true semi-structured data sources, but usually is the result of an export from table-oriented storage, and therefore will be discussed in a separate section. We will continue with our source JSON example. Assume that each JSON record is stored as plain text in the column raw_json of a (potentially external) table named source_json As a reminder here are the JSON records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Recall that the target table named \"Visits\" should look as follows: id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Ingesting semi-structured data"},{"location":"concepts/working-with-semi-structured-data/ingesting-semi-structured-data/#extracting-top-level-scalars-and-arrays","text":"For the top-level keys: \"id\", \"Duration\", and \"tags\" the task is straightforward using JSON_EXTRACT function. This function accepts three parameters: Although \"StartTime\" is also a scalar field, since there is no native DATETIME type in JSON type system it will require an additional step An expression containing a JSON string A JSON Pointer specifying the location in the JSON object from where the value will be extracted, A type specifier indicating Firebolt's SQL type that will be returned from the function. This type should correspond to the JSON type found under the key pointed by the JSON Pointer. For a detailed discussion of JSON to SQL type mapping see the Type Parameters section in the JSON Functions reference. Note that our native support for arrays makes the extraction of tags as simple as other scalar types. Putting those concepts in action will result in the following query that will return the expected tabular representation: SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags FROM source_json Result: id duration tags 1 450 [\"summer-sale\",\"sports\"] 2 959 [\"gadgets\",\"audio\"] Since we want to store \"StartTime\" as a DATETIME SQL typed column, which will allow many optimizations, correct ordering, and other benefits, and since JSON type system lacks such type we will have to cast the result of JSON_EXTRACT of this field: SELECT -- ... other fields CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) FROM source_json","title":"Extracting top-level scalars and arrays"},{"location":"concepts/working-with-semi-structured-data/ingesting-semi-structured-data/#extracting-sub-object-keys-and-values","text":"Now we have to perform a non-trivial transformation to the input data. We need to take the JSON keys of the sub-object user_agent , and their corresponding values and reshape them as two coordinated arrays. The functions JSON_EXTRACT_KEYS and JSON_EXTRACT_VALUES do exactly this. The first will return the keys under the sub-objects pointed by the JSON pointer provided as the first parameter, and the second will return the values of this sub-object as strings . That means that if under a certain key there is an arbitrarily nested sub-object - the whole object will be returned as a single TEXT element in the resulting array.","title":"Extracting sub-object keys and values"},{"location":"concepts/working-with-semi-structured-data/ingesting-semi-structured-data/#putting-it-all-together","text":"The following statement takes the raw JSON input and transforms it into our target schema. The result is provided as an illustration, since an INSERT INTO ... return only the number of affected rows. INSERT INTO Visits SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) as StartTime, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags, JSON_EXTRACT_KEYS(raw_json,'/user_agent') as agent_props_keys, JSON_EXTRACT_VALUES(raw_json,'/user_agent') as agent_props_vals FROM doc_visits_source Result (if the script whould have been excecuted without the INSERT INTO clause): id StartTime duration tags agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Putting it all together"},{"location":"concepts/working-with-semi-structured-data/working-with-arrays/","text":"Working with arrays In this section, we introduce the mechanisms which allow querying and manipulating arrays in a Firebolt warehouse. For a full reference see array functions and aggregate array functions . Array types are declared using ARRAY(<type>) where <type> can be any Firebolt supported type, including array, thus, arrays can be arbitrarily nested. The innermost type (the scalar) can be nullable, however, array typed columns cannot be nullable. Array literals are also supported, so for example, the following is a valid SQL: SELECT [1,2,3,4] Throughout this page we will use the table presented in the overview page and assume its name is visits , the relevant columns are presented here for convenience. id INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] Basic Functionality There are several self-explanatory functions to work with arrays including LENGTH, ARRAY_CONCAT , and FLATTEN . See the respective reference for a full description. Here's a short example: SELECT LENGTH(agent_prop_keys) FROM visits -- returns 3,2 SELECT ARRAY_CONCAT(agent_props_keys, agent_props_vals) FROM visits -- returns [\"agent\", \"platform\", \"resolution\", \"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] SELECT ARRAY_FLATTEN([ [[1,2,3],[4,5]], [[2]] ]) -- returns [1,2,3,4,5,2] Manipulating arrays with Lambda functions Much of the power in Firebolt's array manipulation functionality comes from incorporating Lambda functions. These are expressions passed to functions applied to arrays and in turn, are being applied element by element. The results of the Lambda function interpreted according to the array function in use. The general form of array functions taking a Lambda as an argument is ARRAY_FUNC(<lambda-expression>, arr[, arr1, arr2...]) , where the lambda expression is in the form a1[, a2, a3...] -> <lambda-body> , and arr , arr1 , arr2 ... are expressions evaluating to arrays. The variables a1, a2, ... are called the lambda arguments. The number of the lambda arguments must be equal to the number of arrays passed starting from the second argument of the array function. In addition, the lengths of all the arrays should be equal. We'll start with a simple example using a single array argument. SELECT TRANSFORM(t -> UPPER(t), tags) as up_tags FROM visits Here, the function TRANSFORM will apply the lambda body - that is convert the element to upper-case - on each of the array elements and will result in: up_tags [\"SUMMER_SALE\", \"SPORTS\"] [\"GADGETS\", \"AUDIO\"] A common use case where multiple array arguments are provided is used in the context of two arrays representing a map. The function ARRAY_FIRST returns the first element for which the lambda expression returns a result other than 0. The return value will always be taken from the first array argument provided, however, the lambda expression can compute its result based on all of the lambda arguments corresponding to elements of the array arguments. So if we want to find the value in agent_props_vals corresponding to the key \"platform\" in agent_props_keys the following query: SELECT ARRAY_FIRST(v, k -> k = 'platform', agent_props_vals, agent_props_keys) as platform FROM visits Returns the desired results: platform \"Windows NT 6.1\" \"iOS 14\" UNNEST Sometimes it is desirable to transform the nested array structure to a standard tabular format. This can be used to expose views to BI tools that cannot handle Firebolt array syntax, or the tabular format is more natural to query using standard SQL idioms. UNNEST serves these purposes. UNNEST is part of the FROM clause and it resembles a JOIN sub-clause. Given an array typed column, it unfolds the element of the array and duplicates all other columns found in the SELECT clause per each array element. For example the following query: SELECT id, tags FROM visits UNNEST(tags) Will result in: id tags 1 \"summer-sale\" 1 \"sports\" 2 \"gadgets\" 2 \"audio\"","title":"Working with arrays"},{"location":"concepts/working-with-semi-structured-data/working-with-arrays/#working-with-arrays","text":"In this section, we introduce the mechanisms which allow querying and manipulating arrays in a Firebolt warehouse. For a full reference see array functions and aggregate array functions . Array types are declared using ARRAY(<type>) where <type> can be any Firebolt supported type, including array, thus, arrays can be arbitrarily nested. The innermost type (the scalar) can be nullable, however, array typed columns cannot be nullable. Array literals are also supported, so for example, the following is a valid SQL: SELECT [1,2,3,4] Throughout this page we will use the table presented in the overview page and assume its name is visits , the relevant columns are presented here for convenience. id INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Working with arrays"},{"location":"concepts/working-with-semi-structured-data/working-with-arrays/#basic-functionality","text":"There are several self-explanatory functions to work with arrays including LENGTH, ARRAY_CONCAT , and FLATTEN . See the respective reference for a full description. Here's a short example: SELECT LENGTH(agent_prop_keys) FROM visits -- returns 3,2 SELECT ARRAY_CONCAT(agent_props_keys, agent_props_vals) FROM visits -- returns [\"agent\", \"platform\", \"resolution\", \"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] SELECT ARRAY_FLATTEN([ [[1,2,3],[4,5]], [[2]] ]) -- returns [1,2,3,4,5,2]","title":"Basic Functionality"},{"location":"concepts/working-with-semi-structured-data/working-with-arrays/#manipulating-arrays-with-lambda-functions","text":"Much of the power in Firebolt's array manipulation functionality comes from incorporating Lambda functions. These are expressions passed to functions applied to arrays and in turn, are being applied element by element. The results of the Lambda function interpreted according to the array function in use. The general form of array functions taking a Lambda as an argument is ARRAY_FUNC(<lambda-expression>, arr[, arr1, arr2...]) , where the lambda expression is in the form a1[, a2, a3...] -> <lambda-body> , and arr , arr1 , arr2 ... are expressions evaluating to arrays. The variables a1, a2, ... are called the lambda arguments. The number of the lambda arguments must be equal to the number of arrays passed starting from the second argument of the array function. In addition, the lengths of all the arrays should be equal. We'll start with a simple example using a single array argument. SELECT TRANSFORM(t -> UPPER(t), tags) as up_tags FROM visits Here, the function TRANSFORM will apply the lambda body - that is convert the element to upper-case - on each of the array elements and will result in: up_tags [\"SUMMER_SALE\", \"SPORTS\"] [\"GADGETS\", \"AUDIO\"] A common use case where multiple array arguments are provided is used in the context of two arrays representing a map. The function ARRAY_FIRST returns the first element for which the lambda expression returns a result other than 0. The return value will always be taken from the first array argument provided, however, the lambda expression can compute its result based on all of the lambda arguments corresponding to elements of the array arguments. So if we want to find the value in agent_props_vals corresponding to the key \"platform\" in agent_props_keys the following query: SELECT ARRAY_FIRST(v, k -> k = 'platform', agent_props_vals, agent_props_keys) as platform FROM visits Returns the desired results: platform \"Windows NT 6.1\" \"iOS 14\"","title":"Manipulating arrays with Lambda functions"},{"location":"concepts/working-with-semi-structured-data/working-with-arrays/#unnest","text":"Sometimes it is desirable to transform the nested array structure to a standard tabular format. This can be used to expose views to BI tools that cannot handle Firebolt array syntax, or the tabular format is more natural to query using standard SQL idioms. UNNEST serves these purposes. UNNEST is part of the FROM clause and it resembles a JOIN sub-clause. Given an array typed column, it unfolds the element of the array and duplicates all other columns found in the SELECT clause per each array element. For example the following query: SELECT id, tags FROM visits UNNEST(tags) Will result in: id tags 1 \"summer-sale\" 1 \"sports\" 2 \"gadgets\" 2 \"audio\"","title":"UNNEST"},{"location":"concepts/working-with-semi-structured-data/working-with-parquet-arrays-of-structs-and-maps/","text":"Working with Parquet arrays and maps Apache Parquet is a binary file format that can store complex nested file structures in a compressed, columnar format. This topic provides guidance for ingesting and querying Parquet data that is stored as structs in arrays or as maps of key-value pairs. Defining external table columns for Parquet arrays and maps When you set up an external table to ingest Parquet data files, you use a hierarchical dotted notation syntax to define table columns. Firebolt uses this notation to identify the field to ingest. Prerequisite To run queries and DML commands on external tables that use this notation, you must run the SET command shown in the example below. You cannot run the SET clause within another statement. In other words, it must be on its own and terminated by a semi-colon. This also applies to external tables connected to AWS Glue. SET use_short_column_path_parquet = 1; Syntax for defining a Parquet nested structure You specify the top grouping element of a nested structure in Parquet followed by the field in that structure that contains the data to ingest. You then declare the column type using the ARRAY(<data_type>) notation, where <data type> is the Firebolt data type corresponding to the data type of the field in Parquet. \"<grouping1>.<datafield>\" ARRAY(<data_type>) Examples of this syntax in CREATE EXTERNAL TABLE queries are demonstrated below. Example - ingest and work with structs inside Parquet arrays Consider the Parquet schema example below. The following elements define an array of structs: A single, optional group field, hashtags , contains any number of another group, bag . This is the top grouping element. The bag groups each contain a single, optional group, array_element . The array_element group contains a single, optional field, s . The field some_value contains a value that is a STRING type (in binary primitive format). optional group hashtags (LIST) { repeated group bag { optional group array_element { optional binary some_value (STRING); } The steps below demonstrate the process to ingest the array values into Firebolt. You create an external table, create a fact table, and insert data into the fact table from the external table, which is connected to the Parquet data store. Step 1 - create an external table The CREATE EXTERNAL TABLE example below creates a column in an external table from the Parquet schema shown in the example above. The column definition uses the top level grouping hashtags followed by the field some_value . Intermediate nesting levels are omitted. CREATE EXTERNAL TABLE IF NOT EXISTS my_parquet_array_ext_tbl ( [...,] --additional columns possible, not shown \"hashtags.some_value\" ARRAY(STRING) [,...] ) CREDENTIALS = (AWS_KEY_ID = '****' AWS_SECRET_KEY = '*****') URL = 's3://my_bucket_of_parquet_goodies/' OBJECT_PATTERN = '*.parquet' TYPE = (PARQUET); When connecting your external table to AWS Glue, we create the columns automatically using the same logic as described above. Step 2 - create a fact or dimension table Create a fact or dimension table that defines a column of the same ARRAY(STRING) type that you defined in the external table in step 1. The example below demonstrates this for a fact table. CREATE FACT TABLE IF NOT EXISTS my_parquet_array_fact_tbl ( [...,] --additional columns possible, not shown some_value ARRAY(STRING) [,...] ) [...] --required primary index for fact table not shown --optional partitions not shown ; Step 3 - insert into the fact table from the external table The example below demonstrates an INSERT INTO statement that selects the array values from Parquet data files using the external table column definition in step 1, and then inserts them into the specified fact table column, some_value . SET use_short_column_path_parquet = 1; INSERT INTO my_parquet_array_fact_tbl SELECT \"hashtags.some_value\" AS some_value FROM my_parquet_array_ext_tbl; Step 4 - query array values After you ingest array values into the fact table, you can query the array using the array functions and lambda function approach covered in Working with arrays . Example - ingest and work with maps External tables connected to AWS Glue currently do not support reading maps from Parquet files. Map keys and values in Parquet appear within a group similar to arrays. Consider the Parquet schema example below. The following define the key-value elements of the map: A single, optional group, context , is a group of mappings that contains any number of the group key_value . The key_value groups each contain a required field, key , which contains the key name as a STRING . Each group also contains an optional field value , which contains the value as a STRING corresponding to the key name in the same key_value group. optional group context (MAP) { repeated group key_value { required binary key (STRING); optional binary value (STRING); } } The steps below demonstrate the process of creating an external table, creating a fact table, and inserting data into the fact table from the Parquet file using the external table. Step 1 - create an external table When you create an external table for a Parquet map, you use the same syntax that you use in the example for arrays above. You create one column for keys and another column for values. The CREATE EXTERNAL TABLE example below demonstrates this. CREATE EXTERNAL TABLE IF NOT EXISTS my_parquet_map_ext_tbl ( \"context.keys\" ARRAY(STRING), \"context.values\" ARRAY(STRING) ) CREDENTIALS = (AWS_KEY_ID = '****' AWS_SECRET_KEY = '*****') URL = 's3://my_bucket_of_parquet/' OBJECT_PATTERN = '*.parquet' TYPE = (PARQUET); Step 2 - create a fact or dimension table Create a Firebolt fact or dimension table that defines columns of the same ARRAY(STRING) types that you defined in the external table in step 1. The example below demonstrates this for a fact table. CREATE FACT TABLE IF NOT EXISTS my_parquet_map_fact_tbl ( [...,] --additional columns possible, not shown my_parquet_array_keys ARRAY(STRING) my_parquet_array_values ARRAY(STRING) [,...] ) [...] --required primary index for fact table not shown --optional partitions not shown Step 3 - insert into the fact table from the external table The example below demonstrates an INSERT INTO statement that selects the array values from Parquet data files using the external table column definition in step 1, and inserts them into the specified fact table columns, my_parquet_array_keys and my_parquet_array_values . SET use_short_column_path_parquet = 1; INSERT INTO my_parquet_map_fact_tbl SELECT \"context.keys\" AS my_parquet_array_keys, \"context.values\" AS my_parquet_array_values FROM my_parquet_map_ext_tbl Step 4 - query map values After you ingest array values into the fact table, you can query the array using the array functions and lambda function approach covered in Working with arrays . A query that uses a lambda function to return a specific value by specifying the corresponding key value is shown below. For more information, see Manipulating arrays using lambda functions . SET use_short_column_path_parquet = 1; SELECT ARRAY_FIRST(v, k -> k = 'key_name_of_interest', my_parquet_array_keys, my_parquet_array_values) AS returned_corresponding_key_value FROM my_parquet_map_ext_tbl","title":"Working with Parquet arrays and maps"},{"location":"concepts/working-with-semi-structured-data/working-with-parquet-arrays-of-structs-and-maps/#working-with-parquet-arrays-and-maps","text":"Apache Parquet is a binary file format that can store complex nested file structures in a compressed, columnar format. This topic provides guidance for ingesting and querying Parquet data that is stored as structs in arrays or as maps of key-value pairs.","title":"Working with Parquet arrays and maps"},{"location":"concepts/working-with-semi-structured-data/working-with-parquet-arrays-of-structs-and-maps/#defining-external-table-columns-for-parquet-arrays-and-maps","text":"When you set up an external table to ingest Parquet data files, you use a hierarchical dotted notation syntax to define table columns. Firebolt uses this notation to identify the field to ingest.","title":"Defining external table columns for Parquet arrays and maps"},{"location":"concepts/working-with-semi-structured-data/working-with-parquet-arrays-of-structs-and-maps/#prerequisite","text":"To run queries and DML commands on external tables that use this notation, you must run the SET command shown in the example below. You cannot run the SET clause within another statement. In other words, it must be on its own and terminated by a semi-colon. This also applies to external tables connected to AWS Glue. SET use_short_column_path_parquet = 1;","title":"Prerequisite"},{"location":"concepts/working-with-semi-structured-data/working-with-parquet-arrays-of-structs-and-maps/#syntax-for-defining-a-parquet-nested-structure","text":"You specify the top grouping element of a nested structure in Parquet followed by the field in that structure that contains the data to ingest. You then declare the column type using the ARRAY(<data_type>) notation, where <data type> is the Firebolt data type corresponding to the data type of the field in Parquet. \"<grouping1>.<datafield>\" ARRAY(<data_type>) Examples of this syntax in CREATE EXTERNAL TABLE queries are demonstrated below.","title":"Syntax for defining a Parquet nested structure"},{"location":"concepts/working-with-semi-structured-data/working-with-parquet-arrays-of-structs-and-maps/#example-ingest-and-work-with-structs-inside-parquet-arrays","text":"Consider the Parquet schema example below. The following elements define an array of structs: A single, optional group field, hashtags , contains any number of another group, bag . This is the top grouping element. The bag groups each contain a single, optional group, array_element . The array_element group contains a single, optional field, s . The field some_value contains a value that is a STRING type (in binary primitive format). optional group hashtags (LIST) { repeated group bag { optional group array_element { optional binary some_value (STRING); } The steps below demonstrate the process to ingest the array values into Firebolt. You create an external table, create a fact table, and insert data into the fact table from the external table, which is connected to the Parquet data store. Step 1 - create an external table The CREATE EXTERNAL TABLE example below creates a column in an external table from the Parquet schema shown in the example above. The column definition uses the top level grouping hashtags followed by the field some_value . Intermediate nesting levels are omitted. CREATE EXTERNAL TABLE IF NOT EXISTS my_parquet_array_ext_tbl ( [...,] --additional columns possible, not shown \"hashtags.some_value\" ARRAY(STRING) [,...] ) CREDENTIALS = (AWS_KEY_ID = '****' AWS_SECRET_KEY = '*****') URL = 's3://my_bucket_of_parquet_goodies/' OBJECT_PATTERN = '*.parquet' TYPE = (PARQUET); When connecting your external table to AWS Glue, we create the columns automatically using the same logic as described above. Step 2 - create a fact or dimension table Create a fact or dimension table that defines a column of the same ARRAY(STRING) type that you defined in the external table in step 1. The example below demonstrates this for a fact table. CREATE FACT TABLE IF NOT EXISTS my_parquet_array_fact_tbl ( [...,] --additional columns possible, not shown some_value ARRAY(STRING) [,...] ) [...] --required primary index for fact table not shown --optional partitions not shown ; Step 3 - insert into the fact table from the external table The example below demonstrates an INSERT INTO statement that selects the array values from Parquet data files using the external table column definition in step 1, and then inserts them into the specified fact table column, some_value . SET use_short_column_path_parquet = 1; INSERT INTO my_parquet_array_fact_tbl SELECT \"hashtags.some_value\" AS some_value FROM my_parquet_array_ext_tbl; Step 4 - query array values After you ingest array values into the fact table, you can query the array using the array functions and lambda function approach covered in Working with arrays .","title":"Example - ingest and work with structs inside Parquet arrays"},{"location":"concepts/working-with-semi-structured-data/working-with-parquet-arrays-of-structs-and-maps/#example-ingest-and-work-with-maps","text":"External tables connected to AWS Glue currently do not support reading maps from Parquet files. Map keys and values in Parquet appear within a group similar to arrays. Consider the Parquet schema example below. The following define the key-value elements of the map: A single, optional group, context , is a group of mappings that contains any number of the group key_value . The key_value groups each contain a required field, key , which contains the key name as a STRING . Each group also contains an optional field value , which contains the value as a STRING corresponding to the key name in the same key_value group. optional group context (MAP) { repeated group key_value { required binary key (STRING); optional binary value (STRING); } } The steps below demonstrate the process of creating an external table, creating a fact table, and inserting data into the fact table from the Parquet file using the external table. Step 1 - create an external table When you create an external table for a Parquet map, you use the same syntax that you use in the example for arrays above. You create one column for keys and another column for values. The CREATE EXTERNAL TABLE example below demonstrates this. CREATE EXTERNAL TABLE IF NOT EXISTS my_parquet_map_ext_tbl ( \"context.keys\" ARRAY(STRING), \"context.values\" ARRAY(STRING) ) CREDENTIALS = (AWS_KEY_ID = '****' AWS_SECRET_KEY = '*****') URL = 's3://my_bucket_of_parquet/' OBJECT_PATTERN = '*.parquet' TYPE = (PARQUET); Step 2 - create a fact or dimension table Create a Firebolt fact or dimension table that defines columns of the same ARRAY(STRING) types that you defined in the external table in step 1. The example below demonstrates this for a fact table. CREATE FACT TABLE IF NOT EXISTS my_parquet_map_fact_tbl ( [...,] --additional columns possible, not shown my_parquet_array_keys ARRAY(STRING) my_parquet_array_values ARRAY(STRING) [,...] ) [...] --required primary index for fact table not shown --optional partitions not shown Step 3 - insert into the fact table from the external table The example below demonstrates an INSERT INTO statement that selects the array values from Parquet data files using the external table column definition in step 1, and inserts them into the specified fact table columns, my_parquet_array_keys and my_parquet_array_values . SET use_short_column_path_parquet = 1; INSERT INTO my_parquet_map_fact_tbl SELECT \"context.keys\" AS my_parquet_array_keys, \"context.values\" AS my_parquet_array_values FROM my_parquet_map_ext_tbl Step 4 - query map values After you ingest array values into the fact table, you can query the array using the array functions and lambda function approach covered in Working with arrays . A query that uses a lambda function to return a specific value by specifying the corresponding key value is shown below. For more information, see Manipulating arrays using lambda functions . SET use_short_column_path_parquet = 1; SELECT ARRAY_FIRST(v, k -> k = 'key_name_of_interest', my_parquet_array_keys, my_parquet_array_values) AS returned_corresponding_key_value FROM my_parquet_map_ext_tbl","title":"Example - ingest and work with maps"},{"location":"connecting-to-firebolt/","text":"Connecting to Firebolt These topics provide information about the Firebolt clients, which enable connecting to Firebolt from external tools and applications. Each topic provides installation, configuration, and usage details for the respective Firebolt client. Connecting to Firebolt via JDBC Connecting to Firebolt via REST API","title":"Connecting to Firebolt"},{"location":"connecting-to-firebolt/#connecting-to-firebolt","text":"These topics provide information about the Firebolt clients, which enable connecting to Firebolt from external tools and applications. Each topic provides installation, configuration, and usage details for the respective Firebolt client. Connecting to Firebolt via JDBC Connecting to Firebolt via REST API","title":"Connecting to Firebolt"},{"location":"connecting-to-firebolt/connect-to-your-database-programmatically/","text":"Connect to your database programmatically This section explains how to connect to your database programmatically. If you are using an application like DBeaver that manages your client connections for you, then you can skip this section and move directly to this guide . Connecting a database by using Python Connecting to your database in Firebolt requires authentication. Following is an example that demonstrates how to connect to your database using Python. Prepare in advance: Firebolt username Firebolt password Database name Engine name (optional) Example: connect to a database by using Python Requirements: The JayDeBeApi module should be installed. Use Python 3.0 and above. Download Firebolt's latest JDBC driver from here . Fill in the relevant params in lines 15 - 25. import time import pprint try: import jaydebeapi except: raise Exception(\"Failed to import jaydebeapi, try the following: sudo pip3 install JayDeBeApi\") # A list of queries to run. Each query wrapped by \"\" queries = [\"\"\" provide the first query \"\"\", \"\"\" provide the second query \"\"\"] # Username, e.g: 'user@company.com' username = 'provide the user name' # Password, e.g: 'mypassword' password = 'provide the password' # Database, e.g: 'my_db' database = 'provide the database name' # Jar path, e.g: /users/john/jar/firebolt-jdbc-1.07-jar-with-dependencies.jar' jar_path = \"provide the path to Firebolt's jar\" # Printing the results def type_code_repr(type_code: jaydebeapi.DBAPITypeObject) -> str: SHORTER_REPRS = dict( [ (jaydebeapi.BINARY, \"BINARY\"), (jaydebeapi.DATE, \"DATE\"), (jaydebeapi.DATETIME, \"DATETIME\"), (jaydebeapi.DECIMAL, \"DECIMAL\"), (jaydebeapi.FLOAT, \"FLOAT\"), (jaydebeapi.NUMBER, \"NUMBER\"), (jaydebeapi.ROWID, \"ROWID\"), (jaydebeapi.STRING, \"STRING\"), (jaydebeapi.TEXT, \"TEXT\"), (jaydebeapi.TIME, \"TIME\"), ] ) return SHORTER_REPRS.get(type_code, repr(type_code)) def value_repr(val): if str(type(val)) == \"<java class 'java.math.BigInteger'>\": return str(val) return repr(val) # Connect via Firebolt's JDBC driver def connect_jdbc(): jdbc_url = \"jdbc:firebolt://api.app.firebolt.io/{database}\".format(database=database) jdbc_jar = (jar_path) try: # connect to Firebolt and load driver conn = jaydebeapi.connect(\"com.firebolt.FireboltDriver\", jdbc_url, [username, password], jdbc_jar) return conn.cursor() except: raise Exception(\"Failed to connect via JDBC, error info can be found in logs/firebolt-jdbc.log in the directory you saved the script\") cursor = connect_jdbc() # Run the queries for q in queries: print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()) + \" - Executing: \" + q) cursor.execute(q) # print the query results print('Results: ') fetched = cursor.fetchall() if cursor.description else [] if cursor.description and fetched: # This line outputs the names of the output fields actual_result = \",\".join([repr(x[0]) for x in cursor.description]) + \"\\n\" # This line outputs the types of the output fields actual_result += \",\".join([type_code_repr(x[1]) for x in cursor.description]) + \"\\n\" # This line outputs the data themselves actual_result += \"\\n\".join(map(lambda line: \",\".join(map(value_repr, line)), fetched)) else: actual_result = \"none\" print(actual_result)","title":"Connect to your database programmatically"},{"location":"connecting-to-firebolt/connect-to-your-database-programmatically/#connect-to-your-database-programmatically","text":"This section explains how to connect to your database programmatically. If you are using an application like DBeaver that manages your client connections for you, then you can skip this section and move directly to this guide .","title":"Connect to your database programmatically"},{"location":"connecting-to-firebolt/connect-to-your-database-programmatically/#connecting-a-database-by-using-python","text":"Connecting to your database in Firebolt requires authentication. Following is an example that demonstrates how to connect to your database using Python. Prepare in advance: Firebolt username Firebolt password Database name Engine name (optional)","title":"Connecting a database by using Python"},{"location":"connecting-to-firebolt/connect-to-your-database-programmatically/#example-connect-to-a-database-by-using-python","text":"Requirements: The JayDeBeApi module should be installed. Use Python 3.0 and above. Download Firebolt's latest JDBC driver from here . Fill in the relevant params in lines 15 - 25. import time import pprint try: import jaydebeapi except: raise Exception(\"Failed to import jaydebeapi, try the following: sudo pip3 install JayDeBeApi\") # A list of queries to run. Each query wrapped by \"\" queries = [\"\"\" provide the first query \"\"\", \"\"\" provide the second query \"\"\"] # Username, e.g: 'user@company.com' username = 'provide the user name' # Password, e.g: 'mypassword' password = 'provide the password' # Database, e.g: 'my_db' database = 'provide the database name' # Jar path, e.g: /users/john/jar/firebolt-jdbc-1.07-jar-with-dependencies.jar' jar_path = \"provide the path to Firebolt's jar\" # Printing the results def type_code_repr(type_code: jaydebeapi.DBAPITypeObject) -> str: SHORTER_REPRS = dict( [ (jaydebeapi.BINARY, \"BINARY\"), (jaydebeapi.DATE, \"DATE\"), (jaydebeapi.DATETIME, \"DATETIME\"), (jaydebeapi.DECIMAL, \"DECIMAL\"), (jaydebeapi.FLOAT, \"FLOAT\"), (jaydebeapi.NUMBER, \"NUMBER\"), (jaydebeapi.ROWID, \"ROWID\"), (jaydebeapi.STRING, \"STRING\"), (jaydebeapi.TEXT, \"TEXT\"), (jaydebeapi.TIME, \"TIME\"), ] ) return SHORTER_REPRS.get(type_code, repr(type_code)) def value_repr(val): if str(type(val)) == \"<java class 'java.math.BigInteger'>\": return str(val) return repr(val) # Connect via Firebolt's JDBC driver def connect_jdbc(): jdbc_url = \"jdbc:firebolt://api.app.firebolt.io/{database}\".format(database=database) jdbc_jar = (jar_path) try: # connect to Firebolt and load driver conn = jaydebeapi.connect(\"com.firebolt.FireboltDriver\", jdbc_url, [username, password], jdbc_jar) return conn.cursor() except: raise Exception(\"Failed to connect via JDBC, error info can be found in logs/firebolt-jdbc.log in the directory you saved the script\") cursor = connect_jdbc() # Run the queries for q in queries: print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()) + \" - Executing: \" + q) cursor.execute(q) # print the query results print('Results: ') fetched = cursor.fetchall() if cursor.description else [] if cursor.description and fetched: # This line outputs the names of the output fields actual_result = \",\".join([repr(x[0]) for x in cursor.description]) + \"\\n\" # This line outputs the types of the output fields actual_result += \",\".join([type_code_repr(x[1]) for x in cursor.description]) + \"\\n\" # This line outputs the data themselves actual_result += \"\\n\".join(map(lambda line: \",\".join(map(value_repr, line)), fetched)) else: actual_result = \"none\" print(actual_result)","title":"Example: connect to a database by using Python"},{"location":"connecting-to-firebolt/connecting-via-jdbc/","text":"Firebolt JDBC driver Firebolt provides a JDBC driver (Type 4) that can be used with most applications and tools that support JDBC for connecting to a database server. The driver is provided as a JAR file and requires Java 1.8 (or higher). Downloading the driver Use one of the following links to download Firebolt's JDBC driver: Firebolt generic JDBC driver You can use this driver to connect to Firebolt via Airflow , Tableau , DBeaver , and more. Firebolt JDBC driver for Looker Learn more on how to connect to Firebolt via Looker . Before downloading the driver make sure you read the Firebolt JDBC license terms . Related topics Setting up Airflow JDBC to Firebolt Setting up Looker JDBC to Firebolt Setting up Tableau (Desktop) JDBC to Firebolt Setting up DBeaver JDBC connection to Firebolt","title":"Firebolt JDBC driver"},{"location":"connecting-to-firebolt/connecting-via-jdbc/#firebolt-jdbc-driver","text":"Firebolt provides a JDBC driver (Type 4) that can be used with most applications and tools that support JDBC for connecting to a database server. The driver is provided as a JAR file and requires Java 1.8 (or higher).","title":"Firebolt JDBC driver"},{"location":"connecting-to-firebolt/connecting-via-jdbc/#downloading-the-driver","text":"Use one of the following links to download Firebolt's JDBC driver: Firebolt generic JDBC driver You can use this driver to connect to Firebolt via Airflow , Tableau , DBeaver , and more. Firebolt JDBC driver for Looker Learn more on how to connect to Firebolt via Looker . Before downloading the driver make sure you read the Firebolt JDBC license terms .","title":"Downloading the driver"},{"location":"connecting-to-firebolt/connecting-via-jdbc/#related-topics","text":"Setting up Airflow JDBC to Firebolt Setting up Looker JDBC to Firebolt Setting up Tableau (Desktop) JDBC to Firebolt Setting up DBeaver JDBC connection to Firebolt","title":"Related topics"},{"location":"connecting-to-firebolt/connecting-via-rest-api/","text":"Firebolt REST API Firebolt provides several endpoints that enable you to interact with Firebolt programmatically. This topic contains an overview of the usage details when using the public REST endpoints to ingest data into Firebolt and run analytic queries. In this topic: Authentication Ingest data using the REST API Running queries using the REST API Authentication Authenticating with Firebolt REST API requires to retrieve authentication token using the auth endpoint. Retrieve authentication token The authentication token is a secret string that identifies your user. Perform the following request to get an authentication token: curl --request POST 'https://api.app.firebolt.io/auth/v1/login' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"username\":\"YOUR_USER_EMAIL\",\"password\":\"YOUR_PASSWORD\"}' In response you will get a JSON with similar contents: { \"access_token\": \"YOUR_ACCESS_TOKEN_VALUE\", \"expires_in\": 86400, \"refresh_token\": \"YOUR_REFRESH_TOKEN_VALUE\", \"scope\": \"offline_access\", \"token_type\": \"Bearer\" } Extract the access_token value since it will be required in the following requests and will be used as your authentication token. Notice The access_token is valid for 24 hours Refreshing access tokens When the access_token expires and you start getting 401 HTTP errors from our API, you can either just repeat the login process or use refresh-token endpoint: curl --request POST 'https://api.app.firebolt.io/auth/v1/refresh' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"refresh_token\":\"YOUR_REFRESH_TOKEN_VALUE\"}' The response from this endpoint will be the same as from the login endpoint. Ingesting data using the REST API Data ingestion into Firebolt using REST API requires the following steps: Get you engine's URL Create an external table Create a fact table and import data Get your engine's URL When you create a database in Firebolt the next step is creating an engine and attach it to the database. The engine is composed of a physical cluster of machines that will serve requests for this database and a load balancer in front of this cluster. Each database can have many engines attached to it and one engine configured as a default engine. You need to query our API to retrieve the URL of the default engine. Retrieving the URL of the default engine in your database: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getURLByDatabaseName?database_name=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"engineUrl\": \"YOUR_ENGINE_URL\" } Retrieving the URL of your engine by its name: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines?filter.name_contains=YOUR_ENGINE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"page\": { ... }, \"edges\": [ { ... \"endpoint\": \"YOUR_ENGINE_URL\", ... } } ] } Create an external table Use the following request to create an external table: echo \"CREATE_EXTERNAL_TABLE_SCRIPT\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \"Exchange your database name to engine URL\" section YOUR_DATABASE_NAME with the name of the database CREATE_EXTERNAL_TABLE_SCRIPT with the following SQL script: CREATE EXTERNAL TABLE [ IF NOT EXISTS ] <external_table> ( <col_name> <col_type> [ , ... ]) URL = 's3://<path_to_s3_objects>' [ CREDENTIALS = ( AWS_KEY_ID = '******' AWS_SECRET_KEY = '******' ) ] OBJECT_PATTERN = <pattern_regex> TYPE = ( { CSV | JSON | PARQUET } ); In the above script replace the following: <external_table> with the desired external table name you would like to create <col_name> <col_type> with the relevant column names and types <path_to_s3_objects> with the path to the new files <pattern_regex> with a pattern that identifies the files inside the bucket For example - for the following file name: \u201cfilename.parquet\u201d - the following should be used as <pattern_regex> : \u2018*.parquet\u2019 Create FACT Table and Import Data Create FACT table Use the following request to create a fact table: echo \"CREATE_FACT_TABLE_SCRIPT\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \u201cExchange your database name to engine URL\u201d section YOUR_DATABASE_NAME with the name of the database CREATE_FACT_TABLE_SCRIPT with the following SQL script: CREATE FACT TABLE [IF NOT EXISTS] <fact_table> ( <column_name> <column_type> [ ,... ] ) PRIMARY INDEX <column_list> In the above script, replace the following: <fact_table> with the desired fact table name you would like to create <column_name> <column_type> with the relevant column names and types <column_list> with a list (column names separated by commas, for example: column1, column2) of columns which should be used as a primary index (read more about primary indexes here). Notice It is worth considering configuring an aggregating index as well before importing the data to the fact table to boost performance even further (it is also possible to create the aggregating index in a later stage). Read more about aggregating indexes here . Import Data into the FACT TABLE Use the following request to import data into your fact table: echo \"IMPORT_SCRIPT\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \u201cExchange your database name to engine URL\u201d section. YOUR_DATABASE_NAME with the name of the database IMPORT_SCRIPT with the following SQL script: INSERT INTO <fact_table> SELECT * FROM <external_table> In the above script, replace: <fact_table> with your Firebolt table name <external_table> with your external table name from which you\u2019d like to ingest Running queries using the REST API Running queries using REST API requires the following steps: Get your engine's URL Run your queries Get your engine's URL When you create a database in Firebolt the next step is creating an engine and attach it to the database. The engine is composed of a physical cluster of machines that will serve requests for this database and a load balancer in front of this cluster. Each database can have many engines attached to it and one engine configured as a default engine. You need to query our API to retrieve the URL of the default engine. Retrieving the URL of the default engine in your database: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getURLByDatabaseName?database_name=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"engine_url\": \"YOUR_ENGINE_URL\" } Retrieving the URL of your engine by it's name: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines?filter.name_contains=YOUR_ENGINE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"page\": { ... }, \"edges\": [ { ... \"endpoint\": \"YOUR_ENGINE_URL\", ... } } ] } Run your Queries Use the following request to run your queries: echo \"SELECT_QUERY\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \u201cExchange your database name to engine URL\u201d section YOUR_DATABASE_NAME with the name of the database SELECT_QUERY with any select query, you would like to run. It is possible to have multiple queries run at the same time. Just add \u201c;\u201d after each query - for example, replace SELECT_QUERY with the following: SELECT_QUERY_1; SELECT_QUERY_2; --more queries... SELECT_QUERY_N;","title":"Firebolt REST API"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#firebolt-rest-api","text":"Firebolt provides several endpoints that enable you to interact with Firebolt programmatically. This topic contains an overview of the usage details when using the public REST endpoints to ingest data into Firebolt and run analytic queries.","title":"Firebolt REST API"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#in-this-topic","text":"Authentication Ingest data using the REST API Running queries using the REST API","title":"In this topic:"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#authentication","text":"Authenticating with Firebolt REST API requires to retrieve authentication token using the auth endpoint.","title":"Authentication"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#retrieve-authentication-token","text":"The authentication token is a secret string that identifies your user. Perform the following request to get an authentication token: curl --request POST 'https://api.app.firebolt.io/auth/v1/login' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"username\":\"YOUR_USER_EMAIL\",\"password\":\"YOUR_PASSWORD\"}' In response you will get a JSON with similar contents: { \"access_token\": \"YOUR_ACCESS_TOKEN_VALUE\", \"expires_in\": 86400, \"refresh_token\": \"YOUR_REFRESH_TOKEN_VALUE\", \"scope\": \"offline_access\", \"token_type\": \"Bearer\" } Extract the access_token value since it will be required in the following requests and will be used as your authentication token.","title":"Retrieve authentication token"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#notice","text":"The access_token is valid for 24 hours","title":"Notice"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#refreshing-access-tokens","text":"When the access_token expires and you start getting 401 HTTP errors from our API, you can either just repeat the login process or use refresh-token endpoint: curl --request POST 'https://api.app.firebolt.io/auth/v1/refresh' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"refresh_token\":\"YOUR_REFRESH_TOKEN_VALUE\"}' The response from this endpoint will be the same as from the login endpoint.","title":"Refreshing access tokens"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#ingesting-data-using-the-rest-api","text":"Data ingestion into Firebolt using REST API requires the following steps: Get you engine's URL Create an external table Create a fact table and import data","title":"Ingesting data using the REST API"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#get-your-engines-url","text":"When you create a database in Firebolt the next step is creating an engine and attach it to the database. The engine is composed of a physical cluster of machines that will serve requests for this database and a load balancer in front of this cluster. Each database can have many engines attached to it and one engine configured as a default engine. You need to query our API to retrieve the URL of the default engine. Retrieving the URL of the default engine in your database: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getURLByDatabaseName?database_name=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"engineUrl\": \"YOUR_ENGINE_URL\" } Retrieving the URL of your engine by its name: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines?filter.name_contains=YOUR_ENGINE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"page\": { ... }, \"edges\": [ { ... \"endpoint\": \"YOUR_ENGINE_URL\", ... } } ] }","title":"Get your engine's URL"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#create-an-external-table","text":"Use the following request to create an external table: echo \"CREATE_EXTERNAL_TABLE_SCRIPT\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \"Exchange your database name to engine URL\" section YOUR_DATABASE_NAME with the name of the database CREATE_EXTERNAL_TABLE_SCRIPT with the following SQL script: CREATE EXTERNAL TABLE [ IF NOT EXISTS ] <external_table> ( <col_name> <col_type> [ , ... ]) URL = 's3://<path_to_s3_objects>' [ CREDENTIALS = ( AWS_KEY_ID = '******' AWS_SECRET_KEY = '******' ) ] OBJECT_PATTERN = <pattern_regex> TYPE = ( { CSV | JSON | PARQUET } ); In the above script replace the following: <external_table> with the desired external table name you would like to create <col_name> <col_type> with the relevant column names and types <path_to_s3_objects> with the path to the new files <pattern_regex> with a pattern that identifies the files inside the bucket For example - for the following file name: \u201cfilename.parquet\u201d - the following should be used as <pattern_regex> : \u2018*.parquet\u2019","title":"Create an external table"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#create-fact-table-and-import-data","text":"Create FACT table Use the following request to create a fact table: echo \"CREATE_FACT_TABLE_SCRIPT\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \u201cExchange your database name to engine URL\u201d section YOUR_DATABASE_NAME with the name of the database CREATE_FACT_TABLE_SCRIPT with the following SQL script: CREATE FACT TABLE [IF NOT EXISTS] <fact_table> ( <column_name> <column_type> [ ,... ] ) PRIMARY INDEX <column_list> In the above script, replace the following: <fact_table> with the desired fact table name you would like to create <column_name> <column_type> with the relevant column names and types <column_list> with a list (column names separated by commas, for example: column1, column2) of columns which should be used as a primary index (read more about primary indexes here).","title":"Create FACT Table and Import Data"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#notice_1","text":"It is worth considering configuring an aggregating index as well before importing the data to the fact table to boost performance even further (it is also possible to create the aggregating index in a later stage). Read more about aggregating indexes here . Import Data into the FACT TABLE Use the following request to import data into your fact table: echo \"IMPORT_SCRIPT\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \u201cExchange your database name to engine URL\u201d section. YOUR_DATABASE_NAME with the name of the database IMPORT_SCRIPT with the following SQL script: INSERT INTO <fact_table> SELECT * FROM <external_table> In the above script, replace: <fact_table> with your Firebolt table name <external_table> with your external table name from which you\u2019d like to ingest","title":"Notice"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#running-queries-using-the-rest-api","text":"Running queries using REST API requires the following steps: Get your engine's URL Run your queries","title":"Running queries using the REST API"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#get-your-engines-url_1","text":"When you create a database in Firebolt the next step is creating an engine and attach it to the database. The engine is composed of a physical cluster of machines that will serve requests for this database and a load balancer in front of this cluster. Each database can have many engines attached to it and one engine configured as a default engine. You need to query our API to retrieve the URL of the default engine. Retrieving the URL of the default engine in your database: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getURLByDatabaseName?database_name=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"engine_url\": \"YOUR_ENGINE_URL\" } Retrieving the URL of your engine by it's name: curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines?filter.name_contains=YOUR_ENGINE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Which results with: { \"page\": { ... }, \"edges\": [ { ... \"endpoint\": \"YOUR_ENGINE_URL\", ... } } ] }","title":"Get your engine's URL"},{"location":"connecting-to-firebolt/connecting-via-rest-api/#run-your-queries","text":"Use the following request to run your queries: echo \"SELECT_QUERY\" | curl 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Make sure to replace the following params: URL_TO_CONNECT with the value of \"engineUrl\" in the result JSON of the query run in the \u201cExchange your database name to engine URL\u201d section YOUR_DATABASE_NAME with the name of the database SELECT_QUERY with any select query, you would like to run. It is possible to have multiple queries run at the same time. Just add \u201c;\u201d after each query - for example, replace SELECT_QUERY with the following: SELECT_QUERY_1; SELECT_QUERY_2; --more queries... SELECT_QUERY_N;","title":"Run your Queries"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/","text":"Setting up Airflow JDBC to Firebolt Interested in continuously loading data into Firebolt? See our continuously loading data tutorial . Step 1: Install the latest Firebolt JDBC Driver Download Firebolt\u2019s JDBC driver from here . Put the JDBC jar file in the server which runs Airflow (we have placed it under /airflow/jdbc ). Step 2: Setup the JDBC connection in Airflow Pre-requisites Make sure you have: The name of the database you would like to connect to in Firebolt. The username and password used to log into Firebolt. Airflow version 1.10.12 and above. Configuring the Connection Open Airflow UI. In the Admin tab click on Connections. Click + to create a new connection to Firebolt. Configure the following parameters: Parameter Description Value Conn Id The connection identifier For example: firebolt_jdbc Conn Type The connection type JDBC Connection Connection URL The connection String URL jdbc:firebolt://api.app.firebolt.io/<db_name> Make sure to replace <db_name> with the name of your database in Firebolt. This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Login Your Firebolt username Password Your Firebolt password Driver Path The full path in your Airflow server in which you've stored Firebolt's JDBC driver In our case: /airflow/jdbc/firebolt-jdbc-1.03-jar-with-dependencies.jar Driver Class The class of the JDBC driver com.firebolt.FireboltDriver Click on Save . Step 3: Create a DAG We will create a DAG that runs a script on top of a Firebolt table. Bellow is the Python DAG program: from airflow import DAG from airflow.operators.jdbc_operator import JdbcOperator default_arg = {'owner': 'airflow', 'start_date': '2020-10-20'} dag = DAG('firebolt_dag', default_args=default_arg, schedule_interval=None) firebolt_task = JdbcOperator(dag=dag,jdbc_conn_id='firebolt_db',task_id='firebolt_sql_task',sql=['query_sample.sql']) firebolt_task You can save it as a Python file, and place it under your dags folder to run it in Airflow. We use Airflow's JDBC operator to connect to Firebolt via JDBC and run a SQL script. The SQL script file contains a simple SELECT query. Feel free to use any query you want. Step 4: Run the DAG In Airflow's UI, go to the DAGs tab. Locate your DAG in the list (in our case we should look for 'firebolt_dag' : Click on the trigger button under Links to manually trigger the DAG. Once the DAG has started to run, click on it's Run Id to move to the graph view to track its progress. In our DAG we have a single step called 'firebolt_sql_task' . In the DAG's graph view, the task should appear in green to confirm the DAG was completed successfully. Click on the task 'firebolt_sql_task' : Click on View Logs to inspect the logs.","title":"Setting up Airflow JDBC to Firebolt"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/#setting-up-airflow-jdbc-to-firebolt","text":"Interested in continuously loading data into Firebolt? See our continuously loading data tutorial .","title":"Setting up Airflow JDBC to Firebolt"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/#step-1-install-the-latest-firebolt-jdbc-driver","text":"Download Firebolt\u2019s JDBC driver from here . Put the JDBC jar file in the server which runs Airflow (we have placed it under /airflow/jdbc ).","title":"Step 1: Install the latest Firebolt JDBC Driver"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/#step-2-setup-the-jdbc-connection-in-airflow","text":"","title":"Step 2: Setup the JDBC connection in Airflow"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/#pre-requisites","text":"Make sure you have: The name of the database you would like to connect to in Firebolt. The username and password used to log into Firebolt. Airflow version 1.10.12 and above.","title":"Pre-requisites"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/#configuring-the-connection","text":"Open Airflow UI. In the Admin tab click on Connections. Click + to create a new connection to Firebolt. Configure the following parameters: Parameter Description Value Conn Id The connection identifier For example: firebolt_jdbc Conn Type The connection type JDBC Connection Connection URL The connection String URL jdbc:firebolt://api.app.firebolt.io/<db_name> Make sure to replace <db_name> with the name of your database in Firebolt. This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Login Your Firebolt username Password Your Firebolt password Driver Path The full path in your Airflow server in which you've stored Firebolt's JDBC driver In our case: /airflow/jdbc/firebolt-jdbc-1.03-jar-with-dependencies.jar Driver Class The class of the JDBC driver com.firebolt.FireboltDriver Click on Save .","title":"Configuring the Connection"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/#step-3-create-a-dag","text":"We will create a DAG that runs a script on top of a Firebolt table. Bellow is the Python DAG program: from airflow import DAG from airflow.operators.jdbc_operator import JdbcOperator default_arg = {'owner': 'airflow', 'start_date': '2020-10-20'} dag = DAG('firebolt_dag', default_args=default_arg, schedule_interval=None) firebolt_task = JdbcOperator(dag=dag,jdbc_conn_id='firebolt_db',task_id='firebolt_sql_task',sql=['query_sample.sql']) firebolt_task You can save it as a Python file, and place it under your dags folder to run it in Airflow. We use Airflow's JDBC operator to connect to Firebolt via JDBC and run a SQL script. The SQL script file contains a simple SELECT query. Feel free to use any query you want.","title":"Step 3: Create a DAG"},{"location":"connecting-to-firebolt/setting-up-airflow-jdbc-to-firebolt/#step-4-run-the-dag","text":"In Airflow's UI, go to the DAGs tab. Locate your DAG in the list (in our case we should look for 'firebolt_dag' : Click on the trigger button under Links to manually trigger the DAG. Once the DAG has started to run, click on it's Run Id to move to the graph view to track its progress. In our DAG we have a single step called 'firebolt_sql_task' . In the DAG's graph view, the task should appear in green to confirm the DAG was completed successfully. Click on the task 'firebolt_sql_task' : Click on View Logs to inspect the logs.","title":"Step 4: Run the DAG"},{"location":"connecting-to-firebolt/setting-up-dbeaver-jdbc-connection-to-firebolt/","text":"Setting up DBeaver JDBC connection to Firebolt Adding driver configuration in DBeaver Download Firebolt JDBC driver from here . Open the driver manager dialog: From the top navigation menu click on Database > Driver Manager . Click on New to add a new driver: In order to add the driver, fill in the following parameters: Driver Name: Firebolt . Class Name: com.firebolt.FireboltDriver . Under Libraries , click Add File and choose the driver downloaded in section 1 above. Click OK for completing adding the driver. Adding a database in DBeaver From the top navigation menu click on Database > New Database Connection . Type Firebolt in the search box. Select Firebolt from the list of databases. Click Next . Fill in the following parameters: JDBC URL: jdbc:firebolt://api.app.firebolt.io/<db_name> Where <db_name> should be replaced with the name of your database in Firebolt. This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Username: your FIrebolt username. Password: your Firebolt password. Click on Test Connection (make sure to start your database before you start the test). A successful connection test looks as follows: Click Finish . Querying your Firebolt database In the database navigator, right-click on the database connection and click on SQL Editor . In-case a pop-up appears click on New Script : The SQL editor is being opened and you can run your queries.","title":"Setting up DBeaver JDBC connection to Firebolt"},{"location":"connecting-to-firebolt/setting-up-dbeaver-jdbc-connection-to-firebolt/#setting-up-dbeaver-jdbc-connection-to-firebolt","text":"","title":"Setting up DBeaver JDBC connection to Firebolt"},{"location":"connecting-to-firebolt/setting-up-dbeaver-jdbc-connection-to-firebolt/#adding-driver-configuration-in-dbeaver","text":"Download Firebolt JDBC driver from here . Open the driver manager dialog: From the top navigation menu click on Database > Driver Manager . Click on New to add a new driver: In order to add the driver, fill in the following parameters: Driver Name: Firebolt . Class Name: com.firebolt.FireboltDriver . Under Libraries , click Add File and choose the driver downloaded in section 1 above. Click OK for completing adding the driver.","title":"Adding driver configuration in DBeaver"},{"location":"connecting-to-firebolt/setting-up-dbeaver-jdbc-connection-to-firebolt/#adding-a-database-in-dbeaver","text":"From the top navigation menu click on Database > New Database Connection . Type Firebolt in the search box. Select Firebolt from the list of databases. Click Next . Fill in the following parameters: JDBC URL: jdbc:firebolt://api.app.firebolt.io/<db_name> Where <db_name> should be replaced with the name of your database in Firebolt. This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Username: your FIrebolt username. Password: your Firebolt password. Click on Test Connection (make sure to start your database before you start the test). A successful connection test looks as follows: Click Finish .","title":"Adding a database in DBeaver"},{"location":"connecting-to-firebolt/setting-up-dbeaver-jdbc-connection-to-firebolt/#querying-your-firebolt-database","text":"In the database navigator, right-click on the database connection and click on SQL Editor . In-case a pop-up appears click on New Script : The SQL editor is being opened and you can run your queries.","title":"Querying your Firebolt database"},{"location":"connecting-to-firebolt/setting-up-looker-jdbc-to-firebolt/","text":"Setting up Looker JDBC to Firebolt This topic describes the steps for setting up the Looker JDBC connection to Firebolt. Step 1: Add Firebolt\u2019s JDBC driver to Looker In order to make looker work with Firebolt, follow the instructions in https://docs.looker.com/setup-and-management/database-config/custom_jdbc_drivers while paying attention to the following: Download Firebolt\u2019s JDBC driver for Looker from here (make sure you download the Firebolt JDBC driver for Looker). Put FItebolt\u2019s JDBC driver for Looker here: /home/looker/looker/custom_jdbc_drivers/athena/. If you have any other Firebolt driver there, just remove it, and replace it with the one you\u2019ve just downloaded. Create the following file under:/home/looker/looker/custom_jdbc_config.yml (if you already have this file with the following settings, move on to the next step) and paste the following into it: Make sure to replace <version> with the JDBC version number. For example - in the following JDBC: \"firebolt-jdbc-1.03-jar-looker-with-dependencies.jar\" <version> needs to be replaced with 1.03. yaml - name: athena file_name: athena/firebolt-jdbc-<version>-jar-looker-with-dependencies.jar module_path: com.firebolt.FireboltDriver override_jdbc_url_subprotocol: firebolt Restart looker with the following parameter: text LOOKERARGS=\"--use-custom-jdbc-config\" Step 2: Configure a Connection to Firebolt Pre-requisites Make a note of the database name you would like to connect to in Firebolt. Also, make sure the Database has an up and running engine before you configure the connection in Looker. Make a note of the username and password used to log into Firebolt. Configuring the Connection In Looker\u2019s UI, Select Admin > Connections (under Database ). On the Connections page, click New Connection . Click on Existing Database The following popup appears: Fill in the following parameters: Parameter Description Value Name the connection name in Looker For example: Connection Dialect The SQL Dialect Amazon Athena Host:Port The username used for connecting to FIrbolt api.app.firebolt.io:443 Database The database name in Firebolt. Username The username used for connecting to FIrebolt Your Firebolt username Password The password used for connecting to Firebolt Your FIrebolt password Additional Params Leave blank to query the database using its default engine. If you wish to use another engine, set the following parameter: engine=engineName Replace engineName with the name of the engine you would like to use. Click on the \u2018Test These Settings\u2019 button. A successful connection test results in a success message, similar to this:","title":"Setting up Looker JDBC to Firebolt"},{"location":"connecting-to-firebolt/setting-up-looker-jdbc-to-firebolt/#setting-up-looker-jdbc-to-firebolt","text":"This topic describes the steps for setting up the Looker JDBC connection to Firebolt.","title":"Setting up Looker JDBC to Firebolt"},{"location":"connecting-to-firebolt/setting-up-looker-jdbc-to-firebolt/#step-1-add-firebolts-jdbc-driver-to-looker","text":"In order to make looker work with Firebolt, follow the instructions in https://docs.looker.com/setup-and-management/database-config/custom_jdbc_drivers while paying attention to the following: Download Firebolt\u2019s JDBC driver for Looker from here (make sure you download the Firebolt JDBC driver for Looker). Put FItebolt\u2019s JDBC driver for Looker here: /home/looker/looker/custom_jdbc_drivers/athena/. If you have any other Firebolt driver there, just remove it, and replace it with the one you\u2019ve just downloaded. Create the following file under:/home/looker/looker/custom_jdbc_config.yml (if you already have this file with the following settings, move on to the next step) and paste the following into it: Make sure to replace <version> with the JDBC version number. For example - in the following JDBC: \"firebolt-jdbc-1.03-jar-looker-with-dependencies.jar\" <version> needs to be replaced with 1.03. yaml - name: athena file_name: athena/firebolt-jdbc-<version>-jar-looker-with-dependencies.jar module_path: com.firebolt.FireboltDriver override_jdbc_url_subprotocol: firebolt Restart looker with the following parameter: text LOOKERARGS=\"--use-custom-jdbc-config\"","title":"Step 1: Add Firebolt\u2019s JDBC driver to Looker"},{"location":"connecting-to-firebolt/setting-up-looker-jdbc-to-firebolt/#step-2-configure-a-connection-to-firebolt","text":"","title":"Step 2: Configure a Connection to Firebolt"},{"location":"connecting-to-firebolt/setting-up-looker-jdbc-to-firebolt/#pre-requisites","text":"Make a note of the database name you would like to connect to in Firebolt. Also, make sure the Database has an up and running engine before you configure the connection in Looker. Make a note of the username and password used to log into Firebolt.","title":"Pre-requisites"},{"location":"connecting-to-firebolt/setting-up-looker-jdbc-to-firebolt/#configuring-the-connection","text":"In Looker\u2019s UI, Select Admin > Connections (under Database ). On the Connections page, click New Connection . Click on Existing Database The following popup appears: Fill in the following parameters: Parameter Description Value Name the connection name in Looker For example: Connection Dialect The SQL Dialect Amazon Athena Host:Port The username used for connecting to FIrbolt api.app.firebolt.io:443 Database The database name in Firebolt. Username The username used for connecting to FIrebolt Your Firebolt username Password The password used for connecting to Firebolt Your FIrebolt password Additional Params Leave blank to query the database using its default engine. If you wish to use another engine, set the following parameter: engine=engineName Replace engineName with the name of the engine you would like to use. Click on the \u2018Test These Settings\u2019 button. A successful connection test results in a success message, similar to this:","title":"Configuring the Connection"},{"location":"connecting-to-firebolt/setting-up-tableau-desktop-jdbc-to-firebolt/","text":"Setting up Tableau (Desktop) JDBC to Firebolt This topic describes the steps for setting up the Tableau (Desktop) JDBC connection to Firebolt. Step 1: Install the latest Firebolt JDBC Driver Download Firebolt\u2019s JDBC driver for Tableau from here . Put the JDBC jar file in the Tableau JDBC driver folder: Windows: C:\\Program Files\\Tableau\\Drivers Mac: ~/Library/Tableau/Drivers Step 2: Setup the JDBC connection in Tableau Pre-requisites Make a note of the database name you would like to connect to in Firebolt. Also, make sure the Database has an up and running engine before you configure the connection in Looker. Make a note of the username and password used to log into Firebolt. Configuring the Connection In Tableau's UI under connections click on 'Other Databases (JDBC)\u2019 The following popup appears: Fill in the following parameters: Parameter Description Value URL The connection string URL jdbc:firebolt://api.app.firebolt.io/<db name> This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Dialect The SQL Dialect PostgreSQL Username The username used for connecting to FIrbolt Your Firebolt username Password The password used for connecting to Firebolt Your Firebolt password Properties File Configures additional Tableau properties. If you wish to change any of these properties, edit the jdbc-firebolt.tdc file in your favorite text editor before uploading it (not needed in most cases) jdbc-firebolt.tdc","title":"Setting up Tableau \\(Desktop\\) JDBC to Firebolt"},{"location":"connecting-to-firebolt/setting-up-tableau-desktop-jdbc-to-firebolt/#setting-up-tableau-desktop-jdbc-to-firebolt","text":"This topic describes the steps for setting up the Tableau (Desktop) JDBC connection to Firebolt.","title":"Setting up Tableau (Desktop) JDBC to Firebolt"},{"location":"connecting-to-firebolt/setting-up-tableau-desktop-jdbc-to-firebolt/#step-1-install-the-latest-firebolt-jdbc-driver","text":"Download Firebolt\u2019s JDBC driver for Tableau from here . Put the JDBC jar file in the Tableau JDBC driver folder: Windows: C:\\Program Files\\Tableau\\Drivers Mac: ~/Library/Tableau/Drivers","title":"Step 1: Install the latest Firebolt JDBC Driver"},{"location":"connecting-to-firebolt/setting-up-tableau-desktop-jdbc-to-firebolt/#step-2-setup-the-jdbc-connection-in-tableau","text":"","title":"Step 2: Setup the JDBC connection in Tableau"},{"location":"connecting-to-firebolt/setting-up-tableau-desktop-jdbc-to-firebolt/#pre-requisites","text":"Make a note of the database name you would like to connect to in Firebolt. Also, make sure the Database has an up and running engine before you configure the connection in Looker. Make a note of the username and password used to log into Firebolt.","title":"Pre-requisites"},{"location":"connecting-to-firebolt/setting-up-tableau-desktop-jdbc-to-firebolt/#configuring-the-connection","text":"In Tableau's UI under connections click on 'Other Databases (JDBC)\u2019 The following popup appears: Fill in the following parameters: Parameter Description Value URL The connection string URL jdbc:firebolt://api.app.firebolt.io/<db name> This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Dialect The SQL Dialect PostgreSQL Username The username used for connecting to FIrbolt Your Firebolt username Password The password used for connecting to Firebolt Your Firebolt password Properties File Configures additional Tableau properties. If you wish to change any of these properties, edit the jdbc-firebolt.tdc file in your favorite text editor before uploading it (not needed in most cases) jdbc-firebolt.tdc","title":"Configuring the Connection"},{"location":"developing-with-firebolt/connect-to-your-database-programmatically/","text":"Connecting to your database programmatically This section explains how to connect to your database programmatically. If you are using an application like DBeaver that manages your client connections for you, then you can skip this section and move directly to this guide . Connecting a database by using Python Connecting to your database in Firebolt requires authentication. Following is an example that demonstrates how to connect to your database using Python. Prepare in advance: Firebolt username Firebolt password Database name Engine name (optional) Example: connect to a database by using Python Requirements: The JayDeBeApi module should be installed. Use Python 3.0 and above. Download Firebolt's latest JDBC driver from here . Fill in the relevant params in lines 15 - 25. import time import pprint try: import jaydebeapi except: raise Exception(\"Failed to import jaydebeapi, try the following: sudo pip3 install JayDeBeApi\") # A list of queries to run. Each query wrapped by \"\" queries = [\"\"\" provide the first query \"\"\", \"\"\" provide the second query \"\"\"] # Username, e.g: 'user@company.com' username = 'provide the user name' # Password, e.g: 'mypassword' password = 'provide the password' # Database, e.g: 'my_db' database = 'provide the database name' # Jar path, e.g: /users/john/jar/firebolt-jdbc-1.07-jar-with-dependencies.jar' jar_path = \"provide the path to Firebolt's jar\" # Printing the results def type_code_repr(type_code: jaydebeapi.DBAPITypeObject) -> str: SHORTER_REPRS = dict( [ (jaydebeapi.BINARY, \"BINARY\"), (jaydebeapi.DATE, \"DATE\"), (jaydebeapi.DATETIME, \"DATETIME\"), (jaydebeapi.DECIMAL, \"DECIMAL\"), (jaydebeapi.FLOAT, \"FLOAT\"), (jaydebeapi.NUMBER, \"NUMBER\"), (jaydebeapi.ROWID, \"ROWID\"), (jaydebeapi.STRING, \"STRING\"), (jaydebeapi.TEXT, \"TEXT\"), (jaydebeapi.TIME, \"TIME\"), ] ) return SHORTER_REPRS.get(type_code, repr(type_code)) def value_repr(val): if str(type(val)) == \"<java class 'java.math.BigInteger'>\": return str(val) return repr(val) # Connect via Firebolt's JDBC driver def connect_jdbc(): jdbc_url = \"jdbc:firebolt://api.app.firebolt.io/{database}\".format(database=database) jdbc_jar = (jar_path) try: # connect to Firebolt and load driver conn = jaydebeapi.connect(\"com.firebolt.FireboltDriver\", jdbc_url, [username, password], jdbc_jar) return conn.cursor() except: raise Exception(\"Failed to connect via JDBC, error info can be found in logs/firebolt-jdbc.log in the directory you saved the script\") cursor = connect_jdbc() # Run the queries for q in queries: print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()) + \" - Executing: \" + q) cursor.execute(q) # print the query results print('Results: ') fetched = cursor.fetchall() if cursor.description else [] if cursor.description and fetched: # This line outputs the names of the output fields actual_result = \",\".join([repr(x[0]) for x in cursor.description]) + \"\\n\" # This line outputs the types of the output fields actual_result += \",\".join([type_code_repr(x[1]) for x in cursor.description]) + \"\\n\" # This line outputs the data themselves actual_result += \"\\n\".join(map(lambda line: \",\".join(map(value_repr, line)), fetched)) else: actual_result = \"none\" print(actual_result) The JayDeBeApi Python library may require specific data formatting before input. This includes: Datetime objects should be converted to string format More information about potential driver issues can be found on JayDeBeApi Github page .","title":"Connecting to your database programmatically"},{"location":"developing-with-firebolt/connect-to-your-database-programmatically/#connecting-to-your-database-programmatically","text":"This section explains how to connect to your database programmatically. If you are using an application like DBeaver that manages your client connections for you, then you can skip this section and move directly to this guide .","title":"Connecting to your database programmatically"},{"location":"developing-with-firebolt/connect-to-your-database-programmatically/#connecting-a-database-by-using-python","text":"Connecting to your database in Firebolt requires authentication. Following is an example that demonstrates how to connect to your database using Python. Prepare in advance: Firebolt username Firebolt password Database name Engine name (optional)","title":"Connecting a database by using Python"},{"location":"developing-with-firebolt/connect-to-your-database-programmatically/#example-connect-to-a-database-by-using-python","text":"Requirements: The JayDeBeApi module should be installed. Use Python 3.0 and above. Download Firebolt's latest JDBC driver from here . Fill in the relevant params in lines 15 - 25. import time import pprint try: import jaydebeapi except: raise Exception(\"Failed to import jaydebeapi, try the following: sudo pip3 install JayDeBeApi\") # A list of queries to run. Each query wrapped by \"\" queries = [\"\"\" provide the first query \"\"\", \"\"\" provide the second query \"\"\"] # Username, e.g: 'user@company.com' username = 'provide the user name' # Password, e.g: 'mypassword' password = 'provide the password' # Database, e.g: 'my_db' database = 'provide the database name' # Jar path, e.g: /users/john/jar/firebolt-jdbc-1.07-jar-with-dependencies.jar' jar_path = \"provide the path to Firebolt's jar\" # Printing the results def type_code_repr(type_code: jaydebeapi.DBAPITypeObject) -> str: SHORTER_REPRS = dict( [ (jaydebeapi.BINARY, \"BINARY\"), (jaydebeapi.DATE, \"DATE\"), (jaydebeapi.DATETIME, \"DATETIME\"), (jaydebeapi.DECIMAL, \"DECIMAL\"), (jaydebeapi.FLOAT, \"FLOAT\"), (jaydebeapi.NUMBER, \"NUMBER\"), (jaydebeapi.ROWID, \"ROWID\"), (jaydebeapi.STRING, \"STRING\"), (jaydebeapi.TEXT, \"TEXT\"), (jaydebeapi.TIME, \"TIME\"), ] ) return SHORTER_REPRS.get(type_code, repr(type_code)) def value_repr(val): if str(type(val)) == \"<java class 'java.math.BigInteger'>\": return str(val) return repr(val) # Connect via Firebolt's JDBC driver def connect_jdbc(): jdbc_url = \"jdbc:firebolt://api.app.firebolt.io/{database}\".format(database=database) jdbc_jar = (jar_path) try: # connect to Firebolt and load driver conn = jaydebeapi.connect(\"com.firebolt.FireboltDriver\", jdbc_url, [username, password], jdbc_jar) return conn.cursor() except: raise Exception(\"Failed to connect via JDBC, error info can be found in logs/firebolt-jdbc.log in the directory you saved the script\") cursor = connect_jdbc() # Run the queries for q in queries: print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()) + \" - Executing: \" + q) cursor.execute(q) # print the query results print('Results: ') fetched = cursor.fetchall() if cursor.description else [] if cursor.description and fetched: # This line outputs the names of the output fields actual_result = \",\".join([repr(x[0]) for x in cursor.description]) + \"\\n\" # This line outputs the types of the output fields actual_result += \",\".join([type_code_repr(x[1]) for x in cursor.description]) + \"\\n\" # This line outputs the data themselves actual_result += \"\\n\".join(map(lambda line: \",\".join(map(value_repr, line)), fetched)) else: actual_result = \"none\" print(actual_result) The JayDeBeApi Python library may require specific data formatting before input. This includes: Datetime objects should be converted to string format More information about potential driver issues can be found on JayDeBeApi Github page .","title":"Example: connect to a database by using Python"},{"location":"developing-with-firebolt/connecting-via-rest-api/","text":"Firebolt REST API The Firebolt REST API provides endpoints that enable you to interact with Firebolt programmatically. This topic provides commands for common tasks using the REST API, including authentication, working with engines, and executing queries. In this topic Use tokens for authentication Start, stop, and restart engines Get the URL of an engine Ingest data Run queries Use tokens for authentication Authenticating with the Firebolt REST API requires retrieving an access token using the auth endpoint. Retrieve authentication token The access token is a secret string that identifies your user. Submit the request shown in the example below to get a token. The token for you to use with other commands is in the response example as YOUR_ACCESS_TOKEN_VALUE . The access token is valid for 24 hours (86400 seconds). Replace YOUR_USER_EMAIL and YOUR_PASSWORD as appropriate. Request curl --request POST 'https://api.app.firebolt.io/auth/v1/login' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"username\":\"YOUR_USER_EMAIL\",\"password\":\"YOUR_PASSWORD\"}' Response { \"access_token\": \"YOUR_ACCESS_TOKEN_VALUE\", \"expires_in\": 86400, \"refresh_token\": \"YOUR_REFRESH_TOKEN_VALUE\", \"scope\": \"offline_access\", \"token_type\": \"Bearer\" } Refresh access tokens When an access_token expires, you may get 401 HTTP errors when calling the API. You can either repeat the access token request with login information shown above, or you can use the refresh_token value from the example above to get a new access_token value as shown in the example below. Request curl --request POST 'https://api.app.firebolt.io/auth/v1/refresh' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"refresh_token\":\"YOUR_REFRESH_TOKEN_VALUE\"}' Response { \"access_token\": \"YOUR_REFRESHED_ACCESS_TOKEN_VALUE\", \"scope\": \"offline_access\", \"expires_in\": 86400, \"token_type\": \"Bearer\" } Start, stop, and restart engines An engine in Firebolt is a cluster of nodes that do the work when you run SQL queries. You can use the REST API to start, stop, and restart engines. To perform these operations on an engine, you must have the unique engine ID and the engine must be stopped. The operation to get this ID is shown first, followed by start, stop, and restart operations. For more information about engines, see Working with engines . Get an engine ID Replace YOUR_ENGINE_NAME in the example below with the name of your engine. Request curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getIdByName?engine_name=YOUR_ENGINE_NAME&account=YOUR_ACCOUNT' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine_id\" { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } } Start an engine Replace ENGINE_ID in the example below with the value you retrieved for engine_id . Request curl --request POST 'https://api.app.firebolt.io/core/v1/account/engines/ENGINE_ID:start' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine\": { \"id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"name\": \"YOUR_ENGINE_NAME\", \"description\": \"\", \"emoji\": \"1F3A5\", \"compute_region_id\": { \"provider_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"region_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"settings\": { \"preset\": \"ENGINE_SETTINGS_PRESET_GENERAL_PURPOSE\", \"auto_stop_delay_duration\": \"1200s\", \"minimum_logging_level\": \"ENGINE_SETTINGS_LOGGING_LEVEL_INFO\", \"is_read_only\": false, \"warm_up\": \"ENGINE_SETTINGS_WARM_UP_INDEXES\" }, \"current_status\": \"ENGINE_STATUS_RUNNING_IDLE\", \"current_status_summary\": \"ENGINE_STATUS_SUMMARY_STOPPED\", \"latest_revision_id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_revision_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"endpoint\": \"YOUR_ENGINE_NAME.YOUR_ACCOUNT_NAME.YOUR_REGION.app.firebolt.io\", \"endpoint_serving_revision_id\": null, \"create_time\": \"2021-05-03T20:40:43.024856Z\", \"create_actor\": \"/users/xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"last_update_time\": \"2021-05-21T23:54:15.841494Z\", \"last_update_actor\": \"\", \"last_use_time\": null, \"desired_status\": \"ENGINE_STATUS_UNSPECIFIED\", \"health_status\": \"ENGINE_HEALTH_STATUS_UNSPECIFIED\", \"endpoint_desired_revision_id\": null }, \"desired_engine_revision\": null } Stop an engine Replace ENGINE_ID in the example below with the value you retrieved for engine_id . Request curl --request POST 'https://api.app.firebolt.io/core/v1/account/engines/ENGINE_ID:stop' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine\": { \"id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"name\": \"YOUR_ENGINE_NAME\", \"description\": \"\", \"emoji\": \"1F3A5\", \"compute_region_id\": { \"provider_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"region_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"settings\": { \"preset\": \"ENGINE_SETTINGS_PRESET_GENERAL_PURPOSE\", \"auto_stop_delay_duration\": \"1200s\", \"minimum_logging_level\": \"ENGINE_SETTINGS_LOGGING_LEVEL_INFO\", \"is_read_only\": false, \"warm_up\": \"ENGINE_SETTINGS_WARM_UP_INDEXES\" }, \"current_status\": \"ENGINE_STATUS_RUNNING_REVISION_SERVING\", \"current_status_summary\": \"ENGINE_STATUS_SUMMARY_RUNNING\", \"latest_revision_id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_revision_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"endpoint\": \"YOUR_ENGINE_NAME.YOUR_ACCOUNT_NAME.YOUR_REGION.app.firebolt.io\", \"endpoint_serving_revision_id\": null, \"create_time\": \"2021-05-03T20:40:43.024856Z\", \"create_actor\": \"/users/xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"last_update_time\": \"2021-05-24T21:05:59.388381Z\", \"last_update_actor\": \"\", \"last_use_time\": null, \"desired_status\": \"ENGINE_STATUS_UNSPECIFIED\", \"health_status\": \"ENGINE_HEALTH_STATUS_UNSPECIFIED\", \"endpoint_desired_revision_id\": null }, \"desired_engine_revision\": null } Restart an engine Replace ENGINE_ID with the value you retrieved for engine_id . Request curl --request POST 'https://api.app.firebolt.io/core/v1/account/engines/ENGINE_ID:restart' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine\": { \"id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"name\": \"YOUR_ENGINE_NAME\", \"description\": \"\", \"emoji\": \"1F3A5\", \"compute_region_id\": { \"provider_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"region_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"settings\": { \"preset\": \"ENGINE_SETTINGS_PRESET_GENERAL_PURPOSE\", \"auto_stop_delay_duration\": \"1200s\", \"minimum_logging_level\": \"ENGINE_SETTINGS_LOGGING_LEVEL_INFO\", \"is_read_only\": false, \"warm_up\": \"ENGINE_SETTINGS_WARM_UP_INDEXES\" }, \"current_status\": \"ENGINE_STATUS_RUNNING_REVISION_SERVING\", \"current_status_summary\": \"ENGINE_STATUS_SUMMARY_RUNNING\", \"latest_revision_id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_revision_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"endpoint\": \"YOUR_ENGINE_NAME.YOUR_ACCOUNT_NAME.YOUR_REGION.app.firebolt.io\", \"endpoint_serving_revision_id\": null, \"create_time\": \"2021-05-03T20:40:43.024856Z\", \"create_actor\": \"/users/xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"last_update_time\": \"2021-05-24T21:15:00.522042Z\", \"last_update_actor\": \"\", \"last_use_time\": null, \"desired_status\": \"ENGINE_STATUS_UNSPECIFIED\", \"health_status\": \"ENGINE_HEALTH_STATUS_UNSPECIFIED\", \"endpoint_desired_revision_id\": null }, \"desired_engine_revision\": null } Get the URL of an engine Some Firebolt REST API operations require the URL of the engine to run the request. You can get the URL of an engine in any state, but the engine must be running to accept requests at its URL. Commands to start, stop, and restart and engine use the engine ID instead of the URL. For more information about starting, stopping, and restarting engines using the Firebolt REST API, see Start, stop, and restart engines above. The examples below show you how to get the URL of the default engine by providing the database name, and how to get it using the engine name. Get the URL of the default engine in your database Use the following request to get the URL of the default engine in your database. curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getURLByDatabaseName?database_name=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' This returns: {\"engine_url\": \"YOUR_DATABASES_DEFAULT_ENGINE_URL\"} Get the URL of an engine by providing the engine name Use the following request to get the URL of an engine by using the engine name, curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines?filter.name_contains=YOUR_ENGINE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' This returns ( ... indicates areas of JSON omitted from this example): { \"page\": { ... }, \"edges\": [ { ... \"endpoint\": \"YOUR_ENGINE_URL\", ... } } ] } Ingest data Ingesting data using the Firebolt REST API requires the following steps: Create an external table Create a fact table and impo rt data Create an external table Use a request similar to the example below to create an external table. echo \"CREATE_EXTERNAL_TABLE_SCRIPT\" | curl \\ --request POST 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' \\ --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of engine_url returned by the command shown in Get the URL of an engine. YOUR_DATABASE_NAME is the name of the database. CREATE_EXTERNAL_TABLE_SCRIPT is a SQL script using the CREATE EXTERNAL TABLE statement similar to the following example: CREATE EXTERNAL TABLE [ IF NOT EXISTS ] <external_table> ( <col_name> <col_type> [ , ... ]) URL = 's3://<path_to_s3_objects>' [ CREDENTIALS = ( AWS_KEY_ID = '******' AWS_SECRET_KEY = '******' ) ] OBJECT_PATTERN = <pattern_regex> TYPE = ( { CSV | JSON | PARQUET } ); In the above script, replace the following placeholders: <external_table> is the name of the external table to create. <col_name> <col_type> are columns and corresponding data type specifications. <path_to_s3_objects> is the path to your data store in Amazon S3. <pattern_regex> is a pattern that identifies the files inside the S3 location. For example, to specify all files of <filename>.parquet in the bucket, use '*.parquet' in place of <pattern_regex> . Create a fact table and import data Use the following request to create a fact table: echo \"CREATE_FACT_TABLE_SCRIPT\" | curl \\ --request POST 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' \\ --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value returned by the command shown in Get the URL of an engine . YOUR_DATABASE_NAME is the name of the database. CREATE_FACT_TABLE_SCRIPT is a SQL script similar to the following: CREATE FACT TABLE [IF NOT EXISTS] <fact_table> ( <column_name> <column_type> [ , ... ] ) PRIMARY INDEX <column_list> In the above script, replace the following: <fact_table> is the name of the fact table to create. <column_name> <column_type> are columns and corresponding data type specifications. <column_list> is a list of column names separated by commas (for example, column1, column2 ) to be used for the primary index. For more information, see Primary indexes . Before importing the data to the fact table, consider creating an aggregating index to boost performance even further. You can also create the aggregating index later. For more information, see Aggregating indexes . Import data into the fact table Use the following request to import data into your fact table: echo \"IMPORT_SCRIPT\" | curl \\ --request POST 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of your_engine_url returned by the command shown in Get the URL of an engine . YOUR_DATABASE_NAME is the name of the database. IMPORT_SCRIPT is a SQL script similar to the following: INSERT INTO <fact_table> SELECT * FROM <external_table> <fact_table> is your Firebolt table name. <external_table> is the name of the external table to use for ingestion. Run queries Use the following syntax to submit a query to run to a running engine. You can specify multiple queries separated by a semicolon (;). Request echo \"SELECT_QUERY\" | curl \\ --request POST 'https://YOUR_ENGINE_ENDPOINT/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' \\ --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of engine_url returned by the command shown in Get the URL of an engine . YOUR_DATABASE_NAME is the name of the database. SELECT_QUERY is the query to run. You can separate multiple queries using a semicolon ( ; ) as shown in the example below. SELECT_QUERY_1; SELECT_QUERY_2; --more queries... SELECT_QUERY_N; Cancel a running Query Use the following request to cancel a running query: curl --request POST 'https://YOUR_ENGINE_URL/cancel?query_id=YOUR_QUERY_ID;' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of engine_url returned by the command shown in Get the URL of an engine . YOUR_QUERY_ID is the ID of the query you need to cancel. You can get a query ID using the running queries view .","title":"Firebolt REST API"},{"location":"developing-with-firebolt/connecting-via-rest-api/#firebolt-rest-api","text":"The Firebolt REST API provides endpoints that enable you to interact with Firebolt programmatically. This topic provides commands for common tasks using the REST API, including authentication, working with engines, and executing queries.","title":"Firebolt REST API"},{"location":"developing-with-firebolt/connecting-via-rest-api/#in-this-topic","text":"Use tokens for authentication Start, stop, and restart engines Get the URL of an engine Ingest data Run queries","title":"In this topic"},{"location":"developing-with-firebolt/connecting-via-rest-api/#use-tokens-for-authentication","text":"Authenticating with the Firebolt REST API requires retrieving an access token using the auth endpoint.","title":"Use tokens for authentication"},{"location":"developing-with-firebolt/connecting-via-rest-api/#retrieve-authentication-token","text":"The access token is a secret string that identifies your user. Submit the request shown in the example below to get a token. The token for you to use with other commands is in the response example as YOUR_ACCESS_TOKEN_VALUE . The access token is valid for 24 hours (86400 seconds). Replace YOUR_USER_EMAIL and YOUR_PASSWORD as appropriate. Request curl --request POST 'https://api.app.firebolt.io/auth/v1/login' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"username\":\"YOUR_USER_EMAIL\",\"password\":\"YOUR_PASSWORD\"}' Response { \"access_token\": \"YOUR_ACCESS_TOKEN_VALUE\", \"expires_in\": 86400, \"refresh_token\": \"YOUR_REFRESH_TOKEN_VALUE\", \"scope\": \"offline_access\", \"token_type\": \"Bearer\" }","title":"Retrieve authentication token"},{"location":"developing-with-firebolt/connecting-via-rest-api/#refresh-access-tokens","text":"When an access_token expires, you may get 401 HTTP errors when calling the API. You can either repeat the access token request with login information shown above, or you can use the refresh_token value from the example above to get a new access_token value as shown in the example below. Request curl --request POST 'https://api.app.firebolt.io/auth/v1/refresh' \\ --header 'Content-Type: application/json;charset=UTF-8' \\ --data-binary '{\"refresh_token\":\"YOUR_REFRESH_TOKEN_VALUE\"}' Response { \"access_token\": \"YOUR_REFRESHED_ACCESS_TOKEN_VALUE\", \"scope\": \"offline_access\", \"expires_in\": 86400, \"token_type\": \"Bearer\" }","title":"Refresh access tokens"},{"location":"developing-with-firebolt/connecting-via-rest-api/#start-stop-and-restart-engines","text":"An engine in Firebolt is a cluster of nodes that do the work when you run SQL queries. You can use the REST API to start, stop, and restart engines. To perform these operations on an engine, you must have the unique engine ID and the engine must be stopped. The operation to get this ID is shown first, followed by start, stop, and restart operations. For more information about engines, see Working with engines .","title":"Start, stop, and restart engines"},{"location":"developing-with-firebolt/connecting-via-rest-api/#get-an-engine-id","text":"Replace YOUR_ENGINE_NAME in the example below with the name of your engine. Request curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getIdByName?engine_name=YOUR_ENGINE_NAME&account=YOUR_ACCOUNT' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine_id\" { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } }","title":"Get an engine ID"},{"location":"developing-with-firebolt/connecting-via-rest-api/#start-an-engine","text":"Replace ENGINE_ID in the example below with the value you retrieved for engine_id . Request curl --request POST 'https://api.app.firebolt.io/core/v1/account/engines/ENGINE_ID:start' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine\": { \"id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"name\": \"YOUR_ENGINE_NAME\", \"description\": \"\", \"emoji\": \"1F3A5\", \"compute_region_id\": { \"provider_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"region_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"settings\": { \"preset\": \"ENGINE_SETTINGS_PRESET_GENERAL_PURPOSE\", \"auto_stop_delay_duration\": \"1200s\", \"minimum_logging_level\": \"ENGINE_SETTINGS_LOGGING_LEVEL_INFO\", \"is_read_only\": false, \"warm_up\": \"ENGINE_SETTINGS_WARM_UP_INDEXES\" }, \"current_status\": \"ENGINE_STATUS_RUNNING_IDLE\", \"current_status_summary\": \"ENGINE_STATUS_SUMMARY_STOPPED\", \"latest_revision_id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_revision_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"endpoint\": \"YOUR_ENGINE_NAME.YOUR_ACCOUNT_NAME.YOUR_REGION.app.firebolt.io\", \"endpoint_serving_revision_id\": null, \"create_time\": \"2021-05-03T20:40:43.024856Z\", \"create_actor\": \"/users/xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"last_update_time\": \"2021-05-21T23:54:15.841494Z\", \"last_update_actor\": \"\", \"last_use_time\": null, \"desired_status\": \"ENGINE_STATUS_UNSPECIFIED\", \"health_status\": \"ENGINE_HEALTH_STATUS_UNSPECIFIED\", \"endpoint_desired_revision_id\": null }, \"desired_engine_revision\": null }","title":"Start an engine"},{"location":"developing-with-firebolt/connecting-via-rest-api/#stop-an-engine","text":"Replace ENGINE_ID in the example below with the value you retrieved for engine_id . Request curl --request POST 'https://api.app.firebolt.io/core/v1/account/engines/ENGINE_ID:stop' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine\": { \"id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"name\": \"YOUR_ENGINE_NAME\", \"description\": \"\", \"emoji\": \"1F3A5\", \"compute_region_id\": { \"provider_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"region_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"settings\": { \"preset\": \"ENGINE_SETTINGS_PRESET_GENERAL_PURPOSE\", \"auto_stop_delay_duration\": \"1200s\", \"minimum_logging_level\": \"ENGINE_SETTINGS_LOGGING_LEVEL_INFO\", \"is_read_only\": false, \"warm_up\": \"ENGINE_SETTINGS_WARM_UP_INDEXES\" }, \"current_status\": \"ENGINE_STATUS_RUNNING_REVISION_SERVING\", \"current_status_summary\": \"ENGINE_STATUS_SUMMARY_RUNNING\", \"latest_revision_id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_revision_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"endpoint\": \"YOUR_ENGINE_NAME.YOUR_ACCOUNT_NAME.YOUR_REGION.app.firebolt.io\", \"endpoint_serving_revision_id\": null, \"create_time\": \"2021-05-03T20:40:43.024856Z\", \"create_actor\": \"/users/xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"last_update_time\": \"2021-05-24T21:05:59.388381Z\", \"last_update_actor\": \"\", \"last_use_time\": null, \"desired_status\": \"ENGINE_STATUS_UNSPECIFIED\", \"health_status\": \"ENGINE_HEALTH_STATUS_UNSPECIFIED\", \"endpoint_desired_revision_id\": null }, \"desired_engine_revision\": null }","title":"Stop an engine"},{"location":"developing-with-firebolt/connecting-via-rest-api/#restart-an-engine","text":"Replace ENGINE_ID with the value you retrieved for engine_id . Request curl --request POST 'https://api.app.firebolt.io/core/v1/account/engines/ENGINE_ID:restart' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Response { \"engine\": { \"id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"name\": \"YOUR_ENGINE_NAME\", \"description\": \"\", \"emoji\": \"1F3A5\", \"compute_region_id\": { \"provider_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"region_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"settings\": { \"preset\": \"ENGINE_SETTINGS_PRESET_GENERAL_PURPOSE\", \"auto_stop_delay_duration\": \"1200s\", \"minimum_logging_level\": \"ENGINE_SETTINGS_LOGGING_LEVEL_INFO\", \"is_read_only\": false, \"warm_up\": \"ENGINE_SETTINGS_WARM_UP_INDEXES\" }, \"current_status\": \"ENGINE_STATUS_RUNNING_REVISION_SERVING\", \"current_status_summary\": \"ENGINE_STATUS_SUMMARY_RUNNING\", \"latest_revision_id\": { \"account_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"engine_revision_id\": \"xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }, \"endpoint\": \"YOUR_ENGINE_NAME.YOUR_ACCOUNT_NAME.YOUR_REGION.app.firebolt.io\", \"endpoint_serving_revision_id\": null, \"create_time\": \"2021-05-03T20:40:43.024856Z\", \"create_actor\": \"/users/xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"last_update_time\": \"2021-05-24T21:15:00.522042Z\", \"last_update_actor\": \"\", \"last_use_time\": null, \"desired_status\": \"ENGINE_STATUS_UNSPECIFIED\", \"health_status\": \"ENGINE_HEALTH_STATUS_UNSPECIFIED\", \"endpoint_desired_revision_id\": null }, \"desired_engine_revision\": null }","title":"Restart an engine"},{"location":"developing-with-firebolt/connecting-via-rest-api/#get-the-url-of-an-engine","text":"Some Firebolt REST API operations require the URL of the engine to run the request. You can get the URL of an engine in any state, but the engine must be running to accept requests at its URL. Commands to start, stop, and restart and engine use the engine ID instead of the URL. For more information about starting, stopping, and restarting engines using the Firebolt REST API, see Start, stop, and restart engines above. The examples below show you how to get the URL of the default engine by providing the database name, and how to get it using the engine name. Get the URL of the default engine in your database Use the following request to get the URL of the default engine in your database. curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines:getURLByDatabaseName?database_name=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' This returns: {\"engine_url\": \"YOUR_DATABASES_DEFAULT_ENGINE_URL\"} Get the URL of an engine by providing the engine name Use the following request to get the URL of an engine by using the engine name, curl --request GET 'https://api.app.firebolt.io/core/v1/account/engines?filter.name_contains=YOUR_ENGINE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' This returns ( ... indicates areas of JSON omitted from this example): { \"page\": { ... }, \"edges\": [ { ... \"endpoint\": \"YOUR_ENGINE_URL\", ... } } ] }","title":"Get the URL of an engine"},{"location":"developing-with-firebolt/connecting-via-rest-api/#ingest-data","text":"Ingesting data using the Firebolt REST API requires the following steps: Create an external table Create a fact table and impo rt data","title":"Ingest data"},{"location":"developing-with-firebolt/connecting-via-rest-api/#create-an-external-table","text":"Use a request similar to the example below to create an external table. echo \"CREATE_EXTERNAL_TABLE_SCRIPT\" | curl \\ --request POST 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' \\ --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of engine_url returned by the command shown in Get the URL of an engine. YOUR_DATABASE_NAME is the name of the database. CREATE_EXTERNAL_TABLE_SCRIPT is a SQL script using the CREATE EXTERNAL TABLE statement similar to the following example: CREATE EXTERNAL TABLE [ IF NOT EXISTS ] <external_table> ( <col_name> <col_type> [ , ... ]) URL = 's3://<path_to_s3_objects>' [ CREDENTIALS = ( AWS_KEY_ID = '******' AWS_SECRET_KEY = '******' ) ] OBJECT_PATTERN = <pattern_regex> TYPE = ( { CSV | JSON | PARQUET } ); In the above script, replace the following placeholders: <external_table> is the name of the external table to create. <col_name> <col_type> are columns and corresponding data type specifications. <path_to_s3_objects> is the path to your data store in Amazon S3. <pattern_regex> is a pattern that identifies the files inside the S3 location. For example, to specify all files of <filename>.parquet in the bucket, use '*.parquet' in place of <pattern_regex> .","title":"Create an external table"},{"location":"developing-with-firebolt/connecting-via-rest-api/#create-a-fact-table-and-import-data","text":"Use the following request to create a fact table: echo \"CREATE_FACT_TABLE_SCRIPT\" | curl \\ --request POST 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' \\ --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value returned by the command shown in Get the URL of an engine . YOUR_DATABASE_NAME is the name of the database. CREATE_FACT_TABLE_SCRIPT is a SQL script similar to the following: CREATE FACT TABLE [IF NOT EXISTS] <fact_table> ( <column_name> <column_type> [ , ... ] ) PRIMARY INDEX <column_list> In the above script, replace the following: <fact_table> is the name of the fact table to create. <column_name> <column_type> are columns and corresponding data type specifications. <column_list> is a list of column names separated by commas (for example, column1, column2 ) to be used for the primary index. For more information, see Primary indexes . Before importing the data to the fact table, consider creating an aggregating index to boost performance even further. You can also create the aggregating index later. For more information, see Aggregating indexes . Import data into the fact table Use the following request to import data into your fact table: echo \"IMPORT_SCRIPT\" | curl \\ --request POST 'https://YOUR_ENGINE_URL/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of your_engine_url returned by the command shown in Get the URL of an engine . YOUR_DATABASE_NAME is the name of the database. IMPORT_SCRIPT is a SQL script similar to the following: INSERT INTO <fact_table> SELECT * FROM <external_table> <fact_table> is your Firebolt table name. <external_table> is the name of the external table to use for ingestion.","title":"Create a fact table and import data"},{"location":"developing-with-firebolt/connecting-via-rest-api/#run-queries","text":"Use the following syntax to submit a query to run to a running engine. You can specify multiple queries separated by a semicolon (;).","title":"Run queries"},{"location":"developing-with-firebolt/connecting-via-rest-api/#request","text":"echo \"SELECT_QUERY\" | curl \\ --request POST 'https://YOUR_ENGINE_ENDPOINT/?database=YOUR_DATABASE_NAME' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' \\ --data-binary @- Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of engine_url returned by the command shown in Get the URL of an engine . YOUR_DATABASE_NAME is the name of the database. SELECT_QUERY is the query to run. You can separate multiple queries using a semicolon ( ; ) as shown in the example below. SELECT_QUERY_1; SELECT_QUERY_2; --more queries... SELECT_QUERY_N;","title":"Request"},{"location":"developing-with-firebolt/connecting-via-rest-api/#cancel-a-running-query","text":"Use the following request to cancel a running query: curl --request POST 'https://YOUR_ENGINE_URL/cancel?query_id=YOUR_QUERY_ID;' \\ --header 'Authorization: Bearer YOUR_ACCESS_TOKEN_VALUE' Provide values for placeholders according to the following guidance. YOUR_ENGINE_URL is the value of engine_url returned by the command shown in Get the URL of an engine . YOUR_QUERY_ID is the ID of the query you need to cancel. You can get a query ID using the running queries view .","title":"Cancel a running Query"},{"location":"docs/architecture-overview/","text":"Architecture overview The diagram below depicts Firebolt\u2019s high-level architecture, which comprises a services layer and decoupled storage and compute layers. This topic describes each layer. Services Layer The services layer is multi-tenant. It accepts all incoming requests to Firebolt. Its most important functions are: Administration - Handles account information, user management, and permissions. Metadata - Contains all metadata of databases, engines, tables, indexes, etc. Security - Handles authentication. Isolated Tenancy Unlike the multi-tenant services layer, the compute and storage layers in Firebolt run on isolated tenants. A dedicated and isolated AWS sub-account is created for each Firebolt customer, within which Firebolt manages the storage and compute layers. Each tenant runs within Firebolt\u2019s master account and outside their own VPC. This ensures complete cross-customer isolation for data and query execution. Compute Layer The compute layer runs Firebolt engines . Engines are compute clusters that run database workloads. Each engine is an isolated cluster. Within each cluster, engine instances store data and indexes in the local solid state drive (SSD), which acts as the local cache. The engine loads data from the storage layer into SSD at query runtime based on the query configuration. A benefit of the decoupled storage and compute architecture is that multiple engines can be assigned to the same database. This allows for granular control over which hardware is assigned to which tasks. Each engine can have a different configuration and size depending on the workloads. Engines can work in parallel or separately, and you can share them with different people in your organization. Storage Layer The storage layer within Firebolt runs on Amazon S3. After you ingest data into Firebolt, this is where the data and indexes associated with a database are saved. When you ingest data, you use a Firebolt general purpose engine, which stores the data in the proprietary Firebolt File Format (F3). The data is sorted, compressed, and indexed to support highly efficient pruning for query acceleration. F3 works together with other proprietary Firebolt technologies to deliver exceptional performance at query runtime.","title":"Architecture overview"},{"location":"docs/architecture-overview/#architecture-overview","text":"The diagram below depicts Firebolt\u2019s high-level architecture, which comprises a services layer and decoupled storage and compute layers. This topic describes each layer.","title":"Architecture overview"},{"location":"docs/architecture-overview/#services-layer","text":"The services layer is multi-tenant. It accepts all incoming requests to Firebolt. Its most important functions are: Administration - Handles account information, user management, and permissions. Metadata - Contains all metadata of databases, engines, tables, indexes, etc. Security - Handles authentication.","title":"Services Layer"},{"location":"docs/architecture-overview/#isolated-tenancy","text":"Unlike the multi-tenant services layer, the compute and storage layers in Firebolt run on isolated tenants. A dedicated and isolated AWS sub-account is created for each Firebolt customer, within which Firebolt manages the storage and compute layers. Each tenant runs within Firebolt\u2019s master account and outside their own VPC. This ensures complete cross-customer isolation for data and query execution.","title":"Isolated Tenancy"},{"location":"docs/architecture-overview/#compute-layer","text":"The compute layer runs Firebolt engines . Engines are compute clusters that run database workloads. Each engine is an isolated cluster. Within each cluster, engine instances store data and indexes in the local solid state drive (SSD), which acts as the local cache. The engine loads data from the storage layer into SSD at query runtime based on the query configuration. A benefit of the decoupled storage and compute architecture is that multiple engines can be assigned to the same database. This allows for granular control over which hardware is assigned to which tasks. Each engine can have a different configuration and size depending on the workloads. Engines can work in parallel or separately, and you can share them with different people in your organization.","title":"Compute Layer"},{"location":"docs/architecture-overview/#storage-layer","text":"The storage layer within Firebolt runs on Amazon S3. After you ingest data into Firebolt, this is where the data and indexes associated with a database are saved. When you ingest data, you use a Firebolt general purpose engine, which stores the data in the proprietary Firebolt File Format (F3). The data is sorted, compressed, and indexed to support highly efficient pruning for query acceleration. F3 works together with other proprietary Firebolt technologies to deliver exceptional performance at query runtime.","title":"Storage Layer"},{"location":"general-reference/data-types/","text":"Data types The following table describes all available data types in Firebolt: Category Type Description Numeric INT A whole number ranging from -2,147,483,648 to 2,147,483,647. INT data types require 4 bytes of storage. Synonym for INTEGER . INTEGER Synonym for INT . BIGINT A whole number ranging from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807. BIGINT data types require 8 bytes of storage. Synonym for LONG LONG Synonym for BIGINT . FLOAT A floating-point number that has six decimal-digit precision. Decimal (fixed point) types are not supported. FLOAT data types require 4 bytes of storage. DOUBLE A floating-point number that has 15 decimal-digit precision. Decimal (fixed point) types are not supported. DOUBLE data types require 8 bytes. Synonym for DOUBLE PRECISION DOUBLE PRECISION Synonym for DOUBLE String VARCHAR A string of an arbitrary length that can contain any number of bytes, including null bytes. Useful for arbitrary-length string columns. Firebolt supports UTF-8 escape sequences. Synonym for TEXT , STRING TEXT Synonym for VARCHAR , STRING STRING Synonym for VARCHAR , TEXT Date & Time DATE DATE (cont.) A year, month and day in the format ' YYYY - MM - DD'. This value is stored as a 4-byte unsigned Unix timestamp. The minimum DATE value is 1970-02-01 . The maximum DATE value is 2105-12-31 . It does not specify a time zone. Arithmetic operations can be executed on DATE values. The examples below show the addition and subtraction of integers. CAST(\u20182019-07-31' AS DATE) + 4 Returns 2019-08-04 CAST(\u20182019-07-31' AS DATE) - 4 Returns 2019-07-27 Caution Arithmetic, conditional, and comparative operations are not supported on date values outside the supported range. These operations return inaccurate results because they are based on the minimum and maximum dates in the range rather than the actual dates provided or expected to be returned. The arithmetic operations in the examples below return inaccurate results as shown because the dates returned are outside the supported range. CAST ('1970-02-02' AS DATE) - 365 Returns 1970-01-31 CAST ('2105-02-012' AS DATE) + 365 Returns 2105-12-31 If you work with dates outside the supported range, we recommend that you use a string datatype such as TEXT . For example, the following query returns all rows with the date 1921-12-31 . SELECT FROM tab1text WHERE date_as_text = '1921-12-31'; The example below selects all rows where the date_as_text column specifies a date after 1921-12-31 . SELECT FROM tab1text WHERE date_as_text > '1921-12-31'; The example below generates a count of how many rows in date_as_text are from each month of the year. It uses SUBSTR to extract the month value from the date string, and then it groups the count by month. SELECT COUNT(*), SUBSTR(date_as_text,6,2) FROM tab1text GROUP BY SUBSTR(date_as_text,6,2); TIMESTAMP A year, month, day, hour, minute and second in the format ' YYYY-MM-DD hh:mm:ss' . This value is stored as an unsigned Unix timestamp with 4 bytes. Same range as DATE type. Minimal value: 0000-00-00 00:00:00. To change the default time zone in Firebolt: SET DEFAULT_TIMEZONE = \"Pacific Standard Time\" This is a synonym for DATETIME DATETIME Synonym for TIMESTAMP Boolean BOOLEAN Accepts the following values: true , false , 1 , 0 . Stores the values as 1/0 respectively. Semi-Structured ARRAY Represents dense or sparse arrays. An array can contain all data types including nested arrays (array with arrays). A column whose type is ARRAY cannot be nullable, however the elements of an ARRAY can. Therefore the following is an illegal declaration: nullArray ARRAY(INT) NULL But this one is valid: nullElements ARRAY(INT NULL)","title":"Data types"},{"location":"general-reference/data-types/#data-types","text":"The following table describes all available data types in Firebolt: Category Type Description Numeric INT A whole number ranging from -2,147,483,648 to 2,147,483,647. INT data types require 4 bytes of storage. Synonym for INTEGER . INTEGER Synonym for INT . BIGINT A whole number ranging from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807. BIGINT data types require 8 bytes of storage. Synonym for LONG LONG Synonym for BIGINT . FLOAT A floating-point number that has six decimal-digit precision. Decimal (fixed point) types are not supported. FLOAT data types require 4 bytes of storage. DOUBLE A floating-point number that has 15 decimal-digit precision. Decimal (fixed point) types are not supported. DOUBLE data types require 8 bytes. Synonym for DOUBLE PRECISION DOUBLE PRECISION Synonym for DOUBLE String VARCHAR A string of an arbitrary length that can contain any number of bytes, including null bytes. Useful for arbitrary-length string columns. Firebolt supports UTF-8 escape sequences. Synonym for TEXT , STRING TEXT Synonym for VARCHAR , STRING STRING Synonym for VARCHAR , TEXT Date & Time DATE DATE (cont.) A year, month and day in the format ' YYYY - MM - DD'. This value is stored as a 4-byte unsigned Unix timestamp. The minimum DATE value is 1970-02-01 . The maximum DATE value is 2105-12-31 . It does not specify a time zone. Arithmetic operations can be executed on DATE values. The examples below show the addition and subtraction of integers. CAST(\u20182019-07-31' AS DATE) + 4 Returns 2019-08-04 CAST(\u20182019-07-31' AS DATE) - 4 Returns 2019-07-27 Caution Arithmetic, conditional, and comparative operations are not supported on date values outside the supported range. These operations return inaccurate results because they are based on the minimum and maximum dates in the range rather than the actual dates provided or expected to be returned. The arithmetic operations in the examples below return inaccurate results as shown because the dates returned are outside the supported range. CAST ('1970-02-02' AS DATE) - 365 Returns 1970-01-31 CAST ('2105-02-012' AS DATE) + 365 Returns 2105-12-31 If you work with dates outside the supported range, we recommend that you use a string datatype such as TEXT . For example, the following query returns all rows with the date 1921-12-31 . SELECT FROM tab1text WHERE date_as_text = '1921-12-31'; The example below selects all rows where the date_as_text column specifies a date after 1921-12-31 . SELECT FROM tab1text WHERE date_as_text > '1921-12-31'; The example below generates a count of how many rows in date_as_text are from each month of the year. It uses SUBSTR to extract the month value from the date string, and then it groups the count by month. SELECT COUNT(*), SUBSTR(date_as_text,6,2) FROM tab1text GROUP BY SUBSTR(date_as_text,6,2); TIMESTAMP A year, month, day, hour, minute and second in the format ' YYYY-MM-DD hh:mm:ss' . This value is stored as an unsigned Unix timestamp with 4 bytes. Same range as DATE type. Minimal value: 0000-00-00 00:00:00. To change the default time zone in Firebolt: SET DEFAULT_TIMEZONE = \"Pacific Standard Time\" This is a synonym for DATETIME DATETIME Synonym for TIMESTAMP Boolean BOOLEAN Accepts the following values: true , false , 1 , 0 . Stores the values as 1/0 respectively. Semi-Structured ARRAY Represents dense or sparse arrays. An array can contain all data types including nested arrays (array with arrays). A column whose type is ARRAY cannot be nullable, however the elements of an ARRAY can. Therefore the following is an illegal declaration: nullArray ARRAY(INT) NULL But this one is valid: nullElements ARRAY(INT NULL)","title":"Data types"},{"location":"general-reference/identifier-requirements/","text":"Identifier requirements Firebolt identifiers can refer to the following items: Columns Tables Indexes Databases Views Engines Identifiers must be at least one character and at most 255 characters. Firebolt evaluates unquoted identifiers such as table and column names entirely in lowercase. The following queries: SELECT my_column FROM my_table SELECT MY_COLUMN FROM MY_TABLE SELECT mY_cOlUmn FROM mY_tAbLe are all equivalent to: SELECT my_column FROM my_table Unquoted identifiers in some early Firebolt accounts may be case sensitive. You can keep uppercase identifiers by enclosing them in double-quotes. For example, the following identifiers are unique: \"COLUMN_NAME\" \"column_name\" \"CoLuMn_NaMe\"","title":"Identifier requirements"},{"location":"general-reference/identifier-requirements/#identifier-requirements","text":"Firebolt identifiers can refer to the following items: Columns Tables Indexes Databases Views Engines Identifiers must be at least one character and at most 255 characters. Firebolt evaluates unquoted identifiers such as table and column names entirely in lowercase. The following queries: SELECT my_column FROM my_table SELECT MY_COLUMN FROM MY_TABLE SELECT mY_cOlUmn FROM mY_tAbLe are all equivalent to: SELECT my_column FROM my_table Unquoted identifiers in some early Firebolt accounts may be case sensitive. You can keep uppercase identifiers by enclosing them in double-quotes. For example, the following identifiers are unique: \"COLUMN_NAME\" \"column_name\" \"CoLuMn_NaMe\"","title":"Identifier requirements"},{"location":"general-reference/reserved-words/","text":"Reserved words Reserved words have special meanings to the Firebolt SQL engine. We recommend that you do not use these words for naming databases, tables, columns, variables, or other objects that you define. If you must use a reserved word in this way, enclose it in quotation marks. ALL ALTER AND ARRAY BETWEEN BIGINT BOOL BOOLEAN BOTH CASE CAST CHAR CONCAT COPY CREATE CROSS CURRENT_DATE CURRENT_TIMESTAMP DATABASE DATE DATETIME DECIMAL DELETE DESCRIBE DISTINCT DOUBLE DOUBLECOLON DOW DOY DROP EMPTY_IDENTIFIER EPOCH EXCEPT EXECUTE EXISTS EXPLAIN EXTRACT FALSE FETCH FIRST FLOAT FROM FULL GENERATE GROUP HAVING IF ILIKE IN INNER INSERT INT INTEGER INTERSECT INTERVAL IS ISNULL JOIN JOIN_TYPE LEADING LEFT LIKE LIMIT LIMIT_DISTINCT LOCALTIMESTAMP LONG NATURAL NEXT NOT NULL NUMERIC OFFSET ON ONLY OR ORDER OUTER OVER PARTITION PRECISION PREPARE PRIMARY QUARTER RIGHT ROW ROWS SAMPLE SELECT SET SHOW TEXT TIME TIMESTAMP TOP TRAILING TRIM TRUE TRUNCATE UNION UNKNOWN_CHAR UNNEST UNTERMINATED_STRING UPDATE USING VARCHAR WEEK WHEN WHERE WITH","title":"Reserved words"},{"location":"general-reference/reserved-words/#reserved-words","text":"Reserved words have special meanings to the Firebolt SQL engine. We recommend that you do not use these words for naming databases, tables, columns, variables, or other objects that you define. If you must use a reserved word in this way, enclose it in quotation marks. ALL ALTER AND ARRAY BETWEEN BIGINT BOOL BOOLEAN BOTH CASE CAST CHAR CONCAT COPY CREATE CROSS CURRENT_DATE CURRENT_TIMESTAMP DATABASE DATE DATETIME DECIMAL DELETE DESCRIBE DISTINCT DOUBLE DOUBLECOLON DOW DOY DROP EMPTY_IDENTIFIER EPOCH EXCEPT EXECUTE EXISTS EXPLAIN EXTRACT FALSE FETCH FIRST FLOAT FROM FULL GENERATE GROUP HAVING IF ILIKE IN INNER INSERT INT INTEGER INTERSECT INTERVAL IS ISNULL JOIN JOIN_TYPE LEADING LEFT LIKE LIMIT LIMIT_DISTINCT LOCALTIMESTAMP LONG NATURAL NEXT NOT NULL NUMERIC OFFSET ON ONLY OR ORDER OUTER OVER PARTITION PRECISION PREPARE PRIMARY QUARTER RIGHT ROW ROWS SAMPLE SELECT SET SHOW TEXT TIME TIMESTAMP TOP TRAILING TRIM TRUE TRUNCATE UNION UNKNOWN_CHAR UNNEST UNTERMINATED_STRING UPDATE USING VARCHAR WEEK WHEN WHERE WITH","title":"Reserved words"},{"location":"general-reference/information-schema/","text":"Information schema & usage views The Firebolt information schema consists of a set of system views that contains metadata information on the objects defined in the current database. In addition, it contains a set of database usage (catalog) views such as the query history and running queries views. Firebolt information schema relies on the standard SQL information schema with some additional information and views that are FIrebolt specific. Database usage views Information schema views","title":"Information schema & usage views"},{"location":"general-reference/information-schema/#information-schema-usage-views","text":"The Firebolt information schema consists of a set of system views that contains metadata information on the objects defined in the current database. In addition, it contains a set of database usage (catalog) views such as the query history and running queries views. Firebolt information schema relies on the standard SQL information schema with some additional information and views that are FIrebolt specific.","title":"Information schema &amp; usage views"},{"location":"general-reference/information-schema/#database-usage-views","text":"","title":"Database usage views"},{"location":"general-reference/information-schema/#information-schema-views","text":"","title":"Information schema views"},{"location":"general-reference/information-schema/columns/","text":"Columns This information schema view contains a row per table column in the current database. The view is available in all databases and can be queried as follows: SELECT * FROM information_schema.columns; View columns Column Name Data Type Description table_catalog TEXT Name of the catalog. Firebolt provides a single \u2018default\u2019 catalog table_schema TEXT Name of the database containing the table table_name TEXT Name of the table column_name TEXT Name of the column is_nullable TEXT \u2018YES\u2019 if the column may contain NULL, \u2018NO\u2019 otherwise data_type TEXT The data type of the column is_in_partition_expr TEXT \u2018YES\u2019 if the column is part of the partition by expression and \u2018NO\u2019 otherwise. is_in_primary_index TEXT \u2018YES\u2019 if the column is part of the primary index and \u2018NO\u2019 otherwise. character_maximum_length INT Not applicable for Firebolt character_octet_length INT Not applicable for Firebolt numeric_precision_radix INT Not applicable for Firebolt interval_type TEXT Not applicable for Firebolt interval_precision INT Not applicable for Firebolt character_set_catalog TEXT Not applicable for Firebolt character_set_schema TEXT Not applicable for Firebolt character_set_name TEXT Not applicable for Firebolt collation_catalog TEXT Not applicable for Firebolt collation_schema TEXT Not applicable for Firebolt collation_name TEXT Not applicable for Firebolt domain_catalog TEXT Not applicable for Firebolt domain_schema TEXT Not applicable for Firebolt domain_name TEXT Not applicable for Firebolt udt_catalog TEXT Not applicable for Firebolt udt_schema TEXT Not applicable for Firebolt udt_name TEXT Not applicable for Firebolt scope_catalog TEXT Not applicable for Firebolt scope_schema TEXT Not applicable for Firebolt scope_name TEXT Not applicable for Firebolt maximum_cardinality INT Not applicable for Firebolt dtd_identifier TEXT Not applicable for Firebolt is_self_referencing TEXT Not applicable for Firebolt is_identity TEXT Not applicable for Firebolt identity_generation TEXT Not applicable for Firebolt identity_start TEXT Not applicable for Firebolt identity_increment TEXT Not applicable for Firebolt identity_maximum TEXT Not applicable for Firebolt identity_minimum TEXT Not applicable for Firebolt identity_cycle TEXT Not applicable for Firebolt is_generated TEXT Not applicable for Firebolt generation_expression TEXT Not applicable for Firebolt is_updatable TEXT Not applicable for Firebolt","title":"Columns"},{"location":"general-reference/information-schema/columns/#columns","text":"This information schema view contains a row per table column in the current database. The view is available in all databases and can be queried as follows: SELECT * FROM information_schema.columns;","title":"Columns"},{"location":"general-reference/information-schema/columns/#view-columns","text":"Column Name Data Type Description table_catalog TEXT Name of the catalog. Firebolt provides a single \u2018default\u2019 catalog table_schema TEXT Name of the database containing the table table_name TEXT Name of the table column_name TEXT Name of the column is_nullable TEXT \u2018YES\u2019 if the column may contain NULL, \u2018NO\u2019 otherwise data_type TEXT The data type of the column is_in_partition_expr TEXT \u2018YES\u2019 if the column is part of the partition by expression and \u2018NO\u2019 otherwise. is_in_primary_index TEXT \u2018YES\u2019 if the column is part of the primary index and \u2018NO\u2019 otherwise. character_maximum_length INT Not applicable for Firebolt character_octet_length INT Not applicable for Firebolt numeric_precision_radix INT Not applicable for Firebolt interval_type TEXT Not applicable for Firebolt interval_precision INT Not applicable for Firebolt character_set_catalog TEXT Not applicable for Firebolt character_set_schema TEXT Not applicable for Firebolt character_set_name TEXT Not applicable for Firebolt collation_catalog TEXT Not applicable for Firebolt collation_schema TEXT Not applicable for Firebolt collation_name TEXT Not applicable for Firebolt domain_catalog TEXT Not applicable for Firebolt domain_schema TEXT Not applicable for Firebolt domain_name TEXT Not applicable for Firebolt udt_catalog TEXT Not applicable for Firebolt udt_schema TEXT Not applicable for Firebolt udt_name TEXT Not applicable for Firebolt scope_catalog TEXT Not applicable for Firebolt scope_schema TEXT Not applicable for Firebolt scope_name TEXT Not applicable for Firebolt maximum_cardinality INT Not applicable for Firebolt dtd_identifier TEXT Not applicable for Firebolt is_self_referencing TEXT Not applicable for Firebolt is_identity TEXT Not applicable for Firebolt identity_generation TEXT Not applicable for Firebolt identity_start TEXT Not applicable for Firebolt identity_increment TEXT Not applicable for Firebolt identity_maximum TEXT Not applicable for Firebolt identity_minimum TEXT Not applicable for Firebolt identity_cycle TEXT Not applicable for Firebolt is_generated TEXT Not applicable for Firebolt generation_expression TEXT Not applicable for Firebolt is_updatable TEXT Not applicable for Firebolt","title":"View columns"},{"location":"general-reference/information-schema/databases/","text":"Databases This information schema view contains a row per database in your Firebolt account. The view is available in all databases and can be queried as follows: SELECT * FROM information_schema.databases; View columns Name Data Type Description catalog_name TEXT Name of the catalog. Firebolt provides a single \u2018default\u2019 catalog schema_name TEXT Name of the database default_character_set_catalog TEXT Not applicable for Firebolt default_character_set_schema TEXT Not applicable for Firebolt default_character_set_name TEXT Not applicable for Firebolt sql_path TEXT Not applicable for Firebolt","title":"Databases"},{"location":"general-reference/information-schema/databases/#databases","text":"This information schema view contains a row per database in your Firebolt account. The view is available in all databases and can be queried as follows: SELECT * FROM information_schema.databases;","title":"Databases"},{"location":"general-reference/information-schema/databases/#view-columns","text":"Name Data Type Description catalog_name TEXT Name of the catalog. Firebolt provides a single \u2018default\u2019 catalog schema_name TEXT Name of the database default_character_set_catalog TEXT Not applicable for Firebolt default_character_set_schema TEXT Not applicable for Firebolt default_character_set_name TEXT Not applicable for Firebolt sql_path TEXT Not applicable for Firebolt","title":"View columns"},{"location":"general-reference/information-schema/query-history-view/","text":"Query history The query_history view can be used to explore Firebolt's query history in your account.\\ The view is available in all databases and can be queried, for example, as follows: SELECT * FROM catalog.query_history LIMIT 100; The query_history view contains the following columns: Column Name Description ENGINE_NAME The name of the engine that was used to execute the query ENGINE_ID The id of the engine that was used to execute the query USER_ID The user ID that was used to execute the query ACCOUNT_ID The ID of the account in which the query was executed START_TIME The query execution start time (UTC) END_TIME The query execution end time (UTC) DURATION Duration of query execution (in milliseconds) STATUS Can be one of the following values: \u2018STARTED_EXECUTION' \u2014 Successful start of query execution. 'ENDED_SUCCESSFULLY' \u2014 Successful end of query execution. \u2018PARSE_ERROR' \u2014 Exception before the start of query execution. 'EXECUTION_ERROR' \u2014 Exception during the query execution. QUERY_ID The unique identifier of the SQL query QUERY_TEXT Text of the SQL statement ERROR_MESSAGE The error message that was returned SCANNED_ROWS The total number of rows scanned SCANNED_BYTES The total number of bytes scanned (both from cache and storage) SCANNED_BYTES_CACHE The total number of compressed bytes scanned from the engine's cache (the SSD instance store on engine nodes) SCANNED_BYTES_STORAGE The total number of compressed bytes scanned from F3 storage INSERTED_ROWS The total number of rows written INSERTED_BYTES The total number of bytes written (both to cache and storage) INSERTED_BYTES_CACHE The total number of compressed bytes written to the engine's cache (the SSD instance store on engine nodes) INSERTED_BYTES_STORAGE The total number of compressed bytes written to F3 storage SPILLED_BYTES_COMPRESSED The total number of compressed bytes spilled SPILLED_BYTES_UNCOMPRESSED The total number of uncompressed bytes spilled TOTAL_RAM_CONSUMED The total number of engine bytes in RAM consumed during query execution RETURNED_ROWS The total number of rows returned from the query RETURNED_BYTES The total number of bytes returned from the query","title":"Query history"},{"location":"general-reference/information-schema/query-history-view/#query-history","text":"The query_history view can be used to explore Firebolt's query history in your account.\\ The view is available in all databases and can be queried, for example, as follows: SELECT * FROM catalog.query_history LIMIT 100; The query_history view contains the following columns: Column Name Description ENGINE_NAME The name of the engine that was used to execute the query ENGINE_ID The id of the engine that was used to execute the query USER_ID The user ID that was used to execute the query ACCOUNT_ID The ID of the account in which the query was executed START_TIME The query execution start time (UTC) END_TIME The query execution end time (UTC) DURATION Duration of query execution (in milliseconds) STATUS Can be one of the following values: \u2018STARTED_EXECUTION' \u2014 Successful start of query execution. 'ENDED_SUCCESSFULLY' \u2014 Successful end of query execution. \u2018PARSE_ERROR' \u2014 Exception before the start of query execution. 'EXECUTION_ERROR' \u2014 Exception during the query execution. QUERY_ID The unique identifier of the SQL query QUERY_TEXT Text of the SQL statement ERROR_MESSAGE The error message that was returned SCANNED_ROWS The total number of rows scanned SCANNED_BYTES The total number of bytes scanned (both from cache and storage) SCANNED_BYTES_CACHE The total number of compressed bytes scanned from the engine's cache (the SSD instance store on engine nodes) SCANNED_BYTES_STORAGE The total number of compressed bytes scanned from F3 storage INSERTED_ROWS The total number of rows written INSERTED_BYTES The total number of bytes written (both to cache and storage) INSERTED_BYTES_CACHE The total number of compressed bytes written to the engine's cache (the SSD instance store on engine nodes) INSERTED_BYTES_STORAGE The total number of compressed bytes written to F3 storage SPILLED_BYTES_COMPRESSED The total number of compressed bytes spilled SPILLED_BYTES_UNCOMPRESSED The total number of uncompressed bytes spilled TOTAL_RAM_CONSUMED The total number of engine bytes in RAM consumed during query execution RETURNED_ROWS The total number of rows returned from the query RETURNED_BYTES The total number of bytes returned from the query","title":"Query history"},{"location":"general-reference/information-schema/running-queries/","text":"Running queries The running_queries view can be used to explore Firebolt's running queries in your account.\\ The view is available in all databases and can be queried, for example, as follows: SELECT * FROM catalog.running_queries LIMIT 100; The running_queries view contains the following columns: Column Name Description ENGINE_NAME The name of the engine that was used to execute the query ENGINE_ID The id of the engine that was used to execute the query USER_ID The user ID that was used to execute the query ACCOUNT_ID The ID of the account in which the query was executed START_TIME The query execution start time (UTC) DURATION Duration of query execution (in milliseconds) STATUS The status of the query. Always contains the value 'RUNNING'. QUERY_ID The unique identifier of the SQL query QUERY_TEXT Text of the SQL statement SCANNED_ROWS The total amount of rows scanned SCANNED_BYTES The total amount of bytes scanned (both in cache and storage) INSERTED_ROWS The total amount of rows written INSERTED_BYTES The total amount of bytes written","title":"Running queries"},{"location":"general-reference/information-schema/running-queries/#running-queries","text":"The running_queries view can be used to explore Firebolt's running queries in your account.\\ The view is available in all databases and can be queried, for example, as follows: SELECT * FROM catalog.running_queries LIMIT 100; The running_queries view contains the following columns: Column Name Description ENGINE_NAME The name of the engine that was used to execute the query ENGINE_ID The id of the engine that was used to execute the query USER_ID The user ID that was used to execute the query ACCOUNT_ID The ID of the account in which the query was executed START_TIME The query execution start time (UTC) DURATION Duration of query execution (in milliseconds) STATUS The status of the query. Always contains the value 'RUNNING'. QUERY_ID The unique identifier of the SQL query QUERY_TEXT Text of the SQL statement SCANNED_ROWS The total amount of rows scanned SCANNED_BYTES The total amount of bytes scanned (both in cache and storage) INSERTED_ROWS The total amount of rows written INSERTED_BYTES The total amount of bytes written","title":"Running queries"},{"location":"general-reference/information-schema/tables/","text":"Tables This information schema view contains a row per table in the current database. The view is available in all databases and can be queried as follows: SELECT * FROM information_schema.tables; View columns Column Name Data Type Description table_catalog TEXT Name of the catalog. Firebolt offers a single \u2018default\u2019 catalog table_schema TEXT The name of the database table_name TEXT Name of the table primary_index ARRAY(TEXT) The primary index columns self_referencing_column_name TEXT Not applicable for Firebolt reference_generation TEXT Not applicable for Firebolt user_defined_type_catalog TEXT Not applicable for Firebolt user_defined_type_schema TEXT Not applicable for Firebolt user_defined_type_name TEXT Not applicable for Firebolt is_insertable_into TEXT Not applicable for Firebolt is_typed TEXT Not applicable for Firebolt commit_action TEXT Not applicable for Firebolt","title":"Tables"},{"location":"general-reference/information-schema/tables/#tables","text":"This information schema view contains a row per table in the current database. The view is available in all databases and can be queried as follows: SELECT * FROM information_schema.tables;","title":"Tables"},{"location":"general-reference/information-schema/tables/#view-columns","text":"Column Name Data Type Description table_catalog TEXT Name of the catalog. Firebolt offers a single \u2018default\u2019 catalog table_schema TEXT The name of the database table_name TEXT Name of the table primary_index ARRAY(TEXT) The primary index columns self_referencing_column_name TEXT Not applicable for Firebolt reference_generation TEXT Not applicable for Firebolt user_defined_type_catalog TEXT Not applicable for Firebolt user_defined_type_schema TEXT Not applicable for Firebolt user_defined_type_name TEXT Not applicable for Firebolt is_insertable_into TEXT Not applicable for Firebolt is_typed TEXT Not applicable for Firebolt commit_action TEXT Not applicable for Firebolt","title":"View columns"},{"location":"integrations/connecting-via-jdbc/","text":"Connecting apps and services using the JDBC driver Firebolt provides a JDBC driver (Type 4) that can be used with most applications and tools that support JDBC for connecting to a database server. The driver is provided as a JAR file and requires Java 1.8 (or higher). Downloading the driver Use one of the following links to download Firebolt's JDBC driver: Firebolt generic JDBC driver You can use this driver to connect Airflow , Tableau , DBeaver , and other services to Firebolt. Before downloading the driver make sure you read the Firebolt JDBC license terms . Related topics Connecting to Looker Setting up Airflow JDBC to Firebolt Setting up Tableau (Desktop) JDBC to Firebolt Setting up DBeaver JDBC connection to Firebolt","title":"Connecting apps and services using the JDBC driver"},{"location":"integrations/connecting-via-jdbc/#connecting-apps-and-services-using-the-jdbc-driver","text":"Firebolt provides a JDBC driver (Type 4) that can be used with most applications and tools that support JDBC for connecting to a database server. The driver is provided as a JAR file and requires Java 1.8 (or higher).","title":"Connecting apps and services using the JDBC driver"},{"location":"integrations/connecting-via-jdbc/#downloading-the-driver","text":"Use one of the following links to download Firebolt's JDBC driver: Firebolt generic JDBC driver You can use this driver to connect Airflow , Tableau , DBeaver , and other services to Firebolt. Before downloading the driver make sure you read the Firebolt JDBC license terms .","title":"Downloading the driver"},{"location":"integrations/connecting-via-jdbc/#related-topics","text":"Connecting to Looker Setting up Airflow JDBC to Firebolt Setting up Tableau (Desktop) JDBC to Firebolt Setting up DBeaver JDBC connection to Firebolt","title":"Related topics"},{"location":"integrations/integration-overview/","text":"Integration overview Firebolt is working with an expanding roster of services and applications to utilize our optimized performance speed. The services listed below include tools that have been customized by our partners to connect with Firebolt. Many services not explicitly listed here can still operate with Firebolt's architecture through our drivers and adapters for general connection use. * Apache Airflow * * DBeaver * * dbt * * Keboola * * Looker * * Tableau * Common connection parameters Most integrated services require the same parameters from Firebolt to establish a connection. Unless otherwise noted, connection details for Firebolt can be found as indicated below. Username : The email address used to create your Firebolt account. Password : The password used to create your Firebolt account. Database : The name of your database. From the Database view on the Firebolt Manager, your database name can be found as indicated below: Engine : The engine being used to query your database. Some services will automatically use your default engine unless directed to use another. From the Database view on the Firebolt Manager, the engine names can be found in the image below:","title":"Integration overview"},{"location":"integrations/integration-overview/#integration-overview","text":"Firebolt is working with an expanding roster of services and applications to utilize our optimized performance speed. The services listed below include tools that have been customized by our partners to connect with Firebolt. Many services not explicitly listed here can still operate with Firebolt's architecture through our drivers and adapters for general connection use. * Apache Airflow * * DBeaver * * dbt * * Keboola * * Looker * * Tableau *","title":"Integration overview"},{"location":"integrations/integration-overview/#common-connection-parameters","text":"Most integrated services require the same parameters from Firebolt to establish a connection. Unless otherwise noted, connection details for Firebolt can be found as indicated below. Username : The email address used to create your Firebolt account. Password : The password used to create your Firebolt account. Database : The name of your database. From the Database view on the Firebolt Manager, your database name can be found as indicated below: Engine : The engine being used to query your database. Some services will automatically use your default engine unless directed to use another. From the Database view on the Firebolt Manager, the engine names can be found in the image below:","title":"Common connection parameters"},{"location":"integrations/business-intelligence/","text":"Business intelligence Business intelligence (BI) tools enable data analysis, investigation and reporting to help individuals make more informed business decisions. Many BI tool provide the ability to deliver data visualization through dashboards, charts, and other graphics.","title":"Business intelligence"},{"location":"integrations/business-intelligence/#business-intelligence","text":"Business intelligence (BI) tools enable data analysis, investigation and reporting to help individuals make more informed business decisions. Many BI tool provide the ability to deliver data visualization through dashboards, charts, and other graphics.","title":"Business intelligence"},{"location":"integrations/business-intelligence/connecting-to-looker/","text":"Connecting to Looker You can connect Firebolt and Looker version 21.10 or later by setting up a database connection in Looker. For more information, including supported features, see Firebolt in Looker documentation. If you use a Looker version earlier than 21.10, please contact Firebolt support for connection instructions using the JDBC driver. Configuring basic settings When specifying the Username and Password in Looker, we recommend that you use a Firebolt user with Viewer privileges that you create exclusively for Looker connections. This simplifies user management. By default, Looker connects to the default engine for the database specified in the connection. Use Additional Params to specify a different engine. To specify a different engine for the Looker connection For Additional Params , enter engine=<engine-name>. Configuration best practices Looker can dynamically process your data in a number of ways. Following these best practices helps you configure Looker to take advantage of Firebolt\u2019s features for accelerated query performance. You may not be able to implement all of these for your use case, but the more that you can implement these suggestions the better. Use Firebolt fact tables as base views in Looker Explores The base view in a Looker Explore is the database table that is used in the FROM clause of the SQL query that Looker generates. Looker manifests other views joined into an Explore using a JOIN clause. Typically, base views are joined to other views in a many-to-one direction using LEFT JOIN , and most filters occur on the base view. In Firebolt, fact tables are sharded across engine instances. Dimension tables, which are typically smaller than fact tables, are replicated in each instance. This helps optimize query performance. Firebolt typically performs best when you filter on the fact table and use it in the FROM clause in your SQL queries when using a LEFT JOIN . These preferred patterns in Firebolt make it a best practice to use a Firebolt fact table as the base view of a Looker Explore. If you need multiple fact tables in an Explore, set the base view as the Firebolt fact table that is queried most frequently and join the others. Looker only joins tables when the analyitcal query needs them, and the FROM clause is static. To set a fact table as the base view for a Looker explore Specify the Firebolt fact table name as the view_name in the LookML from parameter for a Looker Explore. Avoid transformations in commonly filtered dimensions Filters on dimensions in Looker are translated to predicates in the WHERE clause of the SQL query that Looker generates. Firebolt performs best when these predicates are included in primary indexes and, if applicable, aggregating indexes. If you use a functions such as CAST or UPPER that transform predicates, Firebolt must read all values to satisfy the conditions of comparison, which undermines the effectiveness of the index in accelerating query performance. For this reason, avoid using transformations in the LookML sql parameter for a Looker dimension, especially for dimensions that exist within Firebolt fact tables and are included in the primary indexes of those tables. An example of such a transformation is shown below. If you need a transformation for an analytical use case, consider creating a virtual column in the fact table. The virtual column can receive a value that you transform from the original column when you ingest with INSERT INTO . You can then specify the virtual column in the primary index and also specify the virtual column in the LookML sql parameter for the Looker dimension. Force the usage of filters represented in Firebolt primary indexes As indicated above, by defining indexes in Firebolt on columns that you use as dimensions in Looker Explores to filter results, you can accelerate the performance of queries that Looker builds for Firebolt. You can use the LookML always_filter parameter to specify these columns and filters to help ensure that Looker users exploring data must add those filters on Firebolt\u2019s primary index. This adds guard rails to prevent novice users from building slow queries in their first experience. The example below shows a LookML always_filter defined for the report_date column in the Firebolt fact table named campaign_fact . In Firebolt, this fact table has a primary index definition that includes the campaign_fact column. Use join indexes Join indexes in Firebolt can accelerate queries with joins. Looker is known to generate a lot of joins, especially if your data is relatively normalized. Evaluating the joins in the LookML for your Looker Explores can provide clues for the join indexes to create so that you can accelerate performance. Join indexes that you define in Firebolt are beneficial when there is a many-to-one relationship between fact tables and a join table, when you typically filter queries using only a subset of available columns in the dimension table, and when the dimension table is large enough that it exceeds the RAM available to any single engine instance. In addition, dimension tables that have a join index defined for them should have a column with unique values that is used as the join key. This column should have the UNIQUE parameter set when the dimension table is created. For more information, see Using indexes for faster queries . Use system activity data to inform aggregating indexes Use Looker dashboards and system activity data to identify the most common or impactful queries, and then create Firebolt aggregating indexes to optimize these queries. When looking at the queries in the system activity, try to group the queries to see if there are commonalities between some of them. If possible, create a single aggregating index that can handle multiple queries. A single aggregating index can contain multiple aggregations to handle a range of queries. For more information, see Using indexes for faster queries .","title":"Connecting to Looker"},{"location":"integrations/business-intelligence/connecting-to-looker/#connecting-to-looker","text":"You can connect Firebolt and Looker version 21.10 or later by setting up a database connection in Looker. For more information, including supported features, see Firebolt in Looker documentation. If you use a Looker version earlier than 21.10, please contact Firebolt support for connection instructions using the JDBC driver.","title":"Connecting to Looker"},{"location":"integrations/business-intelligence/connecting-to-looker/#configuring-basic-settings","text":"When specifying the Username and Password in Looker, we recommend that you use a Firebolt user with Viewer privileges that you create exclusively for Looker connections. This simplifies user management. By default, Looker connects to the default engine for the database specified in the connection. Use Additional Params to specify a different engine. To specify a different engine for the Looker connection For Additional Params , enter engine=<engine-name>.","title":"Configuring basic settings"},{"location":"integrations/business-intelligence/connecting-to-looker/#configuration-best-practices","text":"Looker can dynamically process your data in a number of ways. Following these best practices helps you configure Looker to take advantage of Firebolt\u2019s features for accelerated query performance. You may not be able to implement all of these for your use case, but the more that you can implement these suggestions the better.","title":"Configuration best practices"},{"location":"integrations/business-intelligence/connecting-to-looker/#use-firebolt-fact-tables-as-base-views-in-looker-explores","text":"The base view in a Looker Explore is the database table that is used in the FROM clause of the SQL query that Looker generates. Looker manifests other views joined into an Explore using a JOIN clause. Typically, base views are joined to other views in a many-to-one direction using LEFT JOIN , and most filters occur on the base view. In Firebolt, fact tables are sharded across engine instances. Dimension tables, which are typically smaller than fact tables, are replicated in each instance. This helps optimize query performance. Firebolt typically performs best when you filter on the fact table and use it in the FROM clause in your SQL queries when using a LEFT JOIN . These preferred patterns in Firebolt make it a best practice to use a Firebolt fact table as the base view of a Looker Explore. If you need multiple fact tables in an Explore, set the base view as the Firebolt fact table that is queried most frequently and join the others. Looker only joins tables when the analyitcal query needs them, and the FROM clause is static. To set a fact table as the base view for a Looker explore Specify the Firebolt fact table name as the view_name in the LookML from parameter for a Looker Explore.","title":"Use Firebolt fact tables as base views in Looker Explores "},{"location":"integrations/business-intelligence/connecting-to-looker/#avoid-transformations-in-commonly-filtered-dimensions","text":"Filters on dimensions in Looker are translated to predicates in the WHERE clause of the SQL query that Looker generates. Firebolt performs best when these predicates are included in primary indexes and, if applicable, aggregating indexes. If you use a functions such as CAST or UPPER that transform predicates, Firebolt must read all values to satisfy the conditions of comparison, which undermines the effectiveness of the index in accelerating query performance. For this reason, avoid using transformations in the LookML sql parameter for a Looker dimension, especially for dimensions that exist within Firebolt fact tables and are included in the primary indexes of those tables. An example of such a transformation is shown below. If you need a transformation for an analytical use case, consider creating a virtual column in the fact table. The virtual column can receive a value that you transform from the original column when you ingest with INSERT INTO . You can then specify the virtual column in the primary index and also specify the virtual column in the LookML sql parameter for the Looker dimension.","title":"Avoid transformations in commonly filtered dimensions "},{"location":"integrations/business-intelligence/connecting-to-looker/#force-the-usage-of-filters-represented-in-firebolt-primary-indexes","text":"As indicated above, by defining indexes in Firebolt on columns that you use as dimensions in Looker Explores to filter results, you can accelerate the performance of queries that Looker builds for Firebolt. You can use the LookML always_filter parameter to specify these columns and filters to help ensure that Looker users exploring data must add those filters on Firebolt\u2019s primary index. This adds guard rails to prevent novice users from building slow queries in their first experience. The example below shows a LookML always_filter defined for the report_date column in the Firebolt fact table named campaign_fact . In Firebolt, this fact table has a primary index definition that includes the campaign_fact column.","title":"Force the usage of filters represented in Firebolt primary indexes "},{"location":"integrations/business-intelligence/connecting-to-looker/#use-join-indexes","text":"Join indexes in Firebolt can accelerate queries with joins. Looker is known to generate a lot of joins, especially if your data is relatively normalized. Evaluating the joins in the LookML for your Looker Explores can provide clues for the join indexes to create so that you can accelerate performance. Join indexes that you define in Firebolt are beneficial when there is a many-to-one relationship between fact tables and a join table, when you typically filter queries using only a subset of available columns in the dimension table, and when the dimension table is large enough that it exceeds the RAM available to any single engine instance. In addition, dimension tables that have a join index defined for them should have a column with unique values that is used as the join key. This column should have the UNIQUE parameter set when the dimension table is created. For more information, see Using indexes for faster queries .","title":"Use join indexes "},{"location":"integrations/business-intelligence/connecting-to-looker/#use-system-activity-data-to-inform-aggregating-indexes","text":"Use Looker dashboards and system activity data to identify the most common or impactful queries, and then create Firebolt aggregating indexes to optimize these queries. When looking at the queries in the system activity, try to group the queries to see if there are commonalities between some of them. If possible, create a single aggregating index that can handle multiple queries. A single aggregating index can contain multiple aggregations to handle a range of queries. For more information, see Using indexes for faster queries .","title":"Use system activity data to inform aggregating indexes "},{"location":"integrations/business-intelligence/setting-up-tableau-desktop-jdbc-to-firebolt/","text":"Connecting to Tableau (Desktop) This topic describes the steps for setting up the Tableau (Desktop) connection to Firebolt. Step 1: Install the latest Firebolt JDBC Driver Download Firebolt\u2019s JDBC driver for Tableau from here . Put the JDBC jar file in the Tableau JDBC driver folder: Windows: C:\\Program Files\\Tableau\\Drivers Mac: ~/Library/Tableau/Drivers Step 2: Install the latest Firebolt TACO file Download Firebolt\u2019s packaged connector file for Tableau (with a .taco filename extension) from here . Copy the packaged connector file into your My Tableau Repository/Connectors directory : Windows: C:\\Users\\[your-user]\\Documents\\My Tableau Repository\\Connectors Mac: ~/Documents/My Tableau Repository/Connectors Step 3: Set up the connection to Firebolt in Tableau Pre-requisites Make a note of the database name you would like to connect to in Firebolt. Also, make sure the database has an up-and-running engine before you configure the connection in Tableau. Make a note of the username and password used to log into Firebolt. Start Tableau and disable the signature validation for the TACO file - read more in Tableau's docs under the \"Disabling signature verification\" topic. Configuring the Connection In Tableau's UI under connections click More: Search for \"Connector by Firebolt\" in the list of connectors and click on it. The following popup appears: Fill in the following parameters and click sign-in: Parameter Description Value Server The URL of Firebolt's API server api.app.firebolt.io Database The name of the database you would like to connect to Database name Engine The name of the engine you would like to use (optional). If not specified - the default engine will be used. Engine name Username The username used for connecting to Firebolt Your Firebolt username Password The password used for connecting to Firebolt Your Firebolt password","title":"Connecting to Tableau (Desktop)"},{"location":"integrations/business-intelligence/setting-up-tableau-desktop-jdbc-to-firebolt/#connecting-to-tableau-desktop","text":"This topic describes the steps for setting up the Tableau (Desktop) connection to Firebolt.","title":"Connecting to Tableau (Desktop)"},{"location":"integrations/business-intelligence/setting-up-tableau-desktop-jdbc-to-firebolt/#step-1-install-the-latest-firebolt-jdbc-driver","text":"Download Firebolt\u2019s JDBC driver for Tableau from here . Put the JDBC jar file in the Tableau JDBC driver folder: Windows: C:\\Program Files\\Tableau\\Drivers Mac: ~/Library/Tableau/Drivers","title":"Step 1: Install the latest Firebolt JDBC Driver"},{"location":"integrations/business-intelligence/setting-up-tableau-desktop-jdbc-to-firebolt/#step-2-install-the-latest-firebolt-taco-file","text":"Download Firebolt\u2019s packaged connector file for Tableau (with a .taco filename extension) from here . Copy the packaged connector file into your My Tableau Repository/Connectors directory : Windows: C:\\Users\\[your-user]\\Documents\\My Tableau Repository\\Connectors Mac: ~/Documents/My Tableau Repository/Connectors","title":"Step 2: Install the latest Firebolt TACO file"},{"location":"integrations/business-intelligence/setting-up-tableau-desktop-jdbc-to-firebolt/#step-3-set-up-the-connection-to-firebolt-in-tableau","text":"","title":"Step 3: Set up the connection to Firebolt in Tableau"},{"location":"integrations/business-intelligence/setting-up-tableau-desktop-jdbc-to-firebolt/#pre-requisites","text":"Make a note of the database name you would like to connect to in Firebolt. Also, make sure the database has an up-and-running engine before you configure the connection in Tableau. Make a note of the username and password used to log into Firebolt. Start Tableau and disable the signature validation for the TACO file - read more in Tableau's docs under the \"Disabling signature verification\" topic.","title":"Pre-requisites"},{"location":"integrations/business-intelligence/setting-up-tableau-desktop-jdbc-to-firebolt/#configuring-the-connection","text":"In Tableau's UI under connections click More: Search for \"Connector by Firebolt\" in the list of connectors and click on it. The following popup appears: Fill in the following parameters and click sign-in: Parameter Description Value Server The URL of Firebolt's API server api.app.firebolt.io Database The name of the database you would like to connect to Database name Engine The name of the engine you would like to use (optional). If not specified - the default engine will be used. Engine name Username The username used for connecting to Firebolt Your Firebolt username Password The password used for connecting to Firebolt Your Firebolt password","title":"Configuring the Connection"},{"location":"integrations/connecting-via-jdbc/setting-up-looker-jdbc-to-firebolt/","text":"Setting up Looker JDBC to Firebolt This topic describes the steps for setting up the Looker JDBC connection to Firebolt. Step 1: Add Firebolt\u2019s JDBC driver to Looker In order to make looker work with Firebolt, follow the instructions in https://docs.looker.com/setup-and-management/database-config/custom_jdbc_drivers while paying attention to the following: Download Firebolt\u2019s JDBC driver for Looker from here (make sure you download the Firebolt JDBC driver for Looker). Put FItebolt\u2019s JDBC driver for Looker here: /home/looker/looker/custom_jdbc_drivers/athena/. If you have any other Firebolt driver there, just remove it, and replace it with the one you\u2019ve just downloaded. Create the following file under:/home/looker/looker/custom_jdbc_config.yml (if you already have this file with the following settings, move on to the next step) and paste the following into it: Make sure to replace <version> with the JDBC version number. For example - in the following JDBC: \"firebolt-jdbc-1.03-jar-looker-with-dependencies.jar\" <version> needs to be replaced with 1.03. yaml - name: athena file_name: athena/firebolt-jdbc-<version>-jar-looker-with-dependencies.jar module_path: com.firebolt.FireboltDriver override_jdbc_url_subprotocol: firebolt Restart looker with the following parameter: text LOOKERARGS=\"--use-custom-jdbc-config\" Step 2: Configure a Connection to Firebolt Pre-requisites Make a note of the database name you would like to connect to in Firebolt. Also, make sure the Database has an up and running engine before you configure the connection in Looker. Make a note of the username and password used to log into Firebolt. Configuring the Connection In Looker\u2019s UI, Select Admin > Connections (under Database ). On the Connections page, click New Connection . Click on Existing Database The following popup appears: Fill in the following parameters: Parameter Description Value Name the connection name in Looker For example: Connection Dialect The SQL Dialect Amazon Athena Host:Port The username used for connecting to FIrbolt api.app.firebolt.io:443 Database The database name in Firebolt. Username The username used for connecting to FIrebolt Your Firebolt username Password The password used for connecting to Firebolt Your FIrebolt password Additional Params Leave blank to query the database using its default engine. If you wish to use another engine, set the following parameter: engine=engineName Replace engineName with the name of the engine you would like to use. Click on the \u2018Test These Settings\u2019 button. A successful connection test results in a success message, similar to this:","title":"Setting up Looker JDBC to Firebolt"},{"location":"integrations/connecting-via-jdbc/setting-up-looker-jdbc-to-firebolt/#setting-up-looker-jdbc-to-firebolt","text":"This topic describes the steps for setting up the Looker JDBC connection to Firebolt.","title":"Setting up Looker JDBC to Firebolt"},{"location":"integrations/connecting-via-jdbc/setting-up-looker-jdbc-to-firebolt/#step-1-add-firebolts-jdbc-driver-to-looker","text":"In order to make looker work with Firebolt, follow the instructions in https://docs.looker.com/setup-and-management/database-config/custom_jdbc_drivers while paying attention to the following: Download Firebolt\u2019s JDBC driver for Looker from here (make sure you download the Firebolt JDBC driver for Looker). Put FItebolt\u2019s JDBC driver for Looker here: /home/looker/looker/custom_jdbc_drivers/athena/. If you have any other Firebolt driver there, just remove it, and replace it with the one you\u2019ve just downloaded. Create the following file under:/home/looker/looker/custom_jdbc_config.yml (if you already have this file with the following settings, move on to the next step) and paste the following into it: Make sure to replace <version> with the JDBC version number. For example - in the following JDBC: \"firebolt-jdbc-1.03-jar-looker-with-dependencies.jar\" <version> needs to be replaced with 1.03. yaml - name: athena file_name: athena/firebolt-jdbc-<version>-jar-looker-with-dependencies.jar module_path: com.firebolt.FireboltDriver override_jdbc_url_subprotocol: firebolt Restart looker with the following parameter: text LOOKERARGS=\"--use-custom-jdbc-config\"","title":"Step 1: Add Firebolt\u2019s JDBC driver to Looker"},{"location":"integrations/connecting-via-jdbc/setting-up-looker-jdbc-to-firebolt/#step-2-configure-a-connection-to-firebolt","text":"","title":"Step 2: Configure a Connection to Firebolt"},{"location":"integrations/connecting-via-jdbc/setting-up-looker-jdbc-to-firebolt/#pre-requisites","text":"Make a note of the database name you would like to connect to in Firebolt. Also, make sure the Database has an up and running engine before you configure the connection in Looker. Make a note of the username and password used to log into Firebolt.","title":"Pre-requisites"},{"location":"integrations/connecting-via-jdbc/setting-up-looker-jdbc-to-firebolt/#configuring-the-connection","text":"In Looker\u2019s UI, Select Admin > Connections (under Database ). On the Connections page, click New Connection . Click on Existing Database The following popup appears: Fill in the following parameters: Parameter Description Value Name the connection name in Looker For example: Connection Dialect The SQL Dialect Amazon Athena Host:Port The username used for connecting to FIrbolt api.app.firebolt.io:443 Database The database name in Firebolt. Username The username used for connecting to FIrebolt Your Firebolt username Password The password used for connecting to Firebolt Your FIrebolt password Additional Params Leave blank to query the database using its default engine. If you wish to use another engine, set the following parameter: engine=engineName Replace engineName with the name of the engine you would like to use. Click on the \u2018Test These Settings\u2019 button. A successful connection test results in a success message, similar to this:","title":"Configuring the Connection"},{"location":"integrations/data-integration-and-transformation/","text":"Data integration and transformation","title":"Data integration and transformation"},{"location":"integrations/data-integration-and-transformation/#data-integration-and-transformation","text":"","title":"Data integration and transformation"},{"location":"integrations/data-integration-and-transformation/connecting-to-keboola/","text":"Connecting to Keboola Keboola is a cloud ETL / ELT platform for interconnecting diverse data sets. It is used to extract and manipulate varied data sets and write the results to a destination system. When integrated with Firebolt, Keboola is used to funnel data to the AWS S3 bucket that is used for staging your Firebolt databases. To get started We recommend that you follow the guidelines for Keboola\u2019s suggested configuration for Firebolt Credentials Keboola requires the following credentials to work with Firebolt: Parameter Description Username Your Firebolt username / email Password Your Firebolt password Database Name The identifier for your Firebolt database AWS API Key ID An AWS key to access your database. You can create these credentials by following the AWS guide . We advise against using AWS root access when integrating Keboola for Firebolt. For security reasons, it is better to set up a dedicated AWS user role for Keboola. For help setting up an AWS user role and policies, please see this Keboola guide . AWS API Key Secret The secret access key for accessing your database. You can obtain this key by following the same steps as the AWS API Key ID AWS Staging Bucket The name of your AWS S3 bucket. Firebolt's connector with Keboola will use the designated default engine for your database. Firebolt identifiers normally evaluate all identifiers as lowercase characters, however some integrated Keboola data sources could have case-sensitive requirements. In this scenario, you can use double quotes to ensure case-sensitive identifiers. For more information, please see Firebolt's identifier requirements .","title":"Connecting to Keboola"},{"location":"integrations/data-integration-and-transformation/connecting-to-keboola/#connecting-to-keboola","text":"Keboola is a cloud ETL / ELT platform for interconnecting diverse data sets. It is used to extract and manipulate varied data sets and write the results to a destination system. When integrated with Firebolt, Keboola is used to funnel data to the AWS S3 bucket that is used for staging your Firebolt databases.","title":"Connecting to Keboola"},{"location":"integrations/data-integration-and-transformation/connecting-to-keboola/#to-get-started","text":"We recommend that you follow the guidelines for Keboola\u2019s suggested configuration for Firebolt","title":"To get started"},{"location":"integrations/data-integration-and-transformation/connecting-to-keboola/#credentials","text":"Keboola requires the following credentials to work with Firebolt: Parameter Description Username Your Firebolt username / email Password Your Firebolt password Database Name The identifier for your Firebolt database AWS API Key ID An AWS key to access your database. You can create these credentials by following the AWS guide . We advise against using AWS root access when integrating Keboola for Firebolt. For security reasons, it is better to set up a dedicated AWS user role for Keboola. For help setting up an AWS user role and policies, please see this Keboola guide . AWS API Key Secret The secret access key for accessing your database. You can obtain this key by following the same steps as the AWS API Key ID AWS Staging Bucket The name of your AWS S3 bucket. Firebolt's connector with Keboola will use the designated default engine for your database. Firebolt identifiers normally evaluate all identifiers as lowercase characters, however some integrated Keboola data sources could have case-sensitive requirements. In this scenario, you can use double quotes to ensure case-sensitive identifiers. For more information, please see Firebolt's identifier requirements .","title":"Credentials"},{"location":"integrations/data-integration-and-transformation/connecting-with-dbt/","text":"Connecting with dbt dbt (data build tool) is a modern development framework that enables data analysts and data engineers to transform data by writing select statements. dbt handles turning these select statements into tables and views. The Firebolt adapter for dbt brings together dbt's state-of-the-art development tools and Firebolt's next-generation analytics performance. On top of dbt's core features, the adapter offers native support for all of Firebolt's index types and has been specifically enhanced to support ingestion from S3 using Firebolt's external tables mechanics. To get started We recommend you follow the guidelines for dbt integration in our Github repository .","title":"Connecting with dbt"},{"location":"integrations/data-integration-and-transformation/connecting-with-dbt/#connecting-with-dbt","text":"dbt (data build tool) is a modern development framework that enables data analysts and data engineers to transform data by writing select statements. dbt handles turning these select statements into tables and views. The Firebolt adapter for dbt brings together dbt's state-of-the-art development tools and Firebolt's next-generation analytics performance. On top of dbt's core features, the adapter offers native support for all of Firebolt's index types and has been specifically enhanced to support ingestion from S3 using Firebolt's external tables mechanics.","title":"Connecting with dbt"},{"location":"integrations/data-integration-and-transformation/connecting-with-dbt/#to-get-started","text":"We recommend you follow the guidelines for dbt integration in our Github repository .","title":"To get started"},{"location":"integrations/other-integrations/","text":"Other integrations","title":"Other integrations"},{"location":"integrations/other-integrations/#other-integrations","text":"","title":"Other integrations"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/","text":"Connecting to Airflow Interested in continuously loading data into Firebolt? See our continuously loading data tutorial . Step 1: Install the latest Firebolt JDBC Driver Download Firebolt\u2019s JDBC driver from here . Put the JDBC jar file in the server which runs Airflow (we have placed it under /airflow/jdbc ). Step 2: Setup the JDBC connection in Airflow Pre-requisites Make sure you have: The name of the database you would like to connect to in Firebolt. The username and password used to log into Firebolt. Airflow version 1.10.12 and above. Configuring the Connection Open Airflow UI. In the Admin tab click on Connections. Click + to create a new connection to Firebolt. Configure the following parameters: Parameter Description Value Conn Id The connection identifier For example: firebolt_jdbc Conn Type The connection type JDBC Connection Connection URL The connection String URL jdbc:firebolt://api.app.firebolt.io/<db_name> Make sure to replace <db_name> with the name of your database in Firebolt. This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Login Your Firebolt username Password Your Firebolt password Driver Path The full path in your Airflow server in which you've stored Firebolt's JDBC driver In our case: /airflow/jdbc/firebolt-jdbc-1.03-jar-with-dependencies.jar Driver Class The class of the JDBC driver com.firebolt.FireboltDriver * Click on Save . Step 3: Create a DAG We will create a DAG that runs a script on top of a Firebolt table. Below is the Python DAG program: from airflow import DAG from airflow.operators.jdbc_operator import JdbcOperator default_arg = {'owner': 'airflow', 'start_date': '2020-10-20'} dag = DAG('firebolt_dag', default_args=default_arg, schedule_interval=None) firebolt_task = JdbcOperator(dag=dag,jdbc_conn_id='firebolt_db',task_id='firebolt_sql_task',sql=['query_sample.sql']) firebolt_task You can save it as a Python file, and place it under your dags folder to run it in Airflow. We use Airflow's JDBC operator to connect to Firebolt via JDBC and run a SQL script. The SQL script file contains a simple SELECT query. Feel free to use any query you want. Step 4: Run the DAG In Airflow's UI, go to the DAGs tab. Locate your DAG in the list (in our case we should look for 'firebolt_dag' : Click on the trigger button under Links to manually trigger the DAG. Once the DAG has started to run, click on it's Run Id to move to the graph view to track its progress. In our DAG we have a single step called 'firebolt_sql_task' . In the DAG's graph view, the task should appear in green to confirm the DAG was completed successfully. Click on the task 'firebolt_sql_task' : Click on View Logs to inspect the logs.","title":"Connecting to Airflow"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/#connecting-to-airflow","text":"Interested in continuously loading data into Firebolt? See our continuously loading data tutorial .","title":"Connecting to Airflow"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/#step-1-install-the-latest-firebolt-jdbc-driver","text":"Download Firebolt\u2019s JDBC driver from here . Put the JDBC jar file in the server which runs Airflow (we have placed it under /airflow/jdbc ).","title":"Step 1: Install the latest Firebolt JDBC Driver"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/#step-2-setup-the-jdbc-connection-in-airflow","text":"","title":"Step 2: Setup the JDBC connection in Airflow"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/#pre-requisites","text":"Make sure you have: The name of the database you would like to connect to in Firebolt. The username and password used to log into Firebolt. Airflow version 1.10.12 and above.","title":"Pre-requisites"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/#configuring-the-connection","text":"Open Airflow UI. In the Admin tab click on Connections. Click + to create a new connection to Firebolt. Configure the following parameters: Parameter Description Value Conn Id The connection identifier For example: firebolt_jdbc Conn Type The connection type JDBC Connection Connection URL The connection String URL jdbc:firebolt://api.app.firebolt.io/<db_name> Make sure to replace <db_name> with the name of your database in Firebolt. This enables you to query the database using its default engine. If you wish to use another engine, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=engineName Replace engineName with the name of the engine you would like to use. Login Your Firebolt username Password Your Firebolt password Driver Path The full path in your Airflow server in which you've stored Firebolt's JDBC driver In our case: /airflow/jdbc/firebolt-jdbc-1.03-jar-with-dependencies.jar Driver Class The class of the JDBC driver com.firebolt.FireboltDriver * Click on Save .","title":"Configuring the Connection"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/#step-3-create-a-dag","text":"We will create a DAG that runs a script on top of a Firebolt table. Below is the Python DAG program: from airflow import DAG from airflow.operators.jdbc_operator import JdbcOperator default_arg = {'owner': 'airflow', 'start_date': '2020-10-20'} dag = DAG('firebolt_dag', default_args=default_arg, schedule_interval=None) firebolt_task = JdbcOperator(dag=dag,jdbc_conn_id='firebolt_db',task_id='firebolt_sql_task',sql=['query_sample.sql']) firebolt_task You can save it as a Python file, and place it under your dags folder to run it in Airflow. We use Airflow's JDBC operator to connect to Firebolt via JDBC and run a SQL script. The SQL script file contains a simple SELECT query. Feel free to use any query you want.","title":"Step 3: Create a DAG"},{"location":"integrations/other-integrations/setting-up-airflow-jdbc-to-firebolt/#step-4-run-the-dag","text":"In Airflow's UI, go to the DAGs tab. Locate your DAG in the list (in our case we should look for 'firebolt_dag' : Click on the trigger button under Links to manually trigger the DAG. Once the DAG has started to run, click on it's Run Id to move to the graph view to track its progress. In our DAG we have a single step called 'firebolt_sql_task' . In the DAG's graph view, the task should appear in green to confirm the DAG was completed successfully. Click on the task 'firebolt_sql_task' : Click on View Logs to inspect the logs.","title":"Step 4: Run the DAG"},{"location":"integrations/other-integrations/setting-up-dbeaver-jdbc-connection-to-firebolt/","text":"Connecting to DBeaver DBeaver is a free open-source administration tool used to simplify working across a range of different database types. DBeaver can connect to Firebolt databases using our JDBC driver. Adding driver configuration in DBeaver 1. Download the Firebolt JDBC driver . 2. Open the driver manager dialog: From the top navigation menu click on Database > Driver Manager . 3. Select New to add a new driver: 4. In order to add the driver, fill in the following parameters: Driver Name: Firebolt . Class Name: com.firebolt.FireboltDriver . Under Libraries , click Add File and choose the JDBC driver downloaded in step 1 above. 5. Select OK after completing these steps. Adding a database in DBeaver 1. From the top navigation menu, select Database > New Database Connection . 2. Type Firebolt in the search box, and select it from the list of databases. 3. Select Next . 4. Fill in the following parameters: Parameter Description JDBC URL Use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name> In the path above, be sure to replace <db_name> with the name of your Firebolt database. This enables you to query the database using its default engine. If you wish to use a different engine than your default, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=<engineName> In the path above, replace <engineName> with the name of the engine you would like to use. Username Your Firebolt username. Password Your Firebolt password 5. Select Test Connection (make sure to start your database before you start the test). A successful connection test looks as follows: 6. Select Finish. Querying your Firebolt database In the database navigator, right-click on the database connection and select SQL Editor . If a pop-up window appears, select New Script : 2. The SQL editor should now open and you can run your queries.","title":"Connecting to DBeaver"},{"location":"integrations/other-integrations/setting-up-dbeaver-jdbc-connection-to-firebolt/#connecting-to-dbeaver","text":"DBeaver is a free open-source administration tool used to simplify working across a range of different database types. DBeaver can connect to Firebolt databases using our JDBC driver.","title":"Connecting to DBeaver"},{"location":"integrations/other-integrations/setting-up-dbeaver-jdbc-connection-to-firebolt/#adding-driver-configuration-in-dbeaver","text":"1. Download the Firebolt JDBC driver . 2. Open the driver manager dialog: From the top navigation menu click on Database > Driver Manager . 3. Select New to add a new driver: 4. In order to add the driver, fill in the following parameters: Driver Name: Firebolt . Class Name: com.firebolt.FireboltDriver . Under Libraries , click Add File and choose the JDBC driver downloaded in step 1 above. 5. Select OK after completing these steps.","title":"Adding driver configuration in DBeaver"},{"location":"integrations/other-integrations/setting-up-dbeaver-jdbc-connection-to-firebolt/#adding-a-database-in-dbeaver","text":"1. From the top navigation menu, select Database > New Database Connection . 2. Type Firebolt in the search box, and select it from the list of databases. 3. Select Next . 4. Fill in the following parameters: Parameter Description JDBC URL Use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name> In the path above, be sure to replace <db_name> with the name of your Firebolt database. This enables you to query the database using its default engine. If you wish to use a different engine than your default, use the following URL: jdbc:firebolt://api.app.firebolt.io/<db_name>?engine=<engineName> In the path above, replace <engineName> with the name of the engine you would like to use. Username Your Firebolt username. Password Your Firebolt password 5. Select Test Connection (make sure to start your database before you start the test). A successful connection test looks as follows: 6. Select Finish.","title":"Adding a database in DBeaver"},{"location":"integrations/other-integrations/setting-up-dbeaver-jdbc-connection-to-firebolt/#querying-your-firebolt-database","text":"In the database navigator, right-click on the database connection and select SQL Editor . If a pop-up window appears, select New Script : 2. The SQL editor should now open and you can run your queries.","title":"Querying your Firebolt database"},{"location":"loading-data/","text":"Loading data Loading data into Firebolt is very simple - you can try it in our getting started tutorial . The process is composed of the following steps: Create an external table to connect your external data source to Firebolt - read more here . Create a firebolt table (FACT/DIMENSION) - read more on working with tables here . Load data from the external data source into Firebolt using INSERT INTO command. Additional related topics:","title":"Loading data"},{"location":"loading-data/#loading-data","text":"Loading data into Firebolt is very simple - you can try it in our getting started tutorial . The process is composed of the following steps: Create an external table to connect your external data source to Firebolt - read more here . Create a firebolt table (FACT/DIMENSION) - read more on working with tables here . Load data from the external data source into Firebolt using INSERT INTO command. Additional related topics:","title":"Loading data"},{"location":"loading-data/configuring-aws-role-to-access-amazon-s3/","text":"Using AWS roles to access Amazon S3 Firebolt uses AWS Identity and Access Management (IAM) permissions to load data from Amazon S3 into Firebolt. This requires you to set up permissions using the AWS Management Console. This topic provides instructions for setting up AWS IAM permissions. Create an IAM policy Create the IAM role Increase the max session duration of your role Use the role to load your data into Firebolt Step 1: Create an IAM permissions policy Log in to the AWS Identity and Access Management (IAM) Console . From the left navigation panel, choose Account settings . Under Security Token Service (STS), in the Endpoints list, find the Region name where your account is located. If the status is Inactive , choose Activate . Choose Policies from the left navigation panel. Click Create Policy. Click the JSON tab. Add a policy document that will allow Firebolt to access the S3 bucket and folder. The following policy (in JSON format) provides Firebolt with the required permissions to unload data using a single bucket and folder path. Copy and paste the text into the policy editor (make sure to replace <bucket> and <prefix> with the actual bucket name and path prefix). javascript { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:GetObjectVersion\" ], \"Resource\": \"arn:aws:s3:::<bucket>/<prefix>/*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:GetBucketLocation\", \"Resource\": \"arn:aws:s3:::<bucket>\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::<bucket>\", \"Condition\": { \"StringLike\": { \"s3:prefix\": [ \"<prefix>/*\" ] } } } ] } If you encounter the following error: Access Denied (Status Code: 403; Error Code: AccessDenied) - try to remove the following condition from the IAM policy: javascript \"Condition\": { \"StringLike\": { \"s3:prefix\": [ \"<prefix>/*\" ] } } Choose Review policy , enter the policy Name (for example, firebolt-s3-access ), enter an optional Description , and then choose Create policy . Setting the s3:prefix condition key to * grants access to all prefixes in the specified bucket for the action to which it applies. Step 2: Create the IAM role In the AWS Management Console, create an AWS IAM role. The IAM role will assume the permissions you defined in step 1 to access the S3 locations where your data files are saved. Log in to the AWS Identity and Access Management (IAM) Console . From the left navigation panel, choose Roles , and then choose Create role . Select Another AWS account as the trusted entity type. In the Account ID field, enter your Firebolt AWS Account ID . Users with Account Admin privileges can view this value in the Account & Billing window . If you select Require external ID , enter a value of your choosing and make a note of it. Choose Next . Begin typing the name of the policy you created in step 1 in the search box, select it from the list, and then choose Next . Enter a Name and optional Description for the role, and then choose Create role . You have now created an IAM permissions policy, an IAM role for Firebolt access, and attached the permissions policy to the role. Record the Role ARN listed on the role summary page. Step 3: Increase the max session duration for your AWS role Log in to the AWS Identity and Access Management (IAM) Console . From the left navigation panel, choose Roles . Begin typing the name of the role that you created in step 2 and then select it from the list. In the summary section, locate the Maximum session duration field. Click Edit Choose 12 hours from the list, and then click Save changes. Step 4: Use the AWS Role for loading your data into Firebolt Loading your data into Firebolt begins by creating an external table - full syntax can be found here . Make sure to specify both the role's ARN and the external ID you've recorded during step 2 in the external table's CREDENTIALS specifier. Keep on reading in our getting started tutorial for the full data load flow.","title":"Using AWS roles to access Amazon S3"},{"location":"loading-data/configuring-aws-role-to-access-amazon-s3/#using-aws-roles-to-access-amazon-s3","text":"Firebolt uses AWS Identity and Access Management (IAM) permissions to load data from Amazon S3 into Firebolt. This requires you to set up permissions using the AWS Management Console. This topic provides instructions for setting up AWS IAM permissions. Create an IAM policy Create the IAM role Increase the max session duration of your role Use the role to load your data into Firebolt","title":"Using AWS roles to access Amazon S3"},{"location":"loading-data/configuring-aws-role-to-access-amazon-s3/#step-1-create-an-iam-permissions-policy","text":"Log in to the AWS Identity and Access Management (IAM) Console . From the left navigation panel, choose Account settings . Under Security Token Service (STS), in the Endpoints list, find the Region name where your account is located. If the status is Inactive , choose Activate . Choose Policies from the left navigation panel. Click Create Policy. Click the JSON tab. Add a policy document that will allow Firebolt to access the S3 bucket and folder. The following policy (in JSON format) provides Firebolt with the required permissions to unload data using a single bucket and folder path. Copy and paste the text into the policy editor (make sure to replace <bucket> and <prefix> with the actual bucket name and path prefix). javascript { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:GetObjectVersion\" ], \"Resource\": \"arn:aws:s3:::<bucket>/<prefix>/*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:GetBucketLocation\", \"Resource\": \"arn:aws:s3:::<bucket>\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::<bucket>\", \"Condition\": { \"StringLike\": { \"s3:prefix\": [ \"<prefix>/*\" ] } } } ] } If you encounter the following error: Access Denied (Status Code: 403; Error Code: AccessDenied) - try to remove the following condition from the IAM policy: javascript \"Condition\": { \"StringLike\": { \"s3:prefix\": [ \"<prefix>/*\" ] } } Choose Review policy , enter the policy Name (for example, firebolt-s3-access ), enter an optional Description , and then choose Create policy . Setting the s3:prefix condition key to * grants access to all prefixes in the specified bucket for the action to which it applies.","title":"Step 1: Create an IAM permissions policy "},{"location":"loading-data/configuring-aws-role-to-access-amazon-s3/#step-2-create-the-iam-role","text":"In the AWS Management Console, create an AWS IAM role. The IAM role will assume the permissions you defined in step 1 to access the S3 locations where your data files are saved. Log in to the AWS Identity and Access Management (IAM) Console . From the left navigation panel, choose Roles , and then choose Create role . Select Another AWS account as the trusted entity type. In the Account ID field, enter your Firebolt AWS Account ID . Users with Account Admin privileges can view this value in the Account & Billing window . If you select Require external ID , enter a value of your choosing and make a note of it. Choose Next . Begin typing the name of the policy you created in step 1 in the search box, select it from the list, and then choose Next . Enter a Name and optional Description for the role, and then choose Create role . You have now created an IAM permissions policy, an IAM role for Firebolt access, and attached the permissions policy to the role. Record the Role ARN listed on the role summary page.","title":"Step 2: Create the IAM role "},{"location":"loading-data/configuring-aws-role-to-access-amazon-s3/#step-3-increase-the-max-session-duration-for-your-aws-role","text":"Log in to the AWS Identity and Access Management (IAM) Console . From the left navigation panel, choose Roles . Begin typing the name of the role that you created in step 2 and then select it from the list. In the summary section, locate the Maximum session duration field. Click Edit Choose 12 hours from the list, and then click Save changes.","title":"Step 3: Increase the max session duration for your AWS role "},{"location":"loading-data/configuring-aws-role-to-access-amazon-s3/#step-4-use-the-aws-role-for-loading-your-data-into-firebolt","text":"Loading your data into Firebolt begins by creating an external table - full syntax can be found here . Make sure to specify both the role's ARN and the external ID you've recorded during step 2 in the external table's CREDENTIALS specifier. Keep on reading in our getting started tutorial for the full data load flow.","title":"Step 4: Use the AWS Role for loading your data into Firebolt"},{"location":"loading-data/continuously-loading-data/","text":"Continuously loading data tutorial This tutorial describes the steps for continuously loading data into Firebolt. In order to continuously load the data into Firebolt, we need to schedule the loading workflow. In order to achieve that, in this guide, we are using Apache Airflow which is a platform that enables to programmatically schedule and monitor workflows. Pre-requisites Before we start, make sure you have the following in-place: An active Firebolt account. Apache Airflow up and running. A database. Follow the create your first database section in the getting started tutorial . An external table. Follow step 1 - create an external table in the getting started tutorial . A fact or dimension table to load your data into. In this tutorial, we have chosen to create a fact table. Continuously loading data into a dimension table is similar. On top of the columns required in the table's schema - make sure to add the following columns as well: SOURCE_FILE_NAME of type TEXT , and SOURCE_FILE_TIMESTAMP of type TIMESTAMP . Run the following command on the database created in step 1 in order to create the fact table required for this tutorial: CREATE FACT TABLE IF NOT EXISTS lineitem_detailed ( l_orderkey LONG, l_partkey LONG, l_suppkey LONG, l_linenumber INT, l_quantity LONG, l_extendedprice LONG, l_discount LONG, l_tax LONG, l_returnflag TEXT, l_linestatus TEXT, l_shipdate TEXT, l_commitdate TEXT, l_receiptdate TEXT, l_shipinstruct TEXT, l_shipmode TEXT, l_comment TEXT, source_file_name TEXT, -- required for cont. loading data source_file_timestamp TIMESTAMP -- required for cont. loading data ) PRIMARY INDEX l_orderkey, l_linenumber; Step 1: Setup an Airflow connection to Firebolt Apache Airflow supports several kinds of connectors. In this tutorial, we use the JDBC connector. Perform the following steps in our setting up Airflow JDBC to Firebolt guide: Install the latest Firebolt JDBC driver in Airflow - read more here . Setup the JDBC connection to Firebolt in Airflow - read more here . In this guide, we assume the Conn ID is Firebolt . Step 2: Create a DAG for continuously loading data into Firebolt Apache Airflow supports running DAG scripts written in Python. Below is a script that enables to continuously load data into Firebolt: from airflow import DAG from airflow.operators.jdbc_operator import JdbcOperator default_arg = {'owner': 'airflow', 'start_date': '2020-10-20'} dag = DAG('firebolt_continuous_load_dag', default_args=default_arg, schedule_interval='* * * * *') data_load = JdbcOperator(dag=dag, jdbc_conn_id='firebolt', task_id='data_ingestion', sql=['INSERT INTO lineitem_detailed SELECT *, source_file_name, source_file_timestamp FROM ex_lineitem WHERE source_file_timestamp > ( SELECT MAX(source_file_timestamp) FROM lineitem_detailed )']) data_load Load command The script contains a single step called data_load . It connects to a database in Firebolt via a JDBC connector and runs the following INSERT INTO command: INSERT INTO lineitem_detailed SELECT *, source_file_name, source_file_timestamp FROM ex_lineitem WHERE source_file_timestamp > ( SELECT MAX(source_file_timestamp) FROM lineitem_detailed ) We use the SOURCE_FILE_NAME and SOURCE_FILE_TIMESTAMP external table metadata column to decide which files to load into Firebolt. We load only the new files that were added to S3 based on the SOURCE_FILE_TIMESTAMP column. Scheduling Airflow scheduler can run each DAG according to a predefined schedule. The schedule is being determined by the schedule_interval DAG parameter. It supports either a cron expression or several predefined intervals (read more in the Airflow documentation ). We have set it to '* * * * *' using a cron expression which instructs the Airflow scheduler to run the DAG every 1 minute. Step 3: Run the DAG In Airflow's UI, go to the DAGs tab. Locate your DAG in the list (in our case we should look for 'firebolt_continuous_load_dag' ): Turn the DAG on so it runs automatically every minute. Once the DAG has started to run, click on it's Run Id under the Last Run column to move to the graph view and track its progress. In the DAG's graph view, the task should appear in green to confirm the DAG was completed successfully. Click on the task 'data_load' : Click on View Logs to inspect the logs.","title":"Continuously loading data tutorial"},{"location":"loading-data/continuously-loading-data/#continuously-loading-data-tutorial","text":"This tutorial describes the steps for continuously loading data into Firebolt. In order to continuously load the data into Firebolt, we need to schedule the loading workflow. In order to achieve that, in this guide, we are using Apache Airflow which is a platform that enables to programmatically schedule and monitor workflows.","title":"Continuously loading data tutorial"},{"location":"loading-data/continuously-loading-data/#pre-requisites","text":"Before we start, make sure you have the following in-place: An active Firebolt account. Apache Airflow up and running. A database. Follow the create your first database section in the getting started tutorial . An external table. Follow step 1 - create an external table in the getting started tutorial . A fact or dimension table to load your data into. In this tutorial, we have chosen to create a fact table. Continuously loading data into a dimension table is similar. On top of the columns required in the table's schema - make sure to add the following columns as well: SOURCE_FILE_NAME of type TEXT , and SOURCE_FILE_TIMESTAMP of type TIMESTAMP . Run the following command on the database created in step 1 in order to create the fact table required for this tutorial: CREATE FACT TABLE IF NOT EXISTS lineitem_detailed ( l_orderkey LONG, l_partkey LONG, l_suppkey LONG, l_linenumber INT, l_quantity LONG, l_extendedprice LONG, l_discount LONG, l_tax LONG, l_returnflag TEXT, l_linestatus TEXT, l_shipdate TEXT, l_commitdate TEXT, l_receiptdate TEXT, l_shipinstruct TEXT, l_shipmode TEXT, l_comment TEXT, source_file_name TEXT, -- required for cont. loading data source_file_timestamp TIMESTAMP -- required for cont. loading data ) PRIMARY INDEX l_orderkey, l_linenumber;","title":"Pre-requisites"},{"location":"loading-data/continuously-loading-data/#step-1-setup-an-airflow-connection-to-firebolt","text":"Apache Airflow supports several kinds of connectors. In this tutorial, we use the JDBC connector. Perform the following steps in our setting up Airflow JDBC to Firebolt guide: Install the latest Firebolt JDBC driver in Airflow - read more here . Setup the JDBC connection to Firebolt in Airflow - read more here . In this guide, we assume the Conn ID is Firebolt .","title":"Step 1: Setup an Airflow connection to Firebolt"},{"location":"loading-data/continuously-loading-data/#step-2-create-a-dag-for-continuously-loading-data-into-firebolt","text":"Apache Airflow supports running DAG scripts written in Python. Below is a script that enables to continuously load data into Firebolt: from airflow import DAG from airflow.operators.jdbc_operator import JdbcOperator default_arg = {'owner': 'airflow', 'start_date': '2020-10-20'} dag = DAG('firebolt_continuous_load_dag', default_args=default_arg, schedule_interval='* * * * *') data_load = JdbcOperator(dag=dag, jdbc_conn_id='firebolt', task_id='data_ingestion', sql=['INSERT INTO lineitem_detailed SELECT *, source_file_name, source_file_timestamp FROM ex_lineitem WHERE source_file_timestamp > ( SELECT MAX(source_file_timestamp) FROM lineitem_detailed )']) data_load","title":"Step 2: Create a DAG for continuously loading data into Firebolt"},{"location":"loading-data/continuously-loading-data/#load-command","text":"The script contains a single step called data_load . It connects to a database in Firebolt via a JDBC connector and runs the following INSERT INTO command: INSERT INTO lineitem_detailed SELECT *, source_file_name, source_file_timestamp FROM ex_lineitem WHERE source_file_timestamp > ( SELECT MAX(source_file_timestamp) FROM lineitem_detailed ) We use the SOURCE_FILE_NAME and SOURCE_FILE_TIMESTAMP external table metadata column to decide which files to load into Firebolt. We load only the new files that were added to S3 based on the SOURCE_FILE_TIMESTAMP column.","title":"Load command"},{"location":"loading-data/continuously-loading-data/#scheduling","text":"Airflow scheduler can run each DAG according to a predefined schedule. The schedule is being determined by the schedule_interval DAG parameter. It supports either a cron expression or several predefined intervals (read more in the Airflow documentation ). We have set it to '* * * * *' using a cron expression which instructs the Airflow scheduler to run the DAG every 1 minute.","title":"Scheduling"},{"location":"loading-data/continuously-loading-data/#step-3-run-the-dag","text":"In Airflow's UI, go to the DAGs tab. Locate your DAG in the list (in our case we should look for 'firebolt_continuous_load_dag' ): Turn the DAG on so it runs automatically every minute. Once the DAG has started to run, click on it's Run Id under the Last Run column to move to the graph view and track its progress. In the DAG's graph view, the task should appear in green to confirm the DAG was completed successfully. Click on the task 'data_load' : Click on View Logs to inspect the logs.","title":"Step 3: Run the DAG"},{"location":"loading-data/loading-data-into-firebolt/","text":"Working with external tables Loading data into Firebolt is done using EXTERNAL TABLEs. Those tables are different from fact and dimension tables since the data is stored externally in Amazon S3 and not inside the database. Using external tables, Firebolt can access files stored in your S3 bucket with ease and allow you to load those files into Firebolt. Note External tables in Firebolt can be used to run any SQL query against the external data. However, it is recommended to use them for data ingestion workloads only . Analytic queries on external tables can run successfully but are expected to be relatively slow compared to fact or dimension tables. In order to create an external table - run the CREATE EXTERNAL TABLE command. Once you create an external table, use the INSERT INTO command to load the data from the external table into a fact or dimension table. Workflows For a simple end-to-end workflow that demonstrates loading data into Firebolt, see the getting started tutorial . For a workflow that demonstrates continuously loading data into Firebolt, see the continuously loading data tutorial . Supported file formats Firebolt supports loading the following source file formats from S3: PARQUET , CSV , TSV , JSON , and ORC . We are quick to add support for more types, so make sure to let us know if you need it. Using metadata virtual columns Firebolt external tables include metadata virtual columns that Firebolt populates with useful system data during ingestion. Firebolt includes these columns automatically. You don't need to specify them in the CREATE EXTERNAL TABLE statement. When you use an external table to ingest data, you can explicitly reference these columns to ingest the metadata. First, you define the columns in a CREATE FACT|DIMENSION TABLE statement. Next, you specify the virtual column names to select in the INSERT INTO statement, with the fact or dimension table as the target. You can then query the columns in the fact or dimension table for analysis and troubleshooting. For more information, see the example below. The metadata virtual columns listed below are available in external tables. Metadata column name Description Data type source_file_name The full path of the row's source file in Amazon S3. For Kafka-connected external tables, this is NULL . TEXT source_file_timestamp The creation date of the row's source file in S3. For Kafka-connected external tables, this is NULL . TIMESTAMP Example - querying metadata virtual column values The query example below creates an external table that references an AWS S3 bucket that contains Parquet files for Firebolt to ingest. CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = 'AKIAIOSFODNN7EXAMPLE' AWS_SECRET_KEY = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET); The query example below creates a dimension table to be the target for data ingestion. CREATE DIMENSION TABLE my_dim_table_with_metadata ( c_id INT UNIQUE c_name TEXT, source_file_name TEXT, source_file_timestamp TIMESTAMP, ); The query example below uses my_external_table to ingest the Parquet data into my_dim_table_with_metadata . The statement explicitly specifies the metadata virtual columns in the SELECT clause, which is a requirement. INSERT INTO my_dim_table_with_metadata SELECT *, source_file_name, source_file_timestamp FROM my_external_table; Finally, the query example below retrieves all records in my_dim_table_with_metadata. SELECT * FROM my_dim_table_with_metadata; The query returns output similar to the following. ------------------------------------------------------------------------------- |c_id |c_name |source_file_name |source_file_timestamp ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- |12385 |ClevelandDC8933 |cle.parquet |2021-09-10 10:32:03 |12386 |PortlandXfer9483 |pdx.parquet |2021-09-10 10:32:04 |12387 |NashvilleXfer9987 |bna.parquet |2021-09-10 10:33:01 |12388 |ClevelandXfer8998 |cle.parquet |2021-09-10 10:32:03 [...]","title":"Working with external tables"},{"location":"loading-data/loading-data-into-firebolt/#working-with-external-tables","text":"Loading data into Firebolt is done using EXTERNAL TABLEs. Those tables are different from fact and dimension tables since the data is stored externally in Amazon S3 and not inside the database. Using external tables, Firebolt can access files stored in your S3 bucket with ease and allow you to load those files into Firebolt. Note External tables in Firebolt can be used to run any SQL query against the external data. However, it is recommended to use them for data ingestion workloads only . Analytic queries on external tables can run successfully but are expected to be relatively slow compared to fact or dimension tables. In order to create an external table - run the CREATE EXTERNAL TABLE command. Once you create an external table, use the INSERT INTO command to load the data from the external table into a fact or dimension table.","title":"Working with external tables"},{"location":"loading-data/loading-data-into-firebolt/#workflows","text":"For a simple end-to-end workflow that demonstrates loading data into Firebolt, see the getting started tutorial . For a workflow that demonstrates continuously loading data into Firebolt, see the continuously loading data tutorial .","title":"Workflows"},{"location":"loading-data/loading-data-into-firebolt/#supported-file-formats","text":"Firebolt supports loading the following source file formats from S3: PARQUET , CSV , TSV , JSON , and ORC . We are quick to add support for more types, so make sure to let us know if you need it.","title":"Supported file formats"},{"location":"loading-data/loading-data-into-firebolt/#using-metadata-virtual-columns","text":"Firebolt external tables include metadata virtual columns that Firebolt populates with useful system data during ingestion. Firebolt includes these columns automatically. You don't need to specify them in the CREATE EXTERNAL TABLE statement. When you use an external table to ingest data, you can explicitly reference these columns to ingest the metadata. First, you define the columns in a CREATE FACT|DIMENSION TABLE statement. Next, you specify the virtual column names to select in the INSERT INTO statement, with the fact or dimension table as the target. You can then query the columns in the fact or dimension table for analysis and troubleshooting. For more information, see the example below. The metadata virtual columns listed below are available in external tables. Metadata column name Description Data type source_file_name The full path of the row's source file in Amazon S3. For Kafka-connected external tables, this is NULL . TEXT source_file_timestamp The creation date of the row's source file in S3. For Kafka-connected external tables, this is NULL . TIMESTAMP","title":"Using metadata virtual columns"},{"location":"loading-data/loading-data-into-firebolt/#example-querying-metadata-virtual-column-values","text":"The query example below creates an external table that references an AWS S3 bucket that contains Parquet files for Firebolt to ingest. CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = 'AKIAIOSFODNN7EXAMPLE' AWS_SECRET_KEY = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET); The query example below creates a dimension table to be the target for data ingestion. CREATE DIMENSION TABLE my_dim_table_with_metadata ( c_id INT UNIQUE c_name TEXT, source_file_name TEXT, source_file_timestamp TIMESTAMP, ); The query example below uses my_external_table to ingest the Parquet data into my_dim_table_with_metadata . The statement explicitly specifies the metadata virtual columns in the SELECT clause, which is a requirement. INSERT INTO my_dim_table_with_metadata SELECT *, source_file_name, source_file_timestamp FROM my_external_table; Finally, the query example below retrieves all records in my_dim_table_with_metadata. SELECT * FROM my_dim_table_with_metadata; The query returns output similar to the following. ------------------------------------------------------------------------------- |c_id |c_name |source_file_name |source_file_timestamp ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- |12385 |ClevelandDC8933 |cle.parquet |2021-09-10 10:32:03 |12386 |PortlandXfer9483 |pdx.parquet |2021-09-10 10:32:04 |12387 |NashvilleXfer9987 |bna.parquet |2021-09-10 10:33:01 |12388 |ClevelandXfer8998 |cle.parquet |2021-09-10 10:32:03 [...]","title":"Example - querying metadata virtual column values"},{"location":"sql-functions-reference/aggregation-functions/","text":"Aggregation functions This page describes the aggregation functions supported in Firebolt. MIN Calculates the minimum value of an expression across all input values. Syntax \u200b\u200bMIN(expr) Parameter Description expr The expression used to calculate the minimum values. Valid values for the expression include column names or functions that return a column name. MAX Calculates the maximum value of an expression across all input values. Syntax \u200b\u200bMAX(expr) Parameter Description expr The expression used to calculate the maximum values. Valid values for the expression include column names or functions that return a column name. MAX_BY The MAX_BY function returns the value of arg column at the row in which the val column is maximal. If there is more than one maximal values in val the first will be used. Syntax MAX_BY(arg, val) Parameter Description arg The column from which the value is returned. val The column from which the maximum value is searched. Usage example Assume we have the following prices table: item price apple 4 banana 25 orange 11 kiwi 20 The query MAX_BY(item, price) Will result in 'banana' MIN_BY The MIN_BY function returns the value of arg column at the row in which the val column is minimal. If there is more than one minimal values in val the first will be used. Syntax MIN_BY(arg, val) Parameter Description arg The column from which the value is returned. val The column from which the maximum value is searched. Usage example Assume we have the following prices table: item price apple 4 banana 25 orange 11 kiwi 20 The query MIN_BY(item, price) Will result in 'apple' SUM Calculates the sum of an expression. Syntax \u200b\u200bSUM(expr)\u200b\u200b Parameter Description expr The expression used to calculate the sum. Valid values for the expression include column names or functions that return a column name for columns that contain numeric values. AVG Calculates the average of an expression Syntax \u200b\u200bAVG(expr)\u200b\u200b Parameter Description expr The expression used to calculate the average. Valid values for the expression include column names or functions that return a column name for columns that contain numeric values. The AVG() aggregate function dismisses rows with NULL value; so an AVG from 3 rows containing 1, 2 and NULL values results in 1.5 as the NULL row is dismissed. For accurate AVG(), use SUM(COLUMN)/COUNT(*) COUNT Counts the number of rows or not NULL values. Syntax COUNT([ DISTINCT ] expr) Parameter Description expr Valid values for the expression include column names (or * for counting all columns) or functions that return a column name for columns that contain numeric values. When DISTINCT is being used, counts only the unique number of rows with not NULL values. Notes COUNT(*) will return a total count of rows in the table, while COUNT(<column_name>) will return a count of rows with a non-NULL value in that particular column. By default, COUNT DISTINCT will return approximate results. If you wish to get an accurate result (with a performance penalty), set the following parameter : SET firebolt_optimization_enable_exact_count_distinct=1; Usage example SELECT COUNT(DISTINCT col) FROM my_table; MEDIAN Calculates an approximate median using reservoir sampling for the given expression. Syntax \u200b\u200bMEDIAN(expr)\u200b\u200b Parameter Description col Expression over the column values that returns numeric (INT, FLOAT, etc), DATE or DATETIME data types. ANY_VALUE Returns one arbitrarily value from the given column. Syntax \u200b\u200bANY_VALUE(col)\u200b\u200b Parameter Description col The column from which the value is returned. NEST Takes a column as an argument, and returns an array of the values. See the full description under Semi-structured data functions. STDDEV_SAMP Computes the standard deviation of a sample consisting of a numeric-expression. Syntax STDDEV_SAMP(expr)\u200b Parameter Description expr Any column with numeric values or expression that returns a column with numeric values.","title":"Aggregation functions"},{"location":"sql-functions-reference/aggregation-functions/#aggregation-functions","text":"This page describes the aggregation functions supported in Firebolt.","title":"Aggregation functions"},{"location":"sql-functions-reference/aggregation-functions/#min","text":"Calculates the minimum value of an expression across all input values. Syntax \u200b\u200bMIN(expr) Parameter Description expr The expression used to calculate the minimum values. Valid values for the expression include column names or functions that return a column name.","title":"MIN"},{"location":"sql-functions-reference/aggregation-functions/#max","text":"Calculates the maximum value of an expression across all input values. Syntax \u200b\u200bMAX(expr) Parameter Description expr The expression used to calculate the maximum values. Valid values for the expression include column names or functions that return a column name.","title":"MAX"},{"location":"sql-functions-reference/aggregation-functions/#max_by","text":"The MAX_BY function returns the value of arg column at the row in which the val column is maximal. If there is more than one maximal values in val the first will be used. Syntax MAX_BY(arg, val) Parameter Description arg The column from which the value is returned. val The column from which the maximum value is searched. Usage example Assume we have the following prices table: item price apple 4 banana 25 orange 11 kiwi 20 The query MAX_BY(item, price) Will result in 'banana'","title":"MAX_BY"},{"location":"sql-functions-reference/aggregation-functions/#min_by","text":"The MIN_BY function returns the value of arg column at the row in which the val column is minimal. If there is more than one minimal values in val the first will be used. Syntax MIN_BY(arg, val) Parameter Description arg The column from which the value is returned. val The column from which the maximum value is searched. Usage example Assume we have the following prices table: item price apple 4 banana 25 orange 11 kiwi 20 The query MIN_BY(item, price) Will result in 'apple'","title":"MIN_BY"},{"location":"sql-functions-reference/aggregation-functions/#sum","text":"Calculates the sum of an expression. Syntax \u200b\u200bSUM(expr)\u200b\u200b Parameter Description expr The expression used to calculate the sum. Valid values for the expression include column names or functions that return a column name for columns that contain numeric values.","title":"SUM"},{"location":"sql-functions-reference/aggregation-functions/#avg","text":"Calculates the average of an expression Syntax \u200b\u200bAVG(expr)\u200b\u200b Parameter Description expr The expression used to calculate the average. Valid values for the expression include column names or functions that return a column name for columns that contain numeric values. The AVG() aggregate function dismisses rows with NULL value; so an AVG from 3 rows containing 1, 2 and NULL values results in 1.5 as the NULL row is dismissed. For accurate AVG(), use SUM(COLUMN)/COUNT(*)","title":"AVG"},{"location":"sql-functions-reference/aggregation-functions/#count","text":"Counts the number of rows or not NULL values. Syntax COUNT([ DISTINCT ] expr) Parameter Description expr Valid values for the expression include column names (or * for counting all columns) or functions that return a column name for columns that contain numeric values. When DISTINCT is being used, counts only the unique number of rows with not NULL values. Notes COUNT(*) will return a total count of rows in the table, while COUNT(<column_name>) will return a count of rows with a non-NULL value in that particular column. By default, COUNT DISTINCT will return approximate results. If you wish to get an accurate result (with a performance penalty), set the following parameter : SET firebolt_optimization_enable_exact_count_distinct=1; Usage example SELECT COUNT(DISTINCT col) FROM my_table;","title":"COUNT"},{"location":"sql-functions-reference/aggregation-functions/#median","text":"Calculates an approximate median using reservoir sampling for the given expression. Syntax \u200b\u200bMEDIAN(expr)\u200b\u200b Parameter Description col Expression over the column values that returns numeric (INT, FLOAT, etc), DATE or DATETIME data types.","title":"MEDIAN"},{"location":"sql-functions-reference/aggregation-functions/#any_value","text":"Returns one arbitrarily value from the given column. Syntax \u200b\u200bANY_VALUE(col)\u200b\u200b Parameter Description col The column from which the value is returned.","title":"ANY_VALUE"},{"location":"sql-functions-reference/aggregation-functions/#nest","text":"Takes a column as an argument, and returns an array of the values. See the full description under Semi-structured data functions.","title":"NEST"},{"location":"sql-functions-reference/aggregation-functions/#stddev_samp","text":"Computes the standard deviation of a sample consisting of a numeric-expression. Syntax STDDEV_SAMP(expr)\u200b Parameter Description expr Any column with numeric values or expression that returns a column with numeric values.","title":"STDDEV_SAMP"},{"location":"sql-functions-reference/conditional-and-miscellaneous-functions/","text":"Conditional and miscellaneous functions This page describes the conditional and miscellaneous functions supported in Firebolt. COALESCE Checks from left to right for the first non-NULL argument found for each entry parameter pair. For example, for an Employee table (where each employee can have more than one location), check multiple location parameters, find the first non-null pair per employee (the first location with data per employee). Syntax \u200b\u200bCOALESCE(value,\u2026) Parameter Description value The value(s) to coalesce. Can be either: column name, \u200b \u200ba function applied on a column (or on another function), and a literal (constant value). Usage example SELECT COALESCE(null, \"London\",\"New York\") AS res; Returns: London CAST Converts data types into other data types based on the specified parameters. Syntax CAST(value AS type) Parameter Description value The original value to be converted or an expression that translates into the original value. Can be either:\u200b \u200bcolumn name, \u200b \u200ba function applied on a column (or on another function), and a literal (constant value). type The target data type (case-insensitive). Usage example SELECT CAST('1' AS INT) as res; Returns: 1 CASE The CASE expression is a conditional expression similar to if-then-else statements. If the result of the condition is true then the value of the CASE expression is the result that follows the condition. \u200b If the result is false any subsequent WHEN clauses (conditions) are searched in the same manner. \u200b If no WHEN condition is true then the value of the case expression is the result specified in the ELSE clause. \u200b If the ELSE clause is omitted and no condition matches, the result is null. Syntax CASE WHEN condition THEN result [WHEN ...] [ELSE result] END; Parameter Description condition An expression that returns a boolean result. \u200b A condition can be defined for each WHEN, and ELSE clause. result The result of any condition. Every \u200b THEN \u200b\u200bclause receives a single result. All results in a single \u200b CASE \u200b\u200bfunction must share the same data type. Usage example In a table listing movies by title and length, the following query categorizes each entry by length. If the movie is longer than zero minutes and less than 50 minutes it is categorized as SHORT. When the length is 50-120 minutes, it's categorized as Medium, and when even longer, it's categorized as Long. SELECT title, length, CASE WHEN length > 0 AND length <= 50 THEN 'Short' WHEN length > 50 AND length <= 120 THEN 'Medium' WHEN length > 120 THEN 'Long' END duration FROM film ORDER BY title;","title":"Conditional and miscellaneous functions"},{"location":"sql-functions-reference/conditional-and-miscellaneous-functions/#conditional-and-miscellaneous-functions","text":"This page describes the conditional and miscellaneous functions supported in Firebolt.","title":"Conditional and miscellaneous functions"},{"location":"sql-functions-reference/conditional-and-miscellaneous-functions/#coalesce","text":"Checks from left to right for the first non-NULL argument found for each entry parameter pair. For example, for an Employee table (where each employee can have more than one location), check multiple location parameters, find the first non-null pair per employee (the first location with data per employee). Syntax \u200b\u200bCOALESCE(value,\u2026) Parameter Description value The value(s) to coalesce. Can be either: column name, \u200b \u200ba function applied on a column (or on another function), and a literal (constant value). Usage example SELECT COALESCE(null, \"London\",\"New York\") AS res; Returns: London","title":"COALESCE"},{"location":"sql-functions-reference/conditional-and-miscellaneous-functions/#cast","text":"Converts data types into other data types based on the specified parameters. Syntax CAST(value AS type) Parameter Description value The original value to be converted or an expression that translates into the original value. Can be either:\u200b \u200bcolumn name, \u200b \u200ba function applied on a column (or on another function), and a literal (constant value). type The target data type (case-insensitive). Usage example SELECT CAST('1' AS INT) as res; Returns: 1","title":"CAST"},{"location":"sql-functions-reference/conditional-and-miscellaneous-functions/#case","text":"The CASE expression is a conditional expression similar to if-then-else statements. If the result of the condition is true then the value of the CASE expression is the result that follows the condition. \u200b If the result is false any subsequent WHEN clauses (conditions) are searched in the same manner. \u200b If no WHEN condition is true then the value of the case expression is the result specified in the ELSE clause. \u200b If the ELSE clause is omitted and no condition matches, the result is null. Syntax CASE WHEN condition THEN result [WHEN ...] [ELSE result] END; Parameter Description condition An expression that returns a boolean result. \u200b A condition can be defined for each WHEN, and ELSE clause. result The result of any condition. Every \u200b THEN \u200b\u200bclause receives a single result. All results in a single \u200b CASE \u200b\u200bfunction must share the same data type. Usage example In a table listing movies by title and length, the following query categorizes each entry by length. If the movie is longer than zero minutes and less than 50 minutes it is categorized as SHORT. When the length is 50-120 minutes, it's categorized as Medium, and when even longer, it's categorized as Long. SELECT title, length, CASE WHEN length > 0 AND length <= 50 THEN 'Short' WHEN length > 50 AND length <= 120 THEN 'Medium' WHEN length > 120 THEN 'Long' END duration FROM film ORDER BY title;","title":"CASE"},{"location":"sql-functions-reference/date-and-time-functions/","text":"Date and time functions This page describes the date and time functions and format expressions supported in Firebolt. CURRENT_DATE Returns the current date. Syntax \u200b\u200bCURRENT_DATE()\u200b\u200b NOW Returns the current date and time. Syntax \u200b\u200bNOW()\u200b\u200b TIMEZONE Accepts zero arguments and returns the current timezone of request execution Syntax \u200b\u200bTIMEZONE()\u200b\u200b TO_STRING Converts a date into a STRING. The date is any date data type\u200b\u200b . Syntax TO_STRING(date) Parameter Description date The date to be converted to a string. Usage example SELECT TO_STRING(NOW()); This que r y returns today's date and the current time similar to the following: \u200b 2022-10-10 22:22:33 DATE_ADD Calculates a new date based on the specified unit, interval, and indicated date. Syntax \u200b\u200bDATE_ADD(unit, interval, date_expr)\u200b\u200b Parameter Description unit A valid unit is SECOND | MINUTE | HOUR | DAY | WEEK | YEAR | EPOCH interval The number of times to increase the \u200b date_expr\u200b\u200b . Usage example SELECT DATE_ADD('WEEK', 15, CAST ('2017-06-15 09:34:21' AS DATETIME)); Returns: 2017-09-28 09:34:21 FROM_UNIXTIME Convert Unix time (LONG in epoch seconds) to DATETIME (YYYY-MM-DD HH:mm:ss). Syntax \u200b\u200bFROM_UNIXTIME(unix_time)\u200b\u200b Parameter Description unix_time The UNIX epoch time that is to be converted. Usage example SELECT FROM_UNIXTIME(1493971667); Returns: 2017-05-05 08:07:47 DATE_DIFF Difference between \u200b\u200b start_date \u200b\u200b and \u200b end_date \u200b\u200b expressed in the indicated \u200bunit\u200b\u200b. Syntax \u200b\u200bDATE_DIFF(unit, start_date, end_date)\u200b\u200b Parameter Description unit Time unit, in which the returned value is to be expressed (String). \u200b Supported values: second | minute | hour | day | week | month | quarter | year |. start_date The first time (in DATE or DATETIME format) value to be used to calculate the difference. end_date The last time (in DATE or DATETIME format) value to be used to calculate the difference. Usage examples SELECT DATE_DIFF('hour', CAST('2020/08/31 10:00:00' AS DATETIME), CAST('2020/08/31 11:00:00' AS DATETIME)); Returns: 1 SELECT DATE_DIFF('day', CAST('2020/08/31 10:00:00' AS DATETIME), CAST('2020/08/31 11:00:00' AS DATETIME)); Returns: 0 DATE_TRUNC Truncate a given date to a specified position. Syntax \u200b\u200bDATE_TRUNC(precision, date)\u200b\u200b Parameter Description precision Time unit, in which the returned value is to be expressed (String). \u200b Supported values: second | minute | hour | day | week | month | quarter | year |. date The date which needs to be truncated (DATE or DATETIME) Usage example SELECT DATE_TRUNC('minute', CAST('2020/08/31 10:31:36' AS TIMESTAMP)); Returns: 2020-08-31 10:31:00 DATE_FORMAT Formats a \u200b DATE \u200b\u200bor \u200b DATETIME \u200b\u200baccording to the given format expression. Syntax \u200b\u200bDATE_FORMAT(date, format)\u200b\u200b Parameter Description date The date to be formatted format The format to be used for the output. Formats are described \u200b here\u200b\u200b . Usage example SELECT DATE_FORMAT(CAST('2020/08/31 10:33:44' AS DATETIME), '%Y-%m-%d') as res; Returns: 2020-08-31 EXTRACT Retrieves subfields such as year or hour from date/time values. Syntax \u200b\u200bEXTRACT(field FROM source)\u200b\u200b Parameter Description field Supported fields: DAY | MONTH | YEAR | HOUR | MINUTE | SECOND | EPOCH source A value expression of type timestamp. Usage example SELECT EXTRACT(YEAR FROM TIMESTAMP '2020-01-01 10:00:00') AS res; Returns: 2020 TO_YEAR Converts a date or timestamp (any date format we support) to a number containing the year. Syntax \u200b\u200bTO_YEAR(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the year. Usage example For Tuesday, April 22, 1975: SELECT TO_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns: 1975 TO_QUARTER Converts a date or timestamp (any date format we support) to a number containing the quarter. Syntax \u200b\u200bTO_QUARTER(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the quarter. Usage example For Tuesday, April 22, 1975: SELECT TO_QUARTER(CAST('1975/04/22' AS DATE)) as res; Returns: 2 TO_MONTH Converts a date or timestamp (any date format we support) to a number containing the month. Syntax \u200b\u200bTO_MONTH(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the month. Usage example For Tuesday, April 22, 1975: SELECT TO_MONTH(CAST('1975/04/22' AS DATE)) as res; Returns: 4 TO_WEEK Converts a date or timestamp (any date format we support) to a number containing the week. Syntax \u200b\u200bTO_WEEK(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the week. Usage example For Tuesday, April 22, 1975: SELECT TO_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns: 16 TO_DAY_OF_WEEK Converts a date or timestamp (any date format we support) to a number containing the number of the day of the week (Monday is 1, and Sunday is 7). Syntax \u200b\u200bTO_DAY_OF_WEEK(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the day of the week. Usage example For Tuesday, April 22, 1975: SELECT TO_DAY_OF_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns: 2 TO_DAY_OF_YEAR Converts a date or timestamp (any date format we support) to a number containing the number of the day of the year. Syntax \u200b\u200bTO_DAY_OF_YEAR(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the day of the year. Usage example For Tuesday, April 22, 1975: SELECT TO_DAY_OF_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns: 112 TO_HOUR Converts a timestamp (any date format we support) to a number containing the hour. Syntax \u200b\u200bTO_HOUR(timestamp)\u200b\u200b Parameter Description timestamp The timestamp to be converted into the number of the hour. Usage example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_HOUR(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 12 TO_MINUTE Converts a timestamp (any date format we support) to a number containing the minute. Syntax \u200b\u200bTO_MINUTE(timestamp)\u200b\u200b Parameter Description timestamp The timestamp to be converted into the number of the minute. Usage example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_MINUTE(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 20 TO_SECOND Converts a timestamp (any date format we support) to a number containing the second. Syntax \u200b\u200bTO_SECOND(timestamp)\u200b\u200b Parameter Description timestamp The timestamp to be converted into the number of the second. Usage example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_SECOND(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 5 Date format expressions The following table details the supported constant expression format syntax options for the DATE_FORMAT function. Expression Description Examples for Tuesday the 2nd of April, 1975 at 12:24:48:13 past midnight %C The year divided by 100 and truncated to integer (00-99) 19 %d Day of the month, zero-padded (01-31) 02 %D Short MM/DD/YY date, equivalent to %m/%d/%y 04/02/75 %e Day of the month, space-padded ( 1-31) 2 %F Short YYYY-MM-DD date, equivalent to %Y-%m-%d 1975-04-02 %H The hour in 24h format (00-23) 00 %I The hour in 12h format (01-12) 12 %j Day of the year (001-366) 112 %m month as a decimal number (01-12) 04 %M minute (00-59) 24 %n new-line character (\u2018\u2019) in order to add a new line in the converted format. Returns: 1975 04 %p AM or PM designation PM %R 24-hour HH:MM time, equivalent to %H:%M 00:24 %S The second (00-59) 48 %T ISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S 00:24:48 %u ISO 8601 weekday as number with Monday as 1 (1-7) 2 %V ISO 8601 week number (01-53) 17 %w weekday as a decimal number with Sunday as 0 (0-6) 2 %y Year, last two digits (00-99) 75 %Y Year 1975 %% a % sign %","title":"Date and time functions"},{"location":"sql-functions-reference/date-and-time-functions/#date-and-time-functions","text":"This page describes the date and time functions and format expressions supported in Firebolt.","title":"Date and time functions"},{"location":"sql-functions-reference/date-and-time-functions/#current_date","text":"Returns the current date. Syntax \u200b\u200bCURRENT_DATE()\u200b\u200b","title":"CURRENT_DATE"},{"location":"sql-functions-reference/date-and-time-functions/#now","text":"Returns the current date and time. Syntax \u200b\u200bNOW()\u200b\u200b","title":"NOW"},{"location":"sql-functions-reference/date-and-time-functions/#timezone","text":"Accepts zero arguments and returns the current timezone of request execution Syntax \u200b\u200bTIMEZONE()\u200b\u200b","title":"TIMEZONE"},{"location":"sql-functions-reference/date-and-time-functions/#to_string","text":"Converts a date into a STRING. The date is any date data type\u200b\u200b . Syntax TO_STRING(date) Parameter Description date The date to be converted to a string. Usage example SELECT TO_STRING(NOW()); This que r y returns today's date and the current time similar to the following: \u200b 2022-10-10 22:22:33","title":"TO_STRING"},{"location":"sql-functions-reference/date-and-time-functions/#date_add","text":"Calculates a new date based on the specified unit, interval, and indicated date. Syntax \u200b\u200bDATE_ADD(unit, interval, date_expr)\u200b\u200b Parameter Description unit A valid unit is SECOND | MINUTE | HOUR | DAY | WEEK | YEAR | EPOCH interval The number of times to increase the \u200b date_expr\u200b\u200b . Usage example SELECT DATE_ADD('WEEK', 15, CAST ('2017-06-15 09:34:21' AS DATETIME)); Returns: 2017-09-28 09:34:21","title":"DATE_ADD"},{"location":"sql-functions-reference/date-and-time-functions/#from_unixtime","text":"Convert Unix time (LONG in epoch seconds) to DATETIME (YYYY-MM-DD HH:mm:ss). Syntax \u200b\u200bFROM_UNIXTIME(unix_time)\u200b\u200b Parameter Description unix_time The UNIX epoch time that is to be converted. Usage example SELECT FROM_UNIXTIME(1493971667); Returns: 2017-05-05 08:07:47","title":"FROM_UNIXTIME"},{"location":"sql-functions-reference/date-and-time-functions/#date_diff","text":"Difference between \u200b\u200b start_date \u200b\u200b and \u200b end_date \u200b\u200b expressed in the indicated \u200bunit\u200b\u200b. Syntax \u200b\u200bDATE_DIFF(unit, start_date, end_date)\u200b\u200b Parameter Description unit Time unit, in which the returned value is to be expressed (String). \u200b Supported values: second | minute | hour | day | week | month | quarter | year |. start_date The first time (in DATE or DATETIME format) value to be used to calculate the difference. end_date The last time (in DATE or DATETIME format) value to be used to calculate the difference. Usage examples SELECT DATE_DIFF('hour', CAST('2020/08/31 10:00:00' AS DATETIME), CAST('2020/08/31 11:00:00' AS DATETIME)); Returns: 1 SELECT DATE_DIFF('day', CAST('2020/08/31 10:00:00' AS DATETIME), CAST('2020/08/31 11:00:00' AS DATETIME)); Returns: 0","title":"DATE_DIFF"},{"location":"sql-functions-reference/date-and-time-functions/#date_trunc","text":"Truncate a given date to a specified position. Syntax \u200b\u200bDATE_TRUNC(precision, date)\u200b\u200b Parameter Description precision Time unit, in which the returned value is to be expressed (String). \u200b Supported values: second | minute | hour | day | week | month | quarter | year |. date The date which needs to be truncated (DATE or DATETIME) Usage example SELECT DATE_TRUNC('minute', CAST('2020/08/31 10:31:36' AS TIMESTAMP)); Returns: 2020-08-31 10:31:00","title":"DATE_TRUNC"},{"location":"sql-functions-reference/date-and-time-functions/#date_format","text":"Formats a \u200b DATE \u200b\u200bor \u200b DATETIME \u200b\u200baccording to the given format expression. Syntax \u200b\u200bDATE_FORMAT(date, format)\u200b\u200b Parameter Description date The date to be formatted format The format to be used for the output. Formats are described \u200b here\u200b\u200b . Usage example SELECT DATE_FORMAT(CAST('2020/08/31 10:33:44' AS DATETIME), '%Y-%m-%d') as res; Returns: 2020-08-31","title":"DATE_FORMAT"},{"location":"sql-functions-reference/date-and-time-functions/#extract","text":"Retrieves subfields such as year or hour from date/time values. Syntax \u200b\u200bEXTRACT(field FROM source)\u200b\u200b Parameter Description field Supported fields: DAY | MONTH | YEAR | HOUR | MINUTE | SECOND | EPOCH source A value expression of type timestamp. Usage example SELECT EXTRACT(YEAR FROM TIMESTAMP '2020-01-01 10:00:00') AS res; Returns: 2020","title":"EXTRACT"},{"location":"sql-functions-reference/date-and-time-functions/#to_year","text":"Converts a date or timestamp (any date format we support) to a number containing the year. Syntax \u200b\u200bTO_YEAR(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the year. Usage example For Tuesday, April 22, 1975: SELECT TO_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns: 1975","title":"TO_YEAR"},{"location":"sql-functions-reference/date-and-time-functions/#to_quarter","text":"Converts a date or timestamp (any date format we support) to a number containing the quarter. Syntax \u200b\u200bTO_QUARTER(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the quarter. Usage example For Tuesday, April 22, 1975: SELECT TO_QUARTER(CAST('1975/04/22' AS DATE)) as res; Returns: 2","title":"TO_QUARTER"},{"location":"sql-functions-reference/date-and-time-functions/#to_month","text":"Converts a date or timestamp (any date format we support) to a number containing the month. Syntax \u200b\u200bTO_MONTH(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the month. Usage example For Tuesday, April 22, 1975: SELECT TO_MONTH(CAST('1975/04/22' AS DATE)) as res; Returns: 4","title":"TO_MONTH"},{"location":"sql-functions-reference/date-and-time-functions/#to_week","text":"Converts a date or timestamp (any date format we support) to a number containing the week. Syntax \u200b\u200bTO_WEEK(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the week. Usage example For Tuesday, April 22, 1975: SELECT TO_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns: 16","title":"TO_WEEK"},{"location":"sql-functions-reference/date-and-time-functions/#to_day_of_week","text":"Converts a date or timestamp (any date format we support) to a number containing the number of the day of the week (Monday is 1, and Sunday is 7). Syntax \u200b\u200bTO_DAY_OF_WEEK(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the day of the week. Usage example For Tuesday, April 22, 1975: SELECT TO_DAY_OF_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns: 2","title":"TO_DAY_OF_WEEK"},{"location":"sql-functions-reference/date-and-time-functions/#to_day_of_year","text":"Converts a date or timestamp (any date format we support) to a number containing the number of the day of the year. Syntax \u200b\u200bTO_DAY_OF_YEAR(date)\u200b\u200b Parameter Description date The date or timestamp to be converted into the number of the day of the year. Usage example For Tuesday, April 22, 1975: SELECT TO_DAY_OF_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns: 112","title":"TO_DAY_OF_YEAR"},{"location":"sql-functions-reference/date-and-time-functions/#to_hour","text":"Converts a timestamp (any date format we support) to a number containing the hour. Syntax \u200b\u200bTO_HOUR(timestamp)\u200b\u200b Parameter Description timestamp The timestamp to be converted into the number of the hour. Usage example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_HOUR(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 12","title":"TO_HOUR"},{"location":"sql-functions-reference/date-and-time-functions/#to_minute","text":"Converts a timestamp (any date format we support) to a number containing the minute. Syntax \u200b\u200bTO_MINUTE(timestamp)\u200b\u200b Parameter Description timestamp The timestamp to be converted into the number of the minute. Usage example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_MINUTE(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 20","title":"TO_MINUTE"},{"location":"sql-functions-reference/date-and-time-functions/#to_second","text":"Converts a timestamp (any date format we support) to a number containing the second. Syntax \u200b\u200bTO_SECOND(timestamp)\u200b\u200b Parameter Description timestamp The timestamp to be converted into the number of the second. Usage example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_SECOND(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 5","title":"TO_SECOND"},{"location":"sql-functions-reference/date-and-time-functions/#date-format-expressions","text":"The following table details the supported constant expression format syntax options for the DATE_FORMAT function. Expression Description Examples for Tuesday the 2nd of April, 1975 at 12:24:48:13 past midnight %C The year divided by 100 and truncated to integer (00-99) 19 %d Day of the month, zero-padded (01-31) 02 %D Short MM/DD/YY date, equivalent to %m/%d/%y 04/02/75 %e Day of the month, space-padded ( 1-31) 2 %F Short YYYY-MM-DD date, equivalent to %Y-%m-%d 1975-04-02 %H The hour in 24h format (00-23) 00 %I The hour in 12h format (01-12) 12 %j Day of the year (001-366) 112 %m month as a decimal number (01-12) 04 %M minute (00-59) 24 %n new-line character (\u2018\u2019) in order to add a new line in the converted format. Returns: 1975 04 %p AM or PM designation PM %R 24-hour HH:MM time, equivalent to %H:%M 00:24 %S The second (00-59) 48 %T ISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S 00:24:48 %u ISO 8601 weekday as number with Monday as 1 (1-7) 2 %V ISO 8601 week number (01-53) 17 %w weekday as a decimal number with Sunday as 0 (0-6) 2 %y Year, last two digits (00-99) 75 %Y Year 1975 %% a % sign %","title":"Date format expressions"},{"location":"sql-functions-reference/semi-structured-functions/","text":"Semi-structured data functions This page describes the semi-structured data functions supported in \u200bFirebolt\u200b. NEST Takes a column as an argument, and returns an array of the values. In case the type of the column is nullable, the NULL values will be ignored. Syntax \u200b\u200bNEST(col)\u200b\u200b Parameter Description col The name of the column to be referenced. Usage example Assume we have the following prices table: item price apple 4 banana NULL orange 11 kiwi 20 Running the following query: SELECT NEST(price) as arr from prices; Returns: [4,11,20] ALL_MATCH The ALL_MATCH function results indicate whether all elements of an array match the results of the function indicated in the parameters. Following are the possible values that can be returned: 1 if a function is not specified or if all the elements match the function 0 if one or more elements don\u2019t match NULL if the predicate function returns NULL for one or more elements true for all other elements Syntax ALL_MATCH([func,] arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty. Usage example SELECT ALL_MATCH(x -> x > 0, [1,2,3,9]) AS res; Returns: 1 ANY_MATCH The results of the ANY_MATCH function indicate whether at least one of the elements of an array matches the results of the function indicated in the parameters. Following are the possible values that can be returned: true if one or more elements match the function; false if none of the elements matches; NULL if the predicate function returns NULL for one or more elements and false for all other elements. 1 if a function is not specified. Syntax \u200b\u200bANY_MATCH([func,] arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty Usage example SELECT ANY_MATCH(x -> x > 3, [1,2,3,9]) AS res; Returns: 1 ARRAY_COUNT This function returns the number of elements in the arr array for which the function that is indicated in the parameters returns something other than 0. If no function is specified, it returns the number of non-zero elements in the array. Syntax ARRAY_COUNT([function] arr) Parameter Description function The function to be tested. arr The array to be used to check the function. Usage examples SELECT ARRAY_COUNT(x -> x > 3, [1,2,3,9]) AS res; Returns: 1 SELECT ARRAY_COUNT([1,2,3,9]) AS res; Returns: 4 ARRAY_COUNT_GLOBAL This function returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array to be checked or an array typed column Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr)) ARRAY_CUMULATIVE_SUM This function returns an array of partial sums of elements from the source array (a running sum). If the function is specified, then the values of the array elements are converted by this function before summing. Syntax ARRAY_CUMULATIVE_SUM([func,] arr) Parameter Description func The function used to convert the array members. arr The array to be used for the sum calculations. Usage examples SELECT ARRAY_CUMULATIVE_SUM(x -> x + 1, [1,2,3,9]) AS res; Returns: 2,5,9,19 SELECT ARRAY_CUMULATIVE_SUM([1,2,3,9]) AS res Returns: 1,3,6,15 ARRAY_FILL This function scans through the given array from the first element to the last element and replaces arr[i] by arr[i - 1] if the function that is indicated returns 0. The first element of the given array is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_FILL(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples SELECT ARRAY_FILL(x -> x < 0,[1,2,3,9]) AS res; Returns: 1,1,1,1 SELECT ARRAY_FILL(x -> x > 0,[1,2,3,9]) AS res; Returns: 1,2,3,9 FILTER The FILTER function returns an array that only contains the elements in the given array for which the given func function returns something other than 0. The function can receive one or more arrays as its arguments. If more than one array is provided the following conditions should be met: The number of arguments of the lambda function should be equal to the number of arrays provided. If the condition isn't met - the query will not run and an error will be returned. All the provided array should be of the same length. If the condition isn't met a runtime error will occur. When multiple arrays are provided to the function, the function will receive the current element from each array as its actual parameter, and can use any or all of them to produce the result which will detemine if the element at this index will be included in the result of the function, however, the elements of the result are always taken from the first array provided. Syntax FILTER(func, arr[, arr1, arr2...] ) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT FILTER(x, y -> y > 2,['a','b','c','d'],[1,2,3,9]) AS res; Returns: ['c', 'd'] ARRAY_FIRST Returns the first element in the given array for which the given func function returns something other than 0. The first argument (lambda function) can\u2019t be omitted. Syntax ARRAY_FIRST(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST(x -> x > 2,[1,2,3,9]) AS res; Returns: 3 ARRAY_FIRST_INDEX Returns the index of the first element in the indicated array for which the given func function returns something other than 0. The first argument (lambda function) is mandatory. Syntax ARRAY_FIRST_INDEX(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST_INDEX(x -> x > 2,[1,2,3,9]) AS res; Returns: 3 ARRAY_UNNEST This function \"unfolds\" the given array by creating individual members from the array's values. Syntax ARRAY_UNNEST(arr) Parameter Description arr The array to be unfolded. Usage example SELECT ARRAY_UNNEST([1,2,3,4]) AS res; Returns: res 1 2 3 4 ARRAY_REPLACE_BACKWARDS This function scans the indicated array from the last element to the first element and replaces each of the elements in that array with arr[i + 1]. If the given function returns 0, the last element of the given array is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_REPLACE_BACKWARDS(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_REPLACE_BACKWARDS(x -> x > 2,[1,2,3,9]) AS res; Returns: 3,3,3,9 TRANSFORM This function returns a resulting array based on the calculation of the given array members using the given func function. The first argument (lambda function) is mandatory. Syntax TRANSFORM(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT TRANSFORM(x -> x * 2,[1,2,3,9]) AS res; Returns: 2,4,6,18 ARRAY_SORT This function sorts the members of the given array in ascending order. If a function is specified, the sorting order is determined by the result of the function. Syntax ARRAY_SORT([func,] arr) Parameter Description func An optional function to be used to determine the sort order. arr The array to be sorted. Usage examples SELECT ARRAY_SORT([4,1,3,2]) AS res; Returns: 1,2,3,4 SELECT ARRAY_SORT(x -> x * 2,[4,1,3,2]) AS res; Returns: 1,2,3,4 ARRAY_SUM This function calculates given func function using the members of the given array. When the function results in 0 or 1, the number of values in the array that the function returned 1 for is returned. If the function is omitted, only the sum of the array elements is returned. Syntax ARRAY_SUM([func,] arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples A regular transformation function: SELECT ARRAY_SUM(x -> x + 1,[4,1,3,2]) AS res; Returns: 14 A function that returns 0/1: SELECT ARRAY_SUM(x -> x > 1,[4,1,3,2]) AS res; Returns: 3 No function: SELECT ARRAY_SUM([4,1,3,2]) AS res; Returns: 10 SUM(ARRAY_COUNT(arr)) Assuming that arr is an array typed column, the following is equivalent Parameter Description arr The array column over which the function will count the elements ARRAY_COUNT_GLOBAL(arr) Syntax This function returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function. ARRAY_SUM_GLOBAL This function returns the sum of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_SUM_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr)) ARRAY_CONCAT This function combines arrays that are passed as arguments. Syntax ARRAY_CONCAT(arr1 [,arr\u2026]) Parameter Description arr The arrays to be combined. If only one array is given, the identical array is returned. Usage Example SELECT ARRAY_CONCAT([1,2,3,4],[5,6,7,8]) AS res; Returns:1,2,3,4,5,6,7,8 ARRAY_DISTINCT This function returns an array containing only the unique elements of the given array. In other words, if the given array contains multiple identical members, the returned array will include only a single member of that value. Syntax ARRAY_DISTINCT(arr) Parameter Description arr The array to be analyzed for unique members. Usage Example SELECT ARRAY_DISTINCT([1,1,2,2,3,4]) AS res; Returns: 1,2,3,4 ELEMENT_AT This function retrieves the element with the index n from the given array. n must be any integer type. Indexes in an array begin from one (1). Syntax ELEMENT_AT(arr, n) Parameter Description arr The array containing the index. n The index that is matched by the function. Negative indexes are supported. If used, the function selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. Usage Example SELECT ELEMENT_AT([1,2,3,4],2) AS res; Returns: 2 FLATTEN This function converts an array of arrays into a flat array. That is, for every element that is an array, this function extracts its elements into the new array. The resulting flattened array contains all the elements from all source arrays. The function: Applies to any depth of nested arrays. Does not change arrays that are already flat. Syntax FLATTEN(arr_of_arrs) Parameter Description arr_of_arrs The array of arrays to be flattened. Usage Example SELECT flatten([[[1,2]], [[2,3], [3,4]]]) Returns: (1, 2, 2, 3, 3, 4) ARRAY_INTERSECT This function takes multiple arrays and returns an array containing the elements that are present in all source arrays. The order of the resulting array is the same as in the first array. Syntax ARRAY_INTERSECT(arr) Parameter Description arr Each of the arrays analyzed for mutual elements. Usage Example SELECT ARRAY_INTERSECT([1, 2, 3], [1, 3], [2, 3]) Returns: 3 ARRAY_JOIN This function concatenates the elements of the given array of TEXT using the delimiter (optional). In case the delimiter is not provided, an empty string is being used. Syntax ARRAY_INTERSECT(arr) Parameter Description arr An array of TEXT elements. delimiter The delimiter used for joining the array elements. If you omit this value, an empty string is being used as a delimiter. Usage Example SELECT ARRAY_JOIN(['1', '2', '3']) AS res; Returns: 123 REDUCE This function applies an aggregate function to array elements and returns its result. The name of the aggregation function is passed as a string in single quotes - for example: 'max', 'sum'. When using parametric aggregate functions, the parameter is indicated after the function name in parentheses 'func_name(param)'. Syntax REDUCE(agg_function, arr) Parameter Description agg_function The name of an aggregate function which should be a constant string arr Any number of array type columns as the parameters of the aggregation function. Usage Example SELECT REDUCE('max', [1, 2, 3, 6]) AS res; Returns: 6 ARRAY_REVERSE This function returns an array of the same size as the original array, with the elements in reverse order. Syntax ARRAY_REVERSE(arr) Parameter Description arr The array to be reversed. Usage Example SELECT ARRAY_REVERSE([1, 2, 3, 6]) AS res Returns: 6,3,2,1 SLICE This function returns a \"slice\" of the array based on the indicated offset and length. Syntax SLICE(arr, offset[, length]) Parameter Description arr The array of data to be sliced. Array elements set to NULL are handled as normal values. The numbering of the array items begins with 1. offset Indicates the indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. length The length of the required slice. If you specify a negative value, the function returns an open slice [offset, array_length - length). If you omit this value, the function returns the slice [offset, the_end_of_array]. Usage Example SELECT SLICE([1, 2, NULL, 4, 5], 2, 3) AS res Returns: 2,,4 ARRAY_UNIQ If one argument is passed, this function counts the number of different elements in the array. If multiple arguments are passed, it counts the number of different tuples of elements at corresponding positions in multiple arrays. Syntax ARRAY_UNIQ(arr, ...) Parameter Description arr The array to be analyzed. Usage Example SELECT ARRAY_UNIQ([1, 2, 4, 5]) AS res; Returns: 4 INDEX_OF This function returns the position of the first occurrence of the element in the array (or 0 if not found). Syntax INDEX_OF(arr, x) Parameter Description arr The array to be analyzed. x The element from the array that is to be matched. Usage Example SELECT INDEX_OF([1, 3, 5, 7], 5) AS res; Returns: 3 LENGTH This function returns the length of the given array. Syntax LENGTH(arr) Parameter Description arr The array to be checked for length. Usage Example SELECT LENGTH([1, 2, 3, 4]) AS res; Returns: 4 CONTAINS The function checks whether the given array has the given element. Returns 0 if the element is not in the array, or 1 if it is. NULL is processed as a value. Syntax CONTAINS(arr, x) Parameter Description arr The array to be checked for the given element. x The element to be searched for within the array. Usage Example SELECT CONTAINS([1, 2, 3], 3) AS res; Returns: 1 ARRAY_MAX This function returns the maximum element per each array in the array typed column. In-case an array is provided as literal - the function returns the maximum element in it. Syntax ARRAY_MAX(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MAX([1,2,3,4]) AS res; Returns: 4 ARRAY_MIN This function returns the minimum element per each array in the array typed column. In-case an array is provided as literal - the function returns the minimum element in it. Syntax ARRAY_MIN(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MIN([1,2,3,4]) AS res; Returns: 1 ARRAY_MAX_GLOBAL This function returns the maximum element across all arrays in the array typed column. As such it is an aggregation function. In-case an array is provided as literal (see the below example) - the function returns the maximum element in it. Syntax ARRAY_MAX_GLOBAL(arr) Parameter Description arr The array to be checked or an array typed column Assuming that arr is an array typed column, the following is equivalent: SELECT MAX(ARRAY_MAX(arr)) AS res; Usage Example SELECT ARRAY_MAX_GLOBAL([1,2,3,4]) AS res; Returns: 4 ARRAY_MIN_GLOBAL This function returns the minimum element across all arrays in the array typed column. As such it is an aggregation function. In-case an array is provided as literal (see the below example) - the function returns the minimum element in it. Syntax ARRAY_MIN_GLOBAL(arr) Parameter Description arr The array to be checked or an array typed column Assuming that arr is an array typed column, the following is equivalent: SELECT MIN(ARRAY_MIN(arr)) AS res; Usage Example SELECT ARRAY_MIN_GLOBAL([1,2,3,4]) AS res; Returns: 1","title":"Semi-structured data functions"},{"location":"sql-functions-reference/semi-structured-functions/#semi-structured-data-functions","text":"This page describes the semi-structured data functions supported in \u200bFirebolt\u200b.","title":"Semi-structured data functions"},{"location":"sql-functions-reference/semi-structured-functions/#nest","text":"Takes a column as an argument, and returns an array of the values. In case the type of the column is nullable, the NULL values will be ignored. Syntax \u200b\u200bNEST(col)\u200b\u200b Parameter Description col The name of the column to be referenced. Usage example Assume we have the following prices table: item price apple 4 banana NULL orange 11 kiwi 20 Running the following query: SELECT NEST(price) as arr from prices; Returns: [4,11,20]","title":"NEST"},{"location":"sql-functions-reference/semi-structured-functions/#all_match","text":"The ALL_MATCH function results indicate whether all elements of an array match the results of the function indicated in the parameters. Following are the possible values that can be returned: 1 if a function is not specified or if all the elements match the function 0 if one or more elements don\u2019t match NULL if the predicate function returns NULL for one or more elements true for all other elements Syntax ALL_MATCH([func,] arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty. Usage example SELECT ALL_MATCH(x -> x > 0, [1,2,3,9]) AS res; Returns: 1","title":"ALL_MATCH"},{"location":"sql-functions-reference/semi-structured-functions/#any_match","text":"The results of the ANY_MATCH function indicate whether at least one of the elements of an array matches the results of the function indicated in the parameters. Following are the possible values that can be returned: true if one or more elements match the function; false if none of the elements matches; NULL if the predicate function returns NULL for one or more elements and false for all other elements. 1 if a function is not specified. Syntax \u200b\u200bANY_MATCH([func,] arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty Usage example SELECT ANY_MATCH(x -> x > 3, [1,2,3,9]) AS res; Returns: 1","title":"ANY_MATCH"},{"location":"sql-functions-reference/semi-structured-functions/#array_count","text":"This function returns the number of elements in the arr array for which the function that is indicated in the parameters returns something other than 0. If no function is specified, it returns the number of non-zero elements in the array. Syntax ARRAY_COUNT([function] arr) Parameter Description function The function to be tested. arr The array to be used to check the function. Usage examples SELECT ARRAY_COUNT(x -> x > 3, [1,2,3,9]) AS res; Returns: 1 SELECT ARRAY_COUNT([1,2,3,9]) AS res; Returns: 4","title":"ARRAY_COUNT"},{"location":"sql-functions-reference/semi-structured-functions/#array_count_global","text":"This function returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array to be checked or an array typed column Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr))","title":"ARRAY_COUNT_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/#array_cumulative_sum","text":"This function returns an array of partial sums of elements from the source array (a running sum). If the function is specified, then the values of the array elements are converted by this function before summing. Syntax ARRAY_CUMULATIVE_SUM([func,] arr) Parameter Description func The function used to convert the array members. arr The array to be used for the sum calculations. Usage examples SELECT ARRAY_CUMULATIVE_SUM(x -> x + 1, [1,2,3,9]) AS res; Returns: 2,5,9,19 SELECT ARRAY_CUMULATIVE_SUM([1,2,3,9]) AS res Returns: 1,3,6,15","title":"ARRAY_CUMULATIVE_SUM"},{"location":"sql-functions-reference/semi-structured-functions/#array_fill","text":"This function scans through the given array from the first element to the last element and replaces arr[i] by arr[i - 1] if the function that is indicated returns 0. The first element of the given array is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_FILL(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples SELECT ARRAY_FILL(x -> x < 0,[1,2,3,9]) AS res; Returns: 1,1,1,1 SELECT ARRAY_FILL(x -> x > 0,[1,2,3,9]) AS res; Returns: 1,2,3,9","title":"ARRAY_FILL"},{"location":"sql-functions-reference/semi-structured-functions/#filter","text":"The FILTER function returns an array that only contains the elements in the given array for which the given func function returns something other than 0. The function can receive one or more arrays as its arguments. If more than one array is provided the following conditions should be met: The number of arguments of the lambda function should be equal to the number of arrays provided. If the condition isn't met - the query will not run and an error will be returned. All the provided array should be of the same length. If the condition isn't met a runtime error will occur. When multiple arrays are provided to the function, the function will receive the current element from each array as its actual parameter, and can use any or all of them to produce the result which will detemine if the element at this index will be included in the result of the function, however, the elements of the result are always taken from the first array provided. Syntax FILTER(func, arr[, arr1, arr2...] ) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT FILTER(x, y -> y > 2,['a','b','c','d'],[1,2,3,9]) AS res; Returns: ['c', 'd']","title":"FILTER"},{"location":"sql-functions-reference/semi-structured-functions/#array_first","text":"Returns the first element in the given array for which the given func function returns something other than 0. The first argument (lambda function) can\u2019t be omitted. Syntax ARRAY_FIRST(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST(x -> x > 2,[1,2,3,9]) AS res; Returns: 3","title":"ARRAY_FIRST"},{"location":"sql-functions-reference/semi-structured-functions/#array_first_index","text":"Returns the index of the first element in the indicated array for which the given func function returns something other than 0. The first argument (lambda function) is mandatory. Syntax ARRAY_FIRST_INDEX(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST_INDEX(x -> x > 2,[1,2,3,9]) AS res; Returns: 3","title":"ARRAY_FIRST_INDEX"},{"location":"sql-functions-reference/semi-structured-functions/#array_unnest","text":"This function \"unfolds\" the given array by creating individual members from the array's values. Syntax ARRAY_UNNEST(arr) Parameter Description arr The array to be unfolded. Usage example SELECT ARRAY_UNNEST([1,2,3,4]) AS res; Returns: res 1 2 3 4","title":"ARRAY_UNNEST"},{"location":"sql-functions-reference/semi-structured-functions/#array_replace_backwards","text":"This function scans the indicated array from the last element to the first element and replaces each of the elements in that array with arr[i + 1]. If the given function returns 0, the last element of the given array is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_REPLACE_BACKWARDS(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_REPLACE_BACKWARDS(x -> x > 2,[1,2,3,9]) AS res; Returns: 3,3,3,9","title":"ARRAY_REPLACE_BACKWARDS"},{"location":"sql-functions-reference/semi-structured-functions/#transform","text":"This function returns a resulting array based on the calculation of the given array members using the given func function. The first argument (lambda function) is mandatory. Syntax TRANSFORM(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT TRANSFORM(x -> x * 2,[1,2,3,9]) AS res; Returns: 2,4,6,18","title":"TRANSFORM"},{"location":"sql-functions-reference/semi-structured-functions/#array_sort","text":"This function sorts the members of the given array in ascending order. If a function is specified, the sorting order is determined by the result of the function. Syntax ARRAY_SORT([func,] arr) Parameter Description func An optional function to be used to determine the sort order. arr The array to be sorted. Usage examples SELECT ARRAY_SORT([4,1,3,2]) AS res; Returns: 1,2,3,4 SELECT ARRAY_SORT(x -> x * 2,[4,1,3,2]) AS res; Returns: 1,2,3,4","title":"ARRAY_SORT"},{"location":"sql-functions-reference/semi-structured-functions/#array_sum","text":"This function calculates given func function using the members of the given array. When the function results in 0 or 1, the number of values in the array that the function returned 1 for is returned. If the function is omitted, only the sum of the array elements is returned. Syntax ARRAY_SUM([func,] arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples A regular transformation function: SELECT ARRAY_SUM(x -> x + 1,[4,1,3,2]) AS res; Returns: 14 A function that returns 0/1: SELECT ARRAY_SUM(x -> x > 1,[4,1,3,2]) AS res; Returns: 3 No function: SELECT ARRAY_SUM([4,1,3,2]) AS res; Returns: 10 SUM(ARRAY_COUNT(arr)) Assuming that arr is an array typed column, the following is equivalent Parameter Description arr The array column over which the function will count the elements ARRAY_COUNT_GLOBAL(arr) Syntax This function returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function.","title":"ARRAY_SUM"},{"location":"sql-functions-reference/semi-structured-functions/#array_sum_global","text":"This function returns the sum of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_SUM_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr))","title":"ARRAY_SUM_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/#array_concat","text":"This function combines arrays that are passed as arguments. Syntax ARRAY_CONCAT(arr1 [,arr\u2026]) Parameter Description arr The arrays to be combined. If only one array is given, the identical array is returned. Usage Example SELECT ARRAY_CONCAT([1,2,3,4],[5,6,7,8]) AS res; Returns:1,2,3,4,5,6,7,8","title":"ARRAY_CONCAT"},{"location":"sql-functions-reference/semi-structured-functions/#array_distinct","text":"This function returns an array containing only the unique elements of the given array. In other words, if the given array contains multiple identical members, the returned array will include only a single member of that value. Syntax ARRAY_DISTINCT(arr) Parameter Description arr The array to be analyzed for unique members. Usage Example SELECT ARRAY_DISTINCT([1,1,2,2,3,4]) AS res; Returns: 1,2,3,4","title":"ARRAY_DISTINCT"},{"location":"sql-functions-reference/semi-structured-functions/#element_at","text":"This function retrieves the element with the index n from the given array. n must be any integer type. Indexes in an array begin from one (1). Syntax ELEMENT_AT(arr, n) Parameter Description arr The array containing the index. n The index that is matched by the function. Negative indexes are supported. If used, the function selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. Usage Example SELECT ELEMENT_AT([1,2,3,4],2) AS res; Returns: 2","title":"ELEMENT_AT"},{"location":"sql-functions-reference/semi-structured-functions/#flatten","text":"This function converts an array of arrays into a flat array. That is, for every element that is an array, this function extracts its elements into the new array. The resulting flattened array contains all the elements from all source arrays. The function: Applies to any depth of nested arrays. Does not change arrays that are already flat. Syntax FLATTEN(arr_of_arrs) Parameter Description arr_of_arrs The array of arrays to be flattened. Usage Example SELECT flatten([[[1,2]], [[2,3], [3,4]]]) Returns: (1, 2, 2, 3, 3, 4)","title":"FLATTEN"},{"location":"sql-functions-reference/semi-structured-functions/#array_intersect","text":"This function takes multiple arrays and returns an array containing the elements that are present in all source arrays. The order of the resulting array is the same as in the first array. Syntax ARRAY_INTERSECT(arr) Parameter Description arr Each of the arrays analyzed for mutual elements. Usage Example SELECT ARRAY_INTERSECT([1, 2, 3], [1, 3], [2, 3]) Returns: 3","title":"ARRAY_INTERSECT"},{"location":"sql-functions-reference/semi-structured-functions/#array_join","text":"This function concatenates the elements of the given array of TEXT using the delimiter (optional). In case the delimiter is not provided, an empty string is being used. Syntax ARRAY_INTERSECT(arr) Parameter Description arr An array of TEXT elements. delimiter The delimiter used for joining the array elements. If you omit this value, an empty string is being used as a delimiter. Usage Example SELECT ARRAY_JOIN(['1', '2', '3']) AS res; Returns: 123","title":"ARRAY_JOIN"},{"location":"sql-functions-reference/semi-structured-functions/#reduce","text":"This function applies an aggregate function to array elements and returns its result. The name of the aggregation function is passed as a string in single quotes - for example: 'max', 'sum'. When using parametric aggregate functions, the parameter is indicated after the function name in parentheses 'func_name(param)'. Syntax REDUCE(agg_function, arr) Parameter Description agg_function The name of an aggregate function which should be a constant string arr Any number of array type columns as the parameters of the aggregation function. Usage Example SELECT REDUCE('max', [1, 2, 3, 6]) AS res; Returns: 6","title":"REDUCE"},{"location":"sql-functions-reference/semi-structured-functions/#array_reverse","text":"This function returns an array of the same size as the original array, with the elements in reverse order. Syntax ARRAY_REVERSE(arr) Parameter Description arr The array to be reversed. Usage Example SELECT ARRAY_REVERSE([1, 2, 3, 6]) AS res Returns: 6,3,2,1","title":"ARRAY_REVERSE"},{"location":"sql-functions-reference/semi-structured-functions/#slice","text":"This function returns a \"slice\" of the array based on the indicated offset and length. Syntax SLICE(arr, offset[, length]) Parameter Description arr The array of data to be sliced. Array elements set to NULL are handled as normal values. The numbering of the array items begins with 1. offset Indicates the indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. length The length of the required slice. If you specify a negative value, the function returns an open slice [offset, array_length - length). If you omit this value, the function returns the slice [offset, the_end_of_array]. Usage Example SELECT SLICE([1, 2, NULL, 4, 5], 2, 3) AS res Returns: 2,,4","title":"SLICE"},{"location":"sql-functions-reference/semi-structured-functions/#array_uniq","text":"If one argument is passed, this function counts the number of different elements in the array. If multiple arguments are passed, it counts the number of different tuples of elements at corresponding positions in multiple arrays. Syntax ARRAY_UNIQ(arr, ...) Parameter Description arr The array to be analyzed. Usage Example SELECT ARRAY_UNIQ([1, 2, 4, 5]) AS res; Returns: 4","title":"ARRAY_UNIQ"},{"location":"sql-functions-reference/semi-structured-functions/#index_of","text":"This function returns the position of the first occurrence of the element in the array (or 0 if not found). Syntax INDEX_OF(arr, x) Parameter Description arr The array to be analyzed. x The element from the array that is to be matched. Usage Example SELECT INDEX_OF([1, 3, 5, 7], 5) AS res; Returns: 3","title":"INDEX_OF"},{"location":"sql-functions-reference/semi-structured-functions/#length","text":"This function returns the length of the given array. Syntax LENGTH(arr) Parameter Description arr The array to be checked for length. Usage Example SELECT LENGTH([1, 2, 3, 4]) AS res; Returns: 4","title":"LENGTH"},{"location":"sql-functions-reference/semi-structured-functions/#contains","text":"The function checks whether the given array has the given element. Returns 0 if the element is not in the array, or 1 if it is. NULL is processed as a value. Syntax CONTAINS(arr, x) Parameter Description arr The array to be checked for the given element. x The element to be searched for within the array. Usage Example SELECT CONTAINS([1, 2, 3], 3) AS res; Returns: 1","title":"CONTAINS"},{"location":"sql-functions-reference/semi-structured-functions/#array_max","text":"This function returns the maximum element per each array in the array typed column. In-case an array is provided as literal - the function returns the maximum element in it. Syntax ARRAY_MAX(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MAX([1,2,3,4]) AS res; Returns: 4","title":"ARRAY_MAX"},{"location":"sql-functions-reference/semi-structured-functions/#array_min","text":"This function returns the minimum element per each array in the array typed column. In-case an array is provided as literal - the function returns the minimum element in it. Syntax ARRAY_MIN(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MIN([1,2,3,4]) AS res; Returns: 1","title":"ARRAY_MIN"},{"location":"sql-functions-reference/semi-structured-functions/#array_max_global","text":"This function returns the maximum element across all arrays in the array typed column. As such it is an aggregation function. In-case an array is provided as literal (see the below example) - the function returns the maximum element in it. Syntax ARRAY_MAX_GLOBAL(arr) Parameter Description arr The array to be checked or an array typed column Assuming that arr is an array typed column, the following is equivalent: SELECT MAX(ARRAY_MAX(arr)) AS res; Usage Example SELECT ARRAY_MAX_GLOBAL([1,2,3,4]) AS res; Returns: 4","title":"ARRAY_MAX_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/#array_min_global","text":"This function returns the minimum element across all arrays in the array typed column. As such it is an aggregation function. In-case an array is provided as literal (see the below example) - the function returns the minimum element in it. Syntax ARRAY_MIN_GLOBAL(arr) Parameter Description arr The array to be checked or an array typed column Assuming that arr is an array typed column, the following is equivalent: SELECT MIN(ARRAY_MIN(arr)) AS res; Usage Example SELECT ARRAY_MIN_GLOBAL([1,2,3,4]) AS res; Returns: 1","title":"ARRAY_MIN_GLOBAL"},{"location":"sql-functions-reference/string-functions/","text":"String functions This page describes the string functions supported in Firebolt. SUBSTR Returns a substring starting with the byte from the \u2018offset\u2019 index and \u2018length\u2019 bytes long. Character indexing starts from index 1 (as in standard SQL). The \u2018offset\u2019 and \u2018length\u2019 arguments must be constants. Syntax SUBSTR(s, offset, length) Parameter Description s The string to be offset. offset Count left to right the number of characters to offset from and not include in the results. 1 indicates starting with the first character. length The total amount of characters included in the returned result. Usage example SELECT SUBSTR('hello world', 1, 5); The string is offset by 1 and so begins with \"h\". 5 indicates the resulting string should be only five characters long. Returns: hello CONCAT Concatenates the strings listed in the arguments, without a separator. Syntax CONCAT(s1,s2,\u2026); Or S1 || S2 || S3 Parameter Description s The strings, to be concatenated. Usage example SELECT concat('Hello, ', 'World!') Returns: Hello World! REPLACE Replaces all occurrences of the \u2018pattern\u2019 substring of the \u2018haystack\u2019 string with the \u2018replacement\u2019 substring. Syntax REPLACE (haystack, pattern, replacement)\u200b Parameter Description haystack The entire original string pattern The substring to be searched and replaced in the string. replacement The substring to replace the original substring (the 'pattern'). Usage examples SELECT REPLACE('hello world','hello','nice') AS res; Returns:'nice world' SELECT REPLACE('hello world',' world','') AS res; Returns: 'hello' SELECT REPLACE('hello world','hi','something') AS res; Returns: 'hello world' String functions REGEXP_LIKE This function holds a regular expression, used to check whether a pattern matches the regular expression string. Returns 0 if it doesn\u2019t match, or 1 if it matches. \u200b This is a \u200b\u200bre2\u200b\u200b regular expression. Syntax \u200bREGEXP_LIKE(haystack, pattern)\u200b\u200b Parameter Description haystack The string in which to search for a match.pattern pattern The pattern with which to search for a match in the string. Usage examples \u200b\u200bSELECT REGEXP_LIKE('123','\\\\[a-z]') AS res;\u200b\u200b Returns: 0 SELECT REGEXP_LIKE('123','\\\\d+') AS res;\u200b Returns: 1 STRPOS Returns the position (in bytes) of the substring found in the string, starting from 1. Syntax \u200b\u200bSTRPOS(haystack, needle)\u200b\u200b Parameter Description haystack The string in which to search for the needle. needle The substring to be searched for. Usage examples SELECT STRPOS('hello world','hello') AS res Returns: 1 SELECT STRPOS('hello world','world') AS res Returns: 7 TRIM Removes all specified characters from the start or end of a string. By default removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. Syntax \u200b\u200bTRIM([LEADING|TRAILING|default = BOTH] <trim_character> FROM <target_string>)\u200b\u200b Parameter Description LEADING Removes all special characters from the beginning of the specified string. TRAILING Removes all special characters from the end of the specified string. BOTH Removes all special characters from the beginning and the end of the specified string. trim_character The characters to be removed. target_string The string to be trimmed. Usage example SELECT TRIM('$' FROM '$Hello world$') AS res; Returns: \u2018 Hello world\u2019 LTRIM Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bLTRIM(<target>)\u200b\u200b Parameter Description target The string to be trimmed. Usage example SELECT LTRIM(' Hello, world! ') Returns: Hello, world! RTRIM Removes all consecutive occurrences of common whitespace (ASCII character 32) from the end of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bRTRIM(<target>)\u200b\u200b Parameter Description target The string to be trimmed. Usage example SELECT RTRIM('Hello, world! ') Returns: Hello, world! BTRIM Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning and the end of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc). Syntax \u200b\u200bBTRIM(<target>)\u200b\u200b Parameter Description target The string to be trimmed. Usage example SELECT BTRIM('Hello, world! ') Returns: Hello, world! LPAD Adds the pad in front of the string repetitively up until the length of the resulting string is equivalent to the indicated length. Syntax \u200b\u200bLPAD(<str>, <length> [, <pad>])\u200b\u200b Parameter Description str The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b str can be a literal string or the name of a column. length The length of the string as an integer after it has been left-padded. \u200b A negative number returns an empty string. pad The string to left-pad to primary string (str). Usage example The following statement adds the string ABC in front of the string Firebolt repetitively until the resulting string is equivalent to 20 characters in length. SELECT LPAD(\"Firebolt\", 20, \"ABC\"); Returns: ABCABCABCABCFirebolt RPAD Adds the pad at the end of the string repetitively up until the length of the resulting string is equivalent to the indicated length. Syntax \u200b\u200bRPAD(<str>, <length> [, <pad>])\u200b\u200b Parameter Description str The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b str can be a literal string or the name of a column. length The length of the string as an integer after it has been left-padded. \u200b A negative number returns an empty string. pad The string to right-pad to primary string (str). Usage example The following statement adds the string ABC to the end of the string Firebolt repetitively until the resulting string is equivalent to 20 characters in length. SELECT RPAD(\"Firebolt\", 20, \"ABC\"); Returns: FireboltABCABCABCABC TO_DATE Converts a string to date. Syntax \u200b\u200bTO_DATE(string)\u200b\u200b Parameter Description string The string format should be: \u2018YYYY-MM-DD\u2019 Usage example SELECT TO_DATE('2020-05-31') AS res; Returns: 2020-05-31 TO_TIMESTAMP Converts a string to timestamp. Syntax \u200b\u200bTO_TIMESTAMP(string)\u200b\u200b Parameter Description string The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Usage example SELECT TO_TIMESTAMP('2020-05-31 10:31:14') AS res; Returns: 2020-05-31 10:31:14 TO_UNIX_TIMESTAMP Converts a string to a UNIX timestamp. Syntax \u200b\u200bTO_UNIX_TIMESTAMP(string)\u200b\u200b Parameter Description string The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Usage example SELECT TO_UNIX_TIMESTAMP('2020-05-31') AS res; MD5 Calculates the MD5 hash of string, returning the result as a string in hexadecimal. Syntax \u200b\u200bMD5(string)\u200b\u200b Parameter Description string The string to hash. For NULL, the function returns 0. Usage examples SELECT MD5('text') AS res; Returns: 1cb251ec0d568de6a929b520c4aed8d1 MD5_NUMBER_UPPER64 Represent the upper 64 bits of the MD5 hash value of the input string as BIGINT. Syntax \u200b\u200bMD5_NUMBER_UPPER64('string')\u200b\u200b Parameter Description string The string to calculate the MD5 on and represent as BIGINT Usage examples SELECT MD5_NUMBER_UPPER64('test') AS res; Returns: 688887797400064883 MD5_NUMBER_LOWER64 Represent the lower 64 bits of the MD5 hash value of the input string as BIGINT. Syntax \u200b\u200bMD5_NUMBER_LOWER64('string')\u200b\u200b Parameter Description string The string to calculate the MD5 on and represent as BIGINT Usage examples SELECT MD5_NUMBER_LOWER64('test') AS res; Returns: 14618207765679027446 MATCH Checks whether the string matches the regular expression pattern. A re2 regular expression. \u200b Returns 0 if it doesn\u2019t match, or 1 if it matches. Syntax \u200b\u200bMATCH(haystack, pattern)\u200b\u200b Parameter Description haystack The string in which to search for a match. pattern The pattern with which to search for a match in the string. Usage examples SELECT MATCH('123','\\\\[a-Z|A-Z]') AS res; Returns: 0 SELECT MATCH('123','\\\\d+'); Returns: 1 MATCH_ANY The same as MATCH , but returns 0 if none of the regular expressions are matched and 1 if any of the patterns matches. Syntax \u200b\u200bMATCH_ANY(haystack, [pattern1, pattern2, \u2026, pattern])\u200b\u200b Parameter Description haystack The string in which to search for a match. pattern The pattern with which to search for a match in the string. Usage examples SELECT MATCH_ANY('123',['\\\\d+','\\\\[a-Z|A-Z]']) AS res; The above query searches for any matches within the string \u200b 123 \u200b\u200bto the pattern \u200b['\\d+','\\[a-Z|A-Z]'] \u200b. \u200b Since at least one is found, returns: 1 SPLIT_PART Divides a string into substrings separated by a constant string delimiter of multiple characters as the separator. \u200b The string in the specified index is returned. If the string separator is empty, the string is divided into an array of single characters. Syntax \u200b\u200bSPLIT_PART(string, delimiter, index)\u200b\u200b Parameter Description string The string to be split. delimiter Any string within a string index Indicates the index from where to start searching for matches. Usage examples SELECT SPLIT_PART('hello#world','#',1) AS res; Returns: \u2018hello\u2019 TO_UNIXTIME For \u200b DATETIME \u200b\u200barguments: this function converts the value to its internal numeric representation (Unix Timestamp). \u200b For \u200b TEXT \u200b\u200barguments: this function parses \u200b DATETIME \u200b\u200bfrom a string and returns the corresponding Unix timestamp. Syntax \u200b\u200bTO_UNIXTIME(string)\u200b\u200b Parameter Description string The string to be converted. Usage examples SELECT TO_UNIXTIME('2017-11-05 08:07:47'') AS TO_UNIXTIME; Returns: 1509836867 LENGTH Calculates the String length. Syntax \u200b\u200bLENGTH(string)\u200b\u200b Parameter Description string The string for which to return the length. Usage examples SELECT LENGTH('abcd') AS length; Returns: 4 UPPER Converts the string to upper case format. Syntax \u200b\u200bUPPER(string)\u200b\u200b Parameter Description string The string to be converted. LOWER Converts the string to a lower case format. Syntax \u200b\u200bLOWER(string)\u200b\u200b Parameter Description string The string to be converted. UUID_GENERATE_V4 This function can be used to generate a random unique numeric value for any object in the database. Syntax \u200b\u200bUUID_GENERATE_V4()\u200b\u200b REVERSE This function returns a string of the same size as the original string, with the elements in reverse order. Syntax REVERSE(string) Parameter Description string The string to be reversed. Usage Example SELECT REVERSE('abcd') AS res Returns: 'dcba' SPLIT This function splits a given string by a given separator and returns the result in an array of strings. Syntax SPLIT('seperator',string) Parameter Description seperator The separator to split the string by. string The string to split. Usage Example SELECT SPLIT('|','this|is|my|test') AS res Returns: [\"this\",\"is\",\"my\",\"test\"] REPEAT This function repeats the provided string a requested number of times. Syntax REPEAT('string',repeating_number) Parameter Description string The string to be repeated. repeating number The number of needed repetitions. Usage Example SELECT REPEAT(\u2018repeat 3 times \u2019,3); Returns: 'repeat 3 times repeat 3 times repeat 3 times '","title":"String functions"},{"location":"sql-functions-reference/string-functions/#string-functions","text":"This page describes the string functions supported in Firebolt.","title":"String functions"},{"location":"sql-functions-reference/string-functions/#substr","text":"Returns a substring starting with the byte from the \u2018offset\u2019 index and \u2018length\u2019 bytes long. Character indexing starts from index 1 (as in standard SQL). The \u2018offset\u2019 and \u2018length\u2019 arguments must be constants. Syntax SUBSTR(s, offset, length) Parameter Description s The string to be offset. offset Count left to right the number of characters to offset from and not include in the results. 1 indicates starting with the first character. length The total amount of characters included in the returned result. Usage example SELECT SUBSTR('hello world', 1, 5); The string is offset by 1 and so begins with \"h\". 5 indicates the resulting string should be only five characters long. Returns: hello","title":"SUBSTR"},{"location":"sql-functions-reference/string-functions/#concat","text":"Concatenates the strings listed in the arguments, without a separator. Syntax CONCAT(s1,s2,\u2026); Or S1 || S2 || S3 Parameter Description s The strings, to be concatenated. Usage example SELECT concat('Hello, ', 'World!') Returns: Hello World!","title":"CONCAT"},{"location":"sql-functions-reference/string-functions/#replace","text":"Replaces all occurrences of the \u2018pattern\u2019 substring of the \u2018haystack\u2019 string with the \u2018replacement\u2019 substring. Syntax REPLACE (haystack, pattern, replacement)\u200b Parameter Description haystack The entire original string pattern The substring to be searched and replaced in the string. replacement The substring to replace the original substring (the 'pattern'). Usage examples SELECT REPLACE('hello world','hello','nice') AS res; Returns:'nice world' SELECT REPLACE('hello world',' world','') AS res; Returns: 'hello' SELECT REPLACE('hello world','hi','something') AS res; Returns: 'hello world' String functions","title":"REPLACE"},{"location":"sql-functions-reference/string-functions/#regexp_like","text":"This function holds a regular expression, used to check whether a pattern matches the regular expression string. Returns 0 if it doesn\u2019t match, or 1 if it matches. \u200b This is a \u200b\u200bre2\u200b\u200b regular expression. Syntax \u200bREGEXP_LIKE(haystack, pattern)\u200b\u200b Parameter Description haystack The string in which to search for a match.pattern pattern The pattern with which to search for a match in the string. Usage examples \u200b\u200bSELECT REGEXP_LIKE('123','\\\\[a-z]') AS res;\u200b\u200b Returns: 0 SELECT REGEXP_LIKE('123','\\\\d+') AS res;\u200b Returns: 1","title":"REGEXP_LIKE"},{"location":"sql-functions-reference/string-functions/#strpos","text":"Returns the position (in bytes) of the substring found in the string, starting from 1. Syntax \u200b\u200bSTRPOS(haystack, needle)\u200b\u200b Parameter Description haystack The string in which to search for the needle. needle The substring to be searched for. Usage examples SELECT STRPOS('hello world','hello') AS res Returns: 1 SELECT STRPOS('hello world','world') AS res Returns: 7","title":"STRPOS"},{"location":"sql-functions-reference/string-functions/#trim","text":"Removes all specified characters from the start or end of a string. By default removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. Syntax \u200b\u200bTRIM([LEADING|TRAILING|default = BOTH] <trim_character> FROM <target_string>)\u200b\u200b Parameter Description LEADING Removes all special characters from the beginning of the specified string. TRAILING Removes all special characters from the end of the specified string. BOTH Removes all special characters from the beginning and the end of the specified string. trim_character The characters to be removed. target_string The string to be trimmed. Usage example SELECT TRIM('$' FROM '$Hello world$') AS res; Returns: \u2018 Hello world\u2019","title":"TRIM"},{"location":"sql-functions-reference/string-functions/#ltrim","text":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bLTRIM(<target>)\u200b\u200b Parameter Description target The string to be trimmed. Usage example SELECT LTRIM(' Hello, world! ') Returns: Hello, world!","title":"LTRIM"},{"location":"sql-functions-reference/string-functions/#rtrim","text":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the end of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bRTRIM(<target>)\u200b\u200b Parameter Description target The string to be trimmed. Usage example SELECT RTRIM('Hello, world! ') Returns: Hello, world!","title":"RTRIM"},{"location":"sql-functions-reference/string-functions/#btrim","text":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning and the end of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc). Syntax \u200b\u200bBTRIM(<target>)\u200b\u200b Parameter Description target The string to be trimmed. Usage example SELECT BTRIM('Hello, world! ') Returns: Hello, world!","title":"BTRIM"},{"location":"sql-functions-reference/string-functions/#lpad","text":"Adds the pad in front of the string repetitively up until the length of the resulting string is equivalent to the indicated length. Syntax \u200b\u200bLPAD(<str>, <length> [, <pad>])\u200b\u200b Parameter Description str The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b str can be a literal string or the name of a column. length The length of the string as an integer after it has been left-padded. \u200b A negative number returns an empty string. pad The string to left-pad to primary string (str). Usage example The following statement adds the string ABC in front of the string Firebolt repetitively until the resulting string is equivalent to 20 characters in length. SELECT LPAD(\"Firebolt\", 20, \"ABC\"); Returns: ABCABCABCABCFirebolt","title":"LPAD"},{"location":"sql-functions-reference/string-functions/#rpad","text":"Adds the pad at the end of the string repetitively up until the length of the resulting string is equivalent to the indicated length. Syntax \u200b\u200bRPAD(<str>, <length> [, <pad>])\u200b\u200b Parameter Description str The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b str can be a literal string or the name of a column. length The length of the string as an integer after it has been left-padded. \u200b A negative number returns an empty string. pad The string to right-pad to primary string (str). Usage example The following statement adds the string ABC to the end of the string Firebolt repetitively until the resulting string is equivalent to 20 characters in length. SELECT RPAD(\"Firebolt\", 20, \"ABC\"); Returns: FireboltABCABCABCABC","title":"RPAD"},{"location":"sql-functions-reference/string-functions/#to_date","text":"Converts a string to date. Syntax \u200b\u200bTO_DATE(string)\u200b\u200b Parameter Description string The string format should be: \u2018YYYY-MM-DD\u2019 Usage example SELECT TO_DATE('2020-05-31') AS res; Returns: 2020-05-31","title":"TO_DATE"},{"location":"sql-functions-reference/string-functions/#to_timestamp","text":"Converts a string to timestamp. Syntax \u200b\u200bTO_TIMESTAMP(string)\u200b\u200b Parameter Description string The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Usage example SELECT TO_TIMESTAMP('2020-05-31 10:31:14') AS res; Returns: 2020-05-31 10:31:14","title":"TO_TIMESTAMP"},{"location":"sql-functions-reference/string-functions/#to_unix_timestamp","text":"Converts a string to a UNIX timestamp. Syntax \u200b\u200bTO_UNIX_TIMESTAMP(string)\u200b\u200b Parameter Description string The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Usage example SELECT TO_UNIX_TIMESTAMP('2020-05-31') AS res;","title":"TO_UNIX_TIMESTAMP"},{"location":"sql-functions-reference/string-functions/#md5","text":"Calculates the MD5 hash of string, returning the result as a string in hexadecimal. Syntax \u200b\u200bMD5(string)\u200b\u200b Parameter Description string The string to hash. For NULL, the function returns 0. Usage examples SELECT MD5('text') AS res; Returns: 1cb251ec0d568de6a929b520c4aed8d1","title":"MD5"},{"location":"sql-functions-reference/string-functions/#md5_number_upper64","text":"Represent the upper 64 bits of the MD5 hash value of the input string as BIGINT. Syntax \u200b\u200bMD5_NUMBER_UPPER64('string')\u200b\u200b Parameter Description string The string to calculate the MD5 on and represent as BIGINT Usage examples SELECT MD5_NUMBER_UPPER64('test') AS res; Returns: 688887797400064883","title":"MD5_NUMBER_UPPER64"},{"location":"sql-functions-reference/string-functions/#md5_number_lower64","text":"Represent the lower 64 bits of the MD5 hash value of the input string as BIGINT. Syntax \u200b\u200bMD5_NUMBER_LOWER64('string')\u200b\u200b Parameter Description string The string to calculate the MD5 on and represent as BIGINT Usage examples SELECT MD5_NUMBER_LOWER64('test') AS res; Returns: 14618207765679027446","title":"MD5_NUMBER_LOWER64"},{"location":"sql-functions-reference/string-functions/#match","text":"Checks whether the string matches the regular expression pattern. A re2 regular expression. \u200b Returns 0 if it doesn\u2019t match, or 1 if it matches. Syntax \u200b\u200bMATCH(haystack, pattern)\u200b\u200b Parameter Description haystack The string in which to search for a match. pattern The pattern with which to search for a match in the string. Usage examples SELECT MATCH('123','\\\\[a-Z|A-Z]') AS res; Returns: 0 SELECT MATCH('123','\\\\d+'); Returns: 1","title":"MATCH"},{"location":"sql-functions-reference/string-functions/#match_any","text":"The same as MATCH , but returns 0 if none of the regular expressions are matched and 1 if any of the patterns matches. Syntax \u200b\u200bMATCH_ANY(haystack, [pattern1, pattern2, \u2026, pattern])\u200b\u200b Parameter Description haystack The string in which to search for a match. pattern The pattern with which to search for a match in the string. Usage examples SELECT MATCH_ANY('123',['\\\\d+','\\\\[a-Z|A-Z]']) AS res; The above query searches for any matches within the string \u200b 123 \u200b\u200bto the pattern \u200b['\\d+','\\[a-Z|A-Z]'] \u200b. \u200b Since at least one is found, returns: 1","title":"MATCH_ANY"},{"location":"sql-functions-reference/string-functions/#split_part","text":"Divides a string into substrings separated by a constant string delimiter of multiple characters as the separator. \u200b The string in the specified index is returned. If the string separator is empty, the string is divided into an array of single characters. Syntax \u200b\u200bSPLIT_PART(string, delimiter, index)\u200b\u200b Parameter Description string The string to be split. delimiter Any string within a string index Indicates the index from where to start searching for matches. Usage examples SELECT SPLIT_PART('hello#world','#',1) AS res; Returns: \u2018hello\u2019","title":"SPLIT_PART"},{"location":"sql-functions-reference/string-functions/#to_unixtime","text":"For \u200b DATETIME \u200b\u200barguments: this function converts the value to its internal numeric representation (Unix Timestamp). \u200b For \u200b TEXT \u200b\u200barguments: this function parses \u200b DATETIME \u200b\u200bfrom a string and returns the corresponding Unix timestamp. Syntax \u200b\u200bTO_UNIXTIME(string)\u200b\u200b Parameter Description string The string to be converted. Usage examples SELECT TO_UNIXTIME('2017-11-05 08:07:47'') AS TO_UNIXTIME; Returns: 1509836867","title":"TO_UNIXTIME"},{"location":"sql-functions-reference/string-functions/#length","text":"Calculates the String length. Syntax \u200b\u200bLENGTH(string)\u200b\u200b Parameter Description string The string for which to return the length. Usage examples SELECT LENGTH('abcd') AS length; Returns: 4","title":"LENGTH"},{"location":"sql-functions-reference/string-functions/#upper","text":"Converts the string to upper case format. Syntax \u200b\u200bUPPER(string)\u200b\u200b Parameter Description string The string to be converted.","title":"UPPER"},{"location":"sql-functions-reference/string-functions/#lower","text":"Converts the string to a lower case format. Syntax \u200b\u200bLOWER(string)\u200b\u200b Parameter Description string The string to be converted.","title":"LOWER"},{"location":"sql-functions-reference/string-functions/#uuid_generate_v4","text":"This function can be used to generate a random unique numeric value for any object in the database. Syntax \u200b\u200bUUID_GENERATE_V4()\u200b\u200b","title":"UUID_GENERATE_V4"},{"location":"sql-functions-reference/string-functions/#reverse","text":"This function returns a string of the same size as the original string, with the elements in reverse order. Syntax REVERSE(string) Parameter Description string The string to be reversed. Usage Example SELECT REVERSE('abcd') AS res Returns: 'dcba'","title":"REVERSE"},{"location":"sql-functions-reference/string-functions/#split","text":"This function splits a given string by a given separator and returns the result in an array of strings. Syntax SPLIT('seperator',string) Parameter Description seperator The separator to split the string by. string The string to split. Usage Example SELECT SPLIT('|','this|is|my|test') AS res Returns: [\"this\",\"is\",\"my\",\"test\"]","title":"SPLIT"},{"location":"sql-functions-reference/string-functions/#repeat","text":"This function repeats the provided string a requested number of times. Syntax REPEAT('string',repeating_number) Parameter Description string The string to be repeated. repeating number The number of needed repetitions. Usage Example SELECT REPEAT(\u2018repeat 3 times \u2019,3); Returns: 'repeat 3 times repeat 3 times repeat 3 times '","title":"REPEAT"},{"location":"sql-functions-reference/untitled-1/","text":"Numeric functions This page describes the numeric functions supported in Firebolt. ACOS Calculates the arc cosine. Syntax ACOS(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. ASIN Calculates the arc sinus. Syntax ASIN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. COS Calculates the cosine. Syntax COS(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. SIN Calculates the sinus. Syntax SIN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. TAN Calculates the tangent. Syntax TAN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. ATAN Calculates the arctangent. Syntax ATAN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. MOD Calculates the remainder after division. Syntax MOD(a,b) Parameter Description a Valid values include column names, functions that return a column with numeric values, and constant numeric values. b Valid values include column names, functions that return a column with numeric values, and constant numeric values. RADIANS Converts degrees to radians PI() Returns the \u03c0 (Float). Syntax \u200b\u200bRADIANS(x) \u200b\u200b Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. PI Calculates the \u03c0 (Float). Syntax \u200b\u200bPI() \u200b\u200b EXP Accepts a numeric argument and returns its exponent (FLOAT). Syntax EXP(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. LOG Returns the natural logarithm of a numeric expression based on the desired base. Syntax LOG([base,] numeric); Parameter Description base The base for the log (optional). The default base is 10. numeric Valid values include column names, functions that return a column with numeric values, and constant numeric values. Usage example SELECT LOG(2, 64.0) This que r y returns the log of 64.0 with base 2 (6). SELECT LOG(64.0) This que r y returns the log of 64.0 with the default, base 10 (1.806 ). SQRT Returns the square root of a non-negative numeric expression. Syntax SQRT(x); Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. CBRT Returns the cubic-root of a non-negative numeric expression. Syntax CBRT(x); Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. POW, POWER Returns a number (x) raised to the specified power (y). Syntax POW(x,y)\u200b; POWER(x,y)\u200b; Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. y Valid values include column names, functions that return a column with numeric values, and constant numeric values. FLOOR Returns the largest round number that is less than or equal to x. A round number is a multiple of 1/10N, or the nearest number of the appropriate data type if 1 / 10N isn\u2019t exact. Syntax FLOOR(x[, N]) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative. CEIL, CEILING Returns the smallest round number that is greater than or equal to x. Syntax CEIL(x[, N]); CEILING(x[, N]); Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative. TRUNC Returns the round number with the largest absolute value that has an absolute value less than or equal to x\u2018s. Syntax TRUNC(x[, N]) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative. ROUND Rounds a value to a specified number of decimal places. Syntax ROUND(x[, N]) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative. ABS Rounds a value to a specified number of decimal places. Syntax ABS(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"Numeric functions"},{"location":"sql-functions-reference/untitled-1/#numeric-functions","text":"This page describes the numeric functions supported in Firebolt.","title":"Numeric functions"},{"location":"sql-functions-reference/untitled-1/#acos","text":"Calculates the arc cosine. Syntax ACOS(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"ACOS"},{"location":"sql-functions-reference/untitled-1/#asin","text":"Calculates the arc sinus. Syntax ASIN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"ASIN"},{"location":"sql-functions-reference/untitled-1/#cos","text":"Calculates the cosine. Syntax COS(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"COS"},{"location":"sql-functions-reference/untitled-1/#sin","text":"Calculates the sinus. Syntax SIN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"SIN"},{"location":"sql-functions-reference/untitled-1/#tan","text":"Calculates the tangent. Syntax TAN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"TAN"},{"location":"sql-functions-reference/untitled-1/#atan","text":"Calculates the arctangent. Syntax ATAN(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"ATAN"},{"location":"sql-functions-reference/untitled-1/#mod","text":"Calculates the remainder after division. Syntax MOD(a,b) Parameter Description a Valid values include column names, functions that return a column with numeric values, and constant numeric values. b Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"MOD"},{"location":"sql-functions-reference/untitled-1/#radians","text":"Converts degrees to radians PI() Returns the \u03c0 (Float). Syntax \u200b\u200bRADIANS(x) \u200b\u200b Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"RADIANS"},{"location":"sql-functions-reference/untitled-1/#pi","text":"Calculates the \u03c0 (Float). Syntax \u200b\u200bPI() \u200b\u200b","title":"PI"},{"location":"sql-functions-reference/untitled-1/#exp","text":"Accepts a numeric argument and returns its exponent (FLOAT). Syntax EXP(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"EXP"},{"location":"sql-functions-reference/untitled-1/#log","text":"Returns the natural logarithm of a numeric expression based on the desired base. Syntax LOG([base,] numeric); Parameter Description base The base for the log (optional). The default base is 10. numeric Valid values include column names, functions that return a column with numeric values, and constant numeric values. Usage example SELECT LOG(2, 64.0) This que r y returns the log of 64.0 with base 2 (6). SELECT LOG(64.0) This que r y returns the log of 64.0 with the default, base 10 (1.806 ).","title":"LOG"},{"location":"sql-functions-reference/untitled-1/#sqrt","text":"Returns the square root of a non-negative numeric expression. Syntax SQRT(x); Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"SQRT"},{"location":"sql-functions-reference/untitled-1/#cbrt","text":"Returns the cubic-root of a non-negative numeric expression. Syntax CBRT(x); Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"CBRT"},{"location":"sql-functions-reference/untitled-1/#pow-power","text":"Returns a number (x) raised to the specified power (y). Syntax POW(x,y)\u200b; POWER(x,y)\u200b; Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. y Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"POW, POWER"},{"location":"sql-functions-reference/untitled-1/#floor","text":"Returns the largest round number that is less than or equal to x. A round number is a multiple of 1/10N, or the nearest number of the appropriate data type if 1 / 10N isn\u2019t exact. Syntax FLOOR(x[, N]) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative.","title":"FLOOR"},{"location":"sql-functions-reference/untitled-1/#ceil-ceiling","text":"Returns the smallest round number that is greater than or equal to x. Syntax CEIL(x[, N]); CEILING(x[, N]); Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative.","title":"CEIL, CEILING"},{"location":"sql-functions-reference/untitled-1/#trunc","text":"Returns the round number with the largest absolute value that has an absolute value less than or equal to x\u2018s. Syntax TRUNC(x[, N]) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative.","title":"TRUNC"},{"location":"sql-functions-reference/untitled-1/#round","text":"Rounds a value to a specified number of decimal places. Syntax ROUND(x[, N]) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values. N An INTEGER constant (optional parameter). By default, it is zero, which means to round to an integer. May also be negative.","title":"ROUND"},{"location":"sql-functions-reference/untitled-1/#abs","text":"Rounds a value to a specified number of decimal places. Syntax ABS(x) Parameter Description x Valid values include column names, functions that return a column with numeric values, and constant numeric values.","title":"ABS"},{"location":"sql-functions-reference/window-functions/","text":"Window Functions This page describes the window functions supported in Firebolt. A window function performs an aggregate-like operation on a set of query rows, whereas unlike aggregate operation, it doesn't require 'group by' operation and it produces a result for each query row. Syntax: FUNCTION_NAME() OVER ([PARTITION BY <expr1>] [ORDER BY <expr2> [ASC|DESC]]) Function name Description Comment RANK() Rank the current row within its partition with gaps Must include ORDER BY DENSE_RANK() Rank the current row within its partition without gaps Must include ORDER BY MIN() Calculate the minimum value within its partition Must not include ORDER BY MAX() Calculate the maximum value within its partition Must not include ORDER BY SUM() Calculate the sum of the values within its partition. The SUM function works with numeric values and ignores NULL values. Must not include ORDER BY COUNT() Count the number of values within its partition. Must not include ORDER BY Usage example Consider the following WF table: id purchase_date amount username 1 2020-10-18 45 Joy 2 2020-10-17 343 Adam 3 2020-10-16 65.87 Ruth 4 2020-10-10 65.90 Emma 5 2020-12-18 655 Adam SELECT DENSE_RANK() OVER (PARTITION BY username ORDER BY purchase_date ASC) from WF; SELECT RANK() OVER (PARTITION BY username ORDER BY purchase_date DESC) from WF; SELECT MIN(purchase_date) OVER (PARTITION BY username) from WF; SELECT MAX(amount) OVER (PARTITION BY username) from WF; SELECT SUM(amount) OVER (PARTITION BY username) from WF; SELECT COUNT(amount) OVER (PARTITION BY username) from WF;","title":"Window Functions"},{"location":"sql-functions-reference/window-functions/#window-functions","text":"This page describes the window functions supported in Firebolt. A window function performs an aggregate-like operation on a set of query rows, whereas unlike aggregate operation, it doesn't require 'group by' operation and it produces a result for each query row. Syntax: FUNCTION_NAME() OVER ([PARTITION BY <expr1>] [ORDER BY <expr2> [ASC|DESC]]) Function name Description Comment RANK() Rank the current row within its partition with gaps Must include ORDER BY DENSE_RANK() Rank the current row within its partition without gaps Must include ORDER BY MIN() Calculate the minimum value within its partition Must not include ORDER BY MAX() Calculate the maximum value within its partition Must not include ORDER BY SUM() Calculate the sum of the values within its partition. The SUM function works with numeric values and ignores NULL values. Must not include ORDER BY COUNT() Count the number of values within its partition. Must not include ORDER BY Usage example Consider the following WF table: id purchase_date amount username 1 2020-10-18 45 Joy 2 2020-10-17 343 Adam 3 2020-10-16 65.87 Ruth 4 2020-10-10 65.90 Emma 5 2020-12-18 655 Adam SELECT DENSE_RANK() OVER (PARTITION BY username ORDER BY purchase_date ASC) from WF; SELECT RANK() OVER (PARTITION BY username ORDER BY purchase_date DESC) from WF; SELECT MIN(purchase_date) OVER (PARTITION BY username) from WF; SELECT MAX(amount) OVER (PARTITION BY username) from WF; SELECT SUM(amount) OVER (PARTITION BY username) from WF; SELECT COUNT(amount) OVER (PARTITION BY username) from WF;","title":"Window Functions"},{"location":"sql-functions-reference/semi-structured-functions/","text":"Semi-structured data functions Firebolt supports loading and manipulating semi-structured data such as JSON. This, and similar formats (e.g. Avro, Parquet) can contain compound types such as arrays, maps, and structs. Firebolt uses its native ARRAY type to model and query such semi-structured data. The raw JSON input can be transformed into Firebolt's arrays during ingestion, or stored as a plain TEXT column. In both cases, JSON functions will be used to transform the nested, compound JSON types to Firebolt arrays, while array functions , including Aggregate Array Functions, will be used to manipulate and query the semi-structured data. Array functions - used for the manipulation and querying of array typed columns, such as transformation , filtering , and un-nesting - an operation that converts the array to a regular column. Aggregate array functions - these functions work on array-typed columns, but instead of being applied row by row, they combine the results or all the array belonging to the groups defined by the GROUP BY clause. JSON functions - these function extract and transform raw JSON into Firebolt native types, or JSON sub-objects. They are used either during the ELT process or applied to columns storing JSON objects as plain TEXT","title":"Semi-structured data functions"},{"location":"sql-functions-reference/semi-structured-functions/#semi-structured-data-functions","text":"Firebolt supports loading and manipulating semi-structured data such as JSON. This, and similar formats (e.g. Avro, Parquet) can contain compound types such as arrays, maps, and structs. Firebolt uses its native ARRAY type to model and query such semi-structured data. The raw JSON input can be transformed into Firebolt's arrays during ingestion, or stored as a plain TEXT column. In both cases, JSON functions will be used to transform the nested, compound JSON types to Firebolt arrays, while array functions , including Aggregate Array Functions, will be used to manipulate and query the semi-structured data. Array functions - used for the manipulation and querying of array typed columns, such as transformation , filtering , and un-nesting - an operation that converts the array to a regular column. Aggregate array functions - these functions work on array-typed columns, but instead of being applied row by row, they combine the results or all the array belonging to the groups defined by the GROUP BY clause. JSON functions - these function extract and transform raw JSON into Firebolt native types, or JSON sub-objects. They are used either during the ELT process or applied to columns storing JSON objects as plain TEXT","title":"Semi-structured data functions"},{"location":"sql-functions-reference/semi-structured-functions/aggregate-array-functions/","text":"Aggregate array functions Aggregate semi-structured functions work globally on all the arrays in a given column expression, instead of a row-by-row application. At their simplest form (without a GROUP BY clause) - they will provide the result of globally applying the function on all of the elements of the arrays in the column expression specified as their argument. Thus, fur example, ARRAY_SUM_GLOBAL will return the sum of all the elements in all the array of the given column expression argument, and ARRAY_MAX_GLOBAL will return the maximal element among all of the elements in all of the arrays in the given column expression. When combined with a GROUP BY clause, these operations will be performed on all of the arrays in each group. From the remainder of this page we will use the following table T in our examples: Category vals a [1,3,4] b [3,5,6,7] a [30,50,60] ARRAY_COUNT_GLOBAL Returns the number of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr)) Usage Example SELECT Category, ARRAY_COUNT_GLOBAL(vals) AS cnt FROM T GROUP BY Category; Returns: category cnt a 6 b 4 ARRAY_SUM_GLOBAL Returns the sum of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will sum the elements Usage Example SELECT Category, ARRAY_SUM_GLOBAL(vals) AS sm FROM T GROUP BY Category; Returns: category sm a 148 b 21 ARRAY_MAX_GLOBAL Returns the maximal element taken from all the array elements in each group. Syntax ARRAY_MAX_GLOBAL(arr) AS cnt Parameter Description arr The array column over from which the function returns the maximal element Usage Example SELECT Category, ARRAY_MAX_GLOBAL(vals) AS mx FROM T GROUP BY Category; Returns: category mx a 60 b 7 ARRAY_MIN_GLOBAL Returns the minimal element taken from all the array elements in each group. Syntax ARRAY_MIN_GLOBAL(arr) Parameter Description arr The array column from which the function will return the minimal element Usage Example SELECT Category, ARRAY_MIN_GLOBAL(vals) AS mn FROM T GROUP BY Category; Returns: category sm a 1 b 3","title":"Aggregate array functions"},{"location":"sql-functions-reference/semi-structured-functions/aggregate-array-functions/#aggregate-array-functions","text":"Aggregate semi-structured functions work globally on all the arrays in a given column expression, instead of a row-by-row application. At their simplest form (without a GROUP BY clause) - they will provide the result of globally applying the function on all of the elements of the arrays in the column expression specified as their argument. Thus, fur example, ARRAY_SUM_GLOBAL will return the sum of all the elements in all the array of the given column expression argument, and ARRAY_MAX_GLOBAL will return the maximal element among all of the elements in all of the arrays in the given column expression. When combined with a GROUP BY clause, these operations will be performed on all of the arrays in each group. From the remainder of this page we will use the following table T in our examples: Category vals a [1,3,4] b [3,5,6,7] a [30,50,60]","title":"Aggregate array functions"},{"location":"sql-functions-reference/semi-structured-functions/aggregate-array-functions/#array_count_global","text":"Returns the number of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr)) Usage Example SELECT Category, ARRAY_COUNT_GLOBAL(vals) AS cnt FROM T GROUP BY Category; Returns: category cnt a 6 b 4","title":"ARRAY_COUNT_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/aggregate-array-functions/#array_sum_global","text":"Returns the sum of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will sum the elements Usage Example SELECT Category, ARRAY_SUM_GLOBAL(vals) AS sm FROM T GROUP BY Category; Returns: category sm a 148 b 21","title":"ARRAY_SUM_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/aggregate-array-functions/#array_max_global","text":"Returns the maximal element taken from all the array elements in each group. Syntax ARRAY_MAX_GLOBAL(arr) AS cnt Parameter Description arr The array column over from which the function returns the maximal element Usage Example SELECT Category, ARRAY_MAX_GLOBAL(vals) AS mx FROM T GROUP BY Category; Returns: category mx a 60 b 7","title":"ARRAY_MAX_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/aggregate-array-functions/#array_min_global","text":"Returns the minimal element taken from all the array elements in each group. Syntax ARRAY_MIN_GLOBAL(arr) Parameter Description arr The array column from which the function will return the minimal element Usage Example SELECT Category, ARRAY_MIN_GLOBAL(vals) AS mn FROM T GROUP BY Category; Returns: category sm a 1 b 3","title":"ARRAY_MIN_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/","text":"Array Functions This page describes the functions for working with arrays. NEST Takes a column as an argument, and returns an array of the values. In case the type of the column is nullable, the NULL values will be ignored. Syntax \u200b\u200bNEST(col)\u200b\u200b Parameter Description col The name of the column to be referenced. Usage example Assume we have the following prices table: item price apple 4 banana NULL orange 11 kiwi 20 Running the following query: SELECT NEST(price) as arr from prices; Returns: [4,11,20] ALL_MATCH Returns 1 if all elements of an array match the results of the function provided in the func parameter, otherwise returns 0. Syntax ALL_MATCH(func, arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty. Usage example SELECT ALL_MATCH(x -> x > 0, [1,2,3,9]) AS res; Returns: 1 ANY_MATCH Returns 1 if at least one of the elements of an array matches the results of the function provided in the func parameter. Otherwise returns 0. Syntax \u200b\u200bANY_MATCH(func, arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty Usage example SELECT ANY_MATCH(x -> x > 3, [1,2,3,9]) AS res; Returns: 1 ARRAY_COUNT Returns the number of elements in the arr array for which the f unc returns something other than 0. Syntax ARRAY_COUNT(func, arr) Parameter Description func The function to be tested. arr The array to be used to check the function. Usage examples SELECT ARRAY_COUNT(x -> x > 3, [1,2,3,9]) AS res; Returns: 1 ARRAY_CUMULATIVE_SUM Returns an array of partial sums of elements from the source array (a running sum). If the argument func is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_CUMULATIVE_SUM([func,] arr) Parameter Description func The function used to convert the array members. arr The array to be used for the sum calculations. Usage examples SELECT ARRAY_CUMULATIVE_SUM(x -> x + 1, [1,2,3,9]) AS res; Returns: 2,5,9,19 SELECT ARRAY_CUMULATIVE_SUM([1,2,3,9]) AS res Returns: 1,3,6,15 ARRAY_FILL This function scans through the given array from the first element to the last element and replaces arr[i] by arr[i - 1] if the func returns 0. The first element of the given array is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_FILL(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples SELECT ARRAY_FILL(x -> x < 0,[1,2,3,9]) AS res; Returns: 1,1,1,1 SELECT ARRAY_FILL(x -> x > 0,[1,2,3,9]) AS res; Returns: 1,2,3,9 FILTER Returns an array containing the elements from arr for which the given func function returns something other than 0. The function can receive one or more arrays as its arguments. If more than one array is provided the following conditions should be met: The number of arguments of the lambda function should be equal to the number of arrays provided. If the condition isn't met - the query will not run and an error will be returned. All the provided array should be of the same length. If the condition isn't met a runtime error will occur. When multiple arrays are provided to the function, the function will receive the current element from each array as its actual parameter, and can use any or all of them to produce the result which will detemine if the element at this index will be included in the result of the function, however, the elements of the result are always taken from the first array provided. Syntax FILTER(func, arr[, arr1, arr2...] ) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT FILTER(x, y -> y > 2,['a','b','c','d'],[1,2,3,9]) AS res; Returns: ['c', 'd'] ARRAY_FIRST Returns the first element in the given array for which the given func function returns something other than 0. The first argument (lambda function) can\u2019t be omitted. Syntax ARRAY_FIRST(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST(x -> x > 2,[1,2,3,9]) AS res; Returns: 3 ARRAY_FIRST_INDEX Returns the index of the first element in the indicated array for which the given func function returns something other than 0. The first argument (lambda function) is mandatory. Syntax ARRAY_FIRST_INDEX(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST_INDEX(x -> x > 2,[1,2,3,9]) AS res; Returns: 3 ARRAY_UNNEST This function \"unfolds\" the given array by creating a column result containing the individual members from the array's values. Syntax ARRAY_UNNEST(arr) Parameter Description arr The array to be unfolded. Usage example SELECT ARRAY_UNNEST([1,2,3,4]) AS res; Returns: res 1 2 3 4 ARRAY_REPLACE_BACKWARDS Scans arr from the last element to the first element and replaces each of the elements in that array with arr[i + 1] if the func returns 0. The last element of arr is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_REPLACE_BACKWARDS(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_REPLACE_BACKWARDS(x -> x > 2,[1,2,3,9]) AS res; Returns: 3,3,3,9 TRANSFORM Returns an array resulting by applying func on each element of arr . The first argument (lambda function) is mandatory. Syntax TRANSFORM(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT TRANSFORM(x -> x * 2,[1,2,3,9]) AS res; Returns: 2,4,6,18 ARRAY_SORT Returns the elements of arr in ascending order. If the argument func is provided, the sorting order is determined by the result of applying func on each element of arr . Syntax ARRAY_SORT([func,] arr) Parameter Description func An optional function to be used to determine the sort order. arr The array to be sorted. Usage examples SELECT ARRAY_SORT([4,1,3,2]) AS res; Returns: 1,2,3,4 SELECT ARRAY_SORT(x -> x * 2,[4,1,3,2]) AS res; Returns: 1,2,3,4 ARRAY_SUM Returns the sum of elements of arr . If the argument func is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_SUM([func,] arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples A regular transformation function: SELECT ARRAY_SUM(x -> x + 1,[4,1,3,2]) AS res; Returns: 14 A function that returns 0/1: SELECT ARRAY_SUM(x -> x > 1,[4,1,3,2]) AS res; Returns: 3 No function: SELECT ARRAY_SUM([4,1,3,2]) AS res; Returns: 10 ARRAY_COUNT_GLOBAL Returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr)) ARRAY_CONCAT Combines arrays that are passed as arguments. Syntax ARRAY_CONCAT(arr1 [,arr\u2026]) Parameter Description arr The arrays to be combined. If only one array is given, the identical array is returned. Usage Example SELECT ARRAY_CONCAT([1,2,3,4],[5,6,7,8]) AS res; Returns:1,2,3,4,5,6,7,8 ARRAY_DISTINCT Returns an array containing only the unique elements of the given array. In other words, if the given array contains multiple identical members, the returned array will include only a single member of that value. Syntax ARRAY_DISTINCT(arr) Parameter Description arr The array to be analyzed for unique members. Usage Example SELECT ARRAY_DISTINCT([1,1,2,2,3,4]) AS res; Returns: 1,2,3,4 ELEMENT_AT Returns the element with the index n from the given array. n must be any integer type. Indexes in an array begin from one (1). Syntax ELEMENT_AT(arr, n) Parameter Description arr The array containing the index. n The index that is matched by the function. Negative indexes are supported. If used, the function selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. Usage Example SELECT ELEMENT_AT([1,2,3,4],2) AS res; Returns: 2 FLATTEN Converts an array of arrays into a flat array. That is, for every element that is an array, this function extracts its elements into the new array. The resulting flattened array contains all the elements from all source arrays. The function: Applies to any depth of nested arrays. Does not change arrays that are already flat. Syntax FLATTEN(arr_of_arrs) Parameter Description arr_of_arrs The array of arrays to be flattened. Usage Example SELECT flatten([[[1,2]], [[2,3], [3,4]]]) Returns: (1, 2, 2, 3, 3, 4) ARRAY_INTERSECT Returns an array containing the elements that are present in all the arrays provided as arguments. The order of the resulting array is the same as in the first array. Syntax ARRAY_INTERSECT(arr) Parameter Description arr Each of the arrays analyzed for mutual elements. Usage Example SELECT ARRAY_INTERSECT([1, 2, 3], [1, 3], [2, 3]) Returns: 3 ARRAY_JOIN Concatenates the elements of the given array of TEXT using the delimiter (optional). In case the delimiter is not provided, an empty string is being used. Syntax ARRAY_JOIN(arr[,delimiter]) Parameter Description arr An array of TEXT elements. delimiter The delimiter used for joining the array elements. If you omit this value, an empty string is being used as a delimiter. Usage Example SELECT ARRAY_JOIN(['1', '2', '3']) AS res; Returns: 123 REDUCE Applies an aggregate function on the elements of the array and returns its result. The name of the aggregation function is passed as a string in single quotes - for example: 'max', 'sum'. When using parametric aggregate functions, the parameter is indicated after the function name in parentheses 'func_name(param)'. Syntax REDUCE(agg_function, arr) Parameter Description agg_function The name of an aggregate function which should be a constant string arr Any number of array type columns as the parameters of the aggregation function. Usage Example SELECT REDUCE('max', [1, 2, 3, 6]) AS res; Returns: 6 ARRAY_REVERSE Returns an array of the same size as the original array, with the elements in reverse order. Syntax ARRAY_REVERSE(arr) Parameter Description arr The array to be reversed. Usage Example SELECT ARRAY_REVERSE([1, 2, 3, 6]) AS res Returns: 6,3,2,1 SLICE Returns a \"slice\" of the array based on the indicated offset and length. Syntax SLICE(arr, offset[, length]) Parameter Description arr The array of data to be sliced. Array elements set to NULL are handled as normal values. The numbering of the array items begins with 1. offset Indicates the indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. length The length of the required slice. If you specify a negative value, the function returns an open slice [offset, array_length - length). If you omit this value, the function returns the slice [offset, the_end_of_array]. Usage Example SELECT SLICE([1, 2, NULL, 4, 5], 2, 3) AS res Returns: 2,,4 ARRAY_UNIQ If one argument is passed, returns the number of different elements in the array. If multiple arguments are passed, returns the number of different tuples of elements at corresponding positions in multiple arrays. Syntax ARRAY_UNIQ(arr, ...) Parameter Description arr The array to be analyzed. Usage Example SELECT ARRAY_UNIQ([1, 2, 4, 5]) AS res; Returns: 4 INDEX_OF Returns the position of the first occurrence of the element in the array (or 0 if not found). Syntax INDEX_OF(arr, x) Parameter Description arr The array to be analyzed. x The element from the array that is to be matched. Usage Example SELECT INDEX_OF([1, 3, 5, 7], 5) AS res; Returns: 3 LENGTH Returns the length (number of elements) of the given array. Syntax LENGTH(arr) Parameter Description arr The array to be checked for length. Usage Example SELECT LENGTH([1, 2, 3, 4]) AS res; Returns: 4 CONTAINS Returns 1 if the second argument is present in the array, or 0 otherwise. Syntax CONTAINS(arr, x) Parameter Description arr The array to be checked for the given element. x The element to be searched for within the array. Usage Example SELECT CONTAINS([1, 2, 3], 3) AS res; Returns: 1 ARRAY_MAX Returns the maximum element in arr . Syntax ARRAY_MAX(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MAX([1,2,3,4]) AS res; Returns: 4 ARRAY_MIN Returns the minimum element in arr . Syntax ARRAY_MIN(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MIN([1,2,3,4]) AS res; Returns: 1","title":"Array Functions"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array-functions","text":"This page describes the functions for working with arrays.","title":"Array Functions"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#nest","text":"Takes a column as an argument, and returns an array of the values. In case the type of the column is nullable, the NULL values will be ignored. Syntax \u200b\u200bNEST(col)\u200b\u200b Parameter Description col The name of the column to be referenced. Usage example Assume we have the following prices table: item price apple 4 banana NULL orange 11 kiwi 20 Running the following query: SELECT NEST(price) as arr from prices; Returns: [4,11,20]","title":"NEST"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#all_match","text":"Returns 1 if all elements of an array match the results of the function provided in the func parameter, otherwise returns 0. Syntax ALL_MATCH(func, arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty. Usage example SELECT ALL_MATCH(x -> x > 0, [1,2,3,9]) AS res; Returns: 1","title":"ALL_MATCH"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#any_match","text":"Returns 1 if at least one of the elements of an array matches the results of the function provided in the func parameter. Otherwise returns 0. Syntax \u200b\u200bANY_MATCH(func, arr) Parameter Description func The function against which to check the array. arr The array of possible results for the function. The array cannot be empty Usage example SELECT ANY_MATCH(x -> x > 3, [1,2,3,9]) AS res; Returns: 1","title":"ANY_MATCH"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_count","text":"Returns the number of elements in the arr array for which the f unc returns something other than 0. Syntax ARRAY_COUNT(func, arr) Parameter Description func The function to be tested. arr The array to be used to check the function. Usage examples SELECT ARRAY_COUNT(x -> x > 3, [1,2,3,9]) AS res; Returns: 1","title":"ARRAY_COUNT"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_cumulative_sum","text":"Returns an array of partial sums of elements from the source array (a running sum). If the argument func is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_CUMULATIVE_SUM([func,] arr) Parameter Description func The function used to convert the array members. arr The array to be used for the sum calculations. Usage examples SELECT ARRAY_CUMULATIVE_SUM(x -> x + 1, [1,2,3,9]) AS res; Returns: 2,5,9,19 SELECT ARRAY_CUMULATIVE_SUM([1,2,3,9]) AS res Returns: 1,3,6,15","title":"ARRAY_CUMULATIVE_SUM"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_fill","text":"This function scans through the given array from the first element to the last element and replaces arr[i] by arr[i - 1] if the func returns 0. The first element of the given array is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_FILL(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples SELECT ARRAY_FILL(x -> x < 0,[1,2,3,9]) AS res; Returns: 1,1,1,1 SELECT ARRAY_FILL(x -> x > 0,[1,2,3,9]) AS res; Returns: 1,2,3,9","title":"ARRAY_FILL"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#filter","text":"Returns an array containing the elements from arr for which the given func function returns something other than 0. The function can receive one or more arrays as its arguments. If more than one array is provided the following conditions should be met: The number of arguments of the lambda function should be equal to the number of arrays provided. If the condition isn't met - the query will not run and an error will be returned. All the provided array should be of the same length. If the condition isn't met a runtime error will occur. When multiple arrays are provided to the function, the function will receive the current element from each array as its actual parameter, and can use any or all of them to produce the result which will detemine if the element at this index will be included in the result of the function, however, the elements of the result are always taken from the first array provided. Syntax FILTER(func, arr[, arr1, arr2...] ) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT FILTER(x, y -> y > 2,['a','b','c','d'],[1,2,3,9]) AS res; Returns: ['c', 'd']","title":"FILTER"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_first","text":"Returns the first element in the given array for which the given func function returns something other than 0. The first argument (lambda function) can\u2019t be omitted. Syntax ARRAY_FIRST(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST(x -> x > 2,[1,2,3,9]) AS res; Returns: 3","title":"ARRAY_FIRST"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_first_index","text":"Returns the index of the first element in the indicated array for which the given func function returns something other than 0. The first argument (lambda function) is mandatory. Syntax ARRAY_FIRST_INDEX(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_FIRST_INDEX(x -> x > 2,[1,2,3,9]) AS res; Returns: 3","title":"ARRAY_FIRST_INDEX"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_unnest","text":"This function \"unfolds\" the given array by creating a column result containing the individual members from the array's values. Syntax ARRAY_UNNEST(arr) Parameter Description arr The array to be unfolded. Usage example SELECT ARRAY_UNNEST([1,2,3,4]) AS res; Returns: res 1 2 3 4","title":"ARRAY_UNNEST"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_replace_backwards","text":"Scans arr from the last element to the first element and replaces each of the elements in that array with arr[i + 1] if the func returns 0. The last element of arr is not replaced. The first argument (lambda function) is mandatory. Syntax ARRAY_REPLACE_BACKWARDS(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT ARRAY_REPLACE_BACKWARDS(x -> x > 2,[1,2,3,9]) AS res; Returns: 3,3,3,9","title":"ARRAY_REPLACE_BACKWARDS"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#transform","text":"Returns an array resulting by applying func on each element of arr . The first argument (lambda function) is mandatory. Syntax TRANSFORM(func, arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage example SELECT TRANSFORM(x -> x * 2,[1,2,3,9]) AS res; Returns: 2,4,6,18","title":"TRANSFORM"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_sort","text":"Returns the elements of arr in ascending order. If the argument func is provided, the sorting order is determined by the result of applying func on each element of arr . Syntax ARRAY_SORT([func,] arr) Parameter Description func An optional function to be used to determine the sort order. arr The array to be sorted. Usage examples SELECT ARRAY_SORT([4,1,3,2]) AS res; Returns: 1,2,3,4 SELECT ARRAY_SORT(x -> x * 2,[4,1,3,2]) AS res; Returns: 1,2,3,4","title":"ARRAY_SORT"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_sum","text":"Returns the sum of elements of arr . If the argument func is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_SUM([func,] arr) Parameter Description func The function to be used to calculate the array members. arr The array to be used to calculate the function. Usage examples A regular transformation function: SELECT ARRAY_SUM(x -> x + 1,[4,1,3,2]) AS res; Returns: 14 A function that returns 0/1: SELECT ARRAY_SUM(x -> x > 1,[4,1,3,2]) AS res; Returns: 3 No function: SELECT ARRAY_SUM([4,1,3,2]) AS res; Returns: 10","title":"ARRAY_SUM"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_count_global","text":"Returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr))","title":"ARRAY_COUNT_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_concat","text":"Combines arrays that are passed as arguments. Syntax ARRAY_CONCAT(arr1 [,arr\u2026]) Parameter Description arr The arrays to be combined. If only one array is given, the identical array is returned. Usage Example SELECT ARRAY_CONCAT([1,2,3,4],[5,6,7,8]) AS res; Returns:1,2,3,4,5,6,7,8","title":"ARRAY_CONCAT"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_distinct","text":"Returns an array containing only the unique elements of the given array. In other words, if the given array contains multiple identical members, the returned array will include only a single member of that value. Syntax ARRAY_DISTINCT(arr) Parameter Description arr The array to be analyzed for unique members. Usage Example SELECT ARRAY_DISTINCT([1,1,2,2,3,4]) AS res; Returns: 1,2,3,4","title":"ARRAY_DISTINCT"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#element_at","text":"Returns the element with the index n from the given array. n must be any integer type. Indexes in an array begin from one (1). Syntax ELEMENT_AT(arr, n) Parameter Description arr The array containing the index. n The index that is matched by the function. Negative indexes are supported. If used, the function selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. Usage Example SELECT ELEMENT_AT([1,2,3,4],2) AS res; Returns: 2","title":"ELEMENT_AT"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#flatten","text":"Converts an array of arrays into a flat array. That is, for every element that is an array, this function extracts its elements into the new array. The resulting flattened array contains all the elements from all source arrays. The function: Applies to any depth of nested arrays. Does not change arrays that are already flat. Syntax FLATTEN(arr_of_arrs) Parameter Description arr_of_arrs The array of arrays to be flattened. Usage Example SELECT flatten([[[1,2]], [[2,3], [3,4]]]) Returns: (1, 2, 2, 3, 3, 4)","title":"FLATTEN"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_intersect","text":"Returns an array containing the elements that are present in all the arrays provided as arguments. The order of the resulting array is the same as in the first array. Syntax ARRAY_INTERSECT(arr) Parameter Description arr Each of the arrays analyzed for mutual elements. Usage Example SELECT ARRAY_INTERSECT([1, 2, 3], [1, 3], [2, 3]) Returns: 3","title":"ARRAY_INTERSECT"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_join","text":"Concatenates the elements of the given array of TEXT using the delimiter (optional). In case the delimiter is not provided, an empty string is being used. Syntax ARRAY_JOIN(arr[,delimiter]) Parameter Description arr An array of TEXT elements. delimiter The delimiter used for joining the array elements. If you omit this value, an empty string is being used as a delimiter. Usage Example SELECT ARRAY_JOIN(['1', '2', '3']) AS res; Returns: 123","title":"ARRAY_JOIN"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#reduce","text":"Applies an aggregate function on the elements of the array and returns its result. The name of the aggregation function is passed as a string in single quotes - for example: 'max', 'sum'. When using parametric aggregate functions, the parameter is indicated after the function name in parentheses 'func_name(param)'. Syntax REDUCE(agg_function, arr) Parameter Description agg_function The name of an aggregate function which should be a constant string arr Any number of array type columns as the parameters of the aggregation function. Usage Example SELECT REDUCE('max', [1, 2, 3, 6]) AS res; Returns: 6","title":"REDUCE"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_reverse","text":"Returns an array of the same size as the original array, with the elements in reverse order. Syntax ARRAY_REVERSE(arr) Parameter Description arr The array to be reversed. Usage Example SELECT ARRAY_REVERSE([1, 2, 3, 6]) AS res Returns: 6,3,2,1","title":"ARRAY_REVERSE"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#slice","text":"Returns a \"slice\" of the array based on the indicated offset and length. Syntax SLICE(arr, offset[, length]) Parameter Description arr The array of data to be sliced. Array elements set to NULL are handled as normal values. The numbering of the array items begins with 1. offset Indicates the indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. length The length of the required slice. If you specify a negative value, the function returns an open slice [offset, array_length - length). If you omit this value, the function returns the slice [offset, the_end_of_array]. Usage Example SELECT SLICE([1, 2, NULL, 4, 5], 2, 3) AS res Returns: 2,,4","title":"SLICE"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_uniq","text":"If one argument is passed, returns the number of different elements in the array. If multiple arguments are passed, returns the number of different tuples of elements at corresponding positions in multiple arrays. Syntax ARRAY_UNIQ(arr, ...) Parameter Description arr The array to be analyzed. Usage Example SELECT ARRAY_UNIQ([1, 2, 4, 5]) AS res; Returns: 4","title":"ARRAY_UNIQ"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#index_of","text":"Returns the position of the first occurrence of the element in the array (or 0 if not found). Syntax INDEX_OF(arr, x) Parameter Description arr The array to be analyzed. x The element from the array that is to be matched. Usage Example SELECT INDEX_OF([1, 3, 5, 7], 5) AS res; Returns: 3","title":"INDEX_OF"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#length","text":"Returns the length (number of elements) of the given array. Syntax LENGTH(arr) Parameter Description arr The array to be checked for length. Usage Example SELECT LENGTH([1, 2, 3, 4]) AS res; Returns: 4","title":"LENGTH"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#contains","text":"Returns 1 if the second argument is present in the array, or 0 otherwise. Syntax CONTAINS(arr, x) Parameter Description arr The array to be checked for the given element. x The element to be searched for within the array. Usage Example SELECT CONTAINS([1, 2, 3], 3) AS res; Returns: 1","title":"CONTAINS"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_max","text":"Returns the maximum element in arr . Syntax ARRAY_MAX(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MAX([1,2,3,4]) AS res; Returns: 4","title":"ARRAY_MAX"},{"location":"sql-functions-reference/semi-structured-functions/array-functions/#array_min","text":"Returns the minimum element in arr . Syntax ARRAY_MIN(arr) Parameter Description arr The array to be checked or an array typed column Usage Example SELECT ARRAY_MIN([1,2,3,4]) AS res; Returns: 1","title":"ARRAY_MIN"},{"location":"sql-functions-reference/semi-structured-functions/semi-structured-aggregate-functions/","text":"Semi-structured aggregate functions Aggregate semi-structured functions work globally on all the arrays in a given column expression, instead of a row-by-row application. At their simplest form (without a GROUP BY clause) - they will provide the result of globally applying the function on all of the elements of the arrays in the column expression specified as their argument. Thus, fur example, ARRAY_SUM_GLOBAL will return the sum of all the elements in all the array of the given column expression arguement, and ARRAY_MAX_GLOBAL will return the maximal element among all of the elements in all of the arrays in the given column expression. When combined with a GROUP BY clause, these operations will be performed on all of the arrays in each group. From the remainder of this page we will use the following table T in our examples: Category vals a [1,3,4] b [3,5,6,7] a [30,50,60] ARRAY_COUNT_GLOBAL This function returns the number of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr)) Usage Example SELECT Category, ARRAY_COUNT_GLOBAL(vals) AS cnt FROM T GROUP BY Category; Returns: category cnt a 6 b 4 ARRAY_SUM_GLOBAL This function returns the sum of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will sum the elements Usage Example SELECT Category, ARRAY_SUM_GLOBAL(vals) AS sm FROM T GROUP BY Category; Returns: category sm a 148 b 21 ARRAY_MAX_GLOBAL This function returns the maximal element taken from all the array elements in each group. Syntax ARRAY_MAX_GLOBAL(arr) AS cnt Parameter Description arr The array column over from which the function returns the maximal element Usage Example SELECT Category, ARRAY_MAX_GLOBAL(vals) AS mx FROM T GROUP BY Category; Returns: category mx a 60 b 7 ARRAY_MIN_GLOBAL This function returns the minimal element taken from all the array elements in each group. Syntax ARRAY_MIN_GLOBAL(arr) Parameter Description arr The array column from which the function will return the minimal element Usage Example SELECT Category, ARRAY_MIN_GLOBAL(vals) AS mn FROM T GROUP BY Category; Returns: category sm a 1 b 3","title":"Semi-structured aggregate functions"},{"location":"sql-functions-reference/semi-structured-functions/semi-structured-aggregate-functions/#semi-structured-aggregate-functions","text":"Aggregate semi-structured functions work globally on all the arrays in a given column expression, instead of a row-by-row application. At their simplest form (without a GROUP BY clause) - they will provide the result of globally applying the function on all of the elements of the arrays in the column expression specified as their argument. Thus, fur example, ARRAY_SUM_GLOBAL will return the sum of all the elements in all the array of the given column expression arguement, and ARRAY_MAX_GLOBAL will return the maximal element among all of the elements in all of the arrays in the given column expression. When combined with a GROUP BY clause, these operations will be performed on all of the arrays in each group. From the remainder of this page we will use the following table T in our examples: Category vals a [1,3,4] b [3,5,6,7] a [30,50,60]","title":"Semi-structured aggregate functions"},{"location":"sql-functions-reference/semi-structured-functions/semi-structured-aggregate-functions/#array_count_global","text":"This function returns the number of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will count the elements Assuming that arr is an array typed column, the following is equivalent SUM(ARRAY_COUNT(arr)) Usage Example SELECT Category, ARRAY_COUNT_GLOBAL(vals) AS cnt FROM T GROUP BY Category; Returns: category cnt a 6 b 4","title":"ARRAY_COUNT_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/semi-structured-aggregate-functions/#array_sum_global","text":"This function returns the sum of elements in the array typed column accumulated over the rows in each group. Syntax ARRAY_COUNT_GLOBAL(arr) Parameter Description arr The array column over which the function will sum the elements Usage Example SELECT Category, ARRAY_SUM_GLOBAL(vals) AS sm FROM T GROUP BY Category; Returns: category sm a 148 b 21","title":"ARRAY_SUM_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/semi-structured-aggregate-functions/#array_max_global","text":"This function returns the maximal element taken from all the array elements in each group. Syntax ARRAY_MAX_GLOBAL(arr) AS cnt Parameter Description arr The array column over from which the function returns the maximal element Usage Example SELECT Category, ARRAY_MAX_GLOBAL(vals) AS mx FROM T GROUP BY Category; Returns: category mx a 60 b 7","title":"ARRAY_MAX_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/semi-structured-aggregate-functions/#array_min_global","text":"This function returns the minimal element taken from all the array elements in each group. Syntax ARRAY_MIN_GLOBAL(arr) Parameter Description arr The array column from which the function will return the minimal element Usage Example SELECT Category, ARRAY_MIN_GLOBAL(vals) AS mn FROM T GROUP BY Category; Returns: category sm a 1 b 3","title":"ARRAY_MIN_GLOBAL"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/","text":"JSON Functions This page describes the functions used for JSON manipulation supported in \u200bFirebolt\u200b. JSON functions are used to extract values or sub-object from a JSON string. The functions accept a JSON Pointer argument which specifies the key whose value will be extracted. For an overview of JSON Pointer refer to our documentation and/or the official standard . For the rest of this page we will assume that the variable json contains the following JSON document: { \"key\": 123, \"value\": { \"dyid\": 987, \"uid\": \"987654\", \"keywords\" : [\"insanely\",\"fast\",\"analytics\"], \"tagIdToHits\": { \"map\": { \"1737729\": 32, \"1775582\": 35 } }, \"events\":[ { \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }, { \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } } ] } } Type parameters Some of the functions described below accept a type parameter . This parameter is given as a literal string name of one of the supported Firebolt SQL types that correspond to the expected type to be found under the key pointed by the JSON pointer parameter. The JSON type system has fewer types than SQL, therefore not all SQL types are supported by the type parameter Supported Types INT - used for integers as well as JSON boolean DOUBLE - used for real numbers. It will also work with integers, for performance reasons prefer using INT when you know that the values in the JSON document are integers TEXT ARRAY of one of the above types Types that are not supported : DATE , DATETIME , FLOAT - for real numbers always use DOUBLE . JSON_EXTRACT Takes an expression containing JSON string, a JSON Pointer, and a type parameter, and returns a typed scalar, or an array pointed by the JSON Pointer. If the key pointed by the JSON pointer is not found, or the type of the value under that key is different from the one specified, the function returns NULL Syntax \u200b\u200bJSON_EXTRACT(json, json_pointer_expression, expected_type) Parameter Type Description json TEXT The JSON document from which the value is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the value in the JSON document expected_type Literal String A literal string name of the expected return type. See supported types bellow Return Value If the key pointed by the JSON path exists and its type conforms with the expected_type parameter - the value under that key. Otherwise, return NULL Usage example SELECT JSON_EXTRACT(json,'/value/dyid', 'INT') Returns: 987 JSON_EXTRACT(json, '/value/no_such_key', 'TEXT') Returns: NULL JSON_EXTRACT(json, '/value/data/uid', 'INT') Returns: NULL since the JSON type under that key is a string. JSON_EXTRACT(json,'/value/keywords', 'ARRAY(TEXT)') Returns: [\"insanely\",\"fast\",\"analytics\"] JSON_EXTRACT_RAW Returns the scalar or sub-object pointed by the JSON Pointer as a string. Syntax \u200b\u200bJSON_EXTRACT_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the sub-object is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the sub-object in the JSON Return value A string representation of the scalar or sub-object under the specified key, if such key exists. Otherwise NULL Usage Example SELECT JSON_EXTRACT_RAW(json,'/value/dyid') Returns: \"987\" JSON_EXTRACT_RAW(json, '/value/data/tagIdToHits') Returns (as a TEXT): \"map\": { \"1737729\": 32, \"1775582\": 35 } JSON_EXTRACT_ARRAY_RAW Returns a string representation of a JSON array pointed by the supplied JSON pointer. This function is useful when working with heterogeneously typed arrays and arrays containing JSON objects in which case each object will be further processed by functions such as TRANSFORM . Syntax \u200b\u200bJSON_EXTRACT_ARRAY_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the array is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the array in the JSON Return value A Firebolt array whose elements are string representations of the scalars or objects contained in the JSON array under the specified key, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_ARRAY_RAW(json, '/value/events') Returns (as an array of TEXT): [ '{ \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }', '{ \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } }' ] Note that the single quotes denote that these are SQL strings, within a Firebolt array. The exact output may vary depending on the selected output format. JSON_EXTRACT_KEYS Returns an array of strings containing the keys under the (sub)-object pointed by the JSON Pointer. Syntax JSON_EXTRACT_KEYS(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from the keys are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose keys are to be extracted Return value A Firebolt array of strings consisting the keys of JSON (sub)-object, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_KEYS(json, 'value') FROM RAW_JSON Returns: [\"dyid\",\"uid\",\"keywords\",\"tagIdToHits\",\"events\"] JSON_EXTRACT_VALUES Returns an array of string representations, each element containing the value (scalar or sub-object) pointed by the JSON Pointer. Syntax \u200b\u200bJSON_EXTRACT_VALUES(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the values are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose values are to be extracted Return value A Firebolt array of strings consisting the values of the JSON (sub)-object, if such key exists. Otherwise NULL Usage example select JSON_EXTRACT_VALUES(j, 'value') FROM RAW_JSON Returns: [ '\"987\"', '\"987654\"', '[\"insanely\",\"fast\",\"analytics\"]', '{\"map\":{\"1737729\":32,\"1775582\":35}}', '[{\"EventId\":547,\"EventProperties\":{\"UserName\":\"John Doe\",\"Successful\":true}},{\"EventId\":548,\"EventProperties\":{\"ProductID\":\"xy123\",\"items\":2}}]' ] Here, as with JSON_EXTRACT_KEYS , the single quotes are to illustrate that it is a Firebolt array whose elements are SQL strings. The results as a whole is not a JSON string. Each element of the result is.","title":"JSON Functions"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#json-functions","text":"This page describes the functions used for JSON manipulation supported in \u200bFirebolt\u200b. JSON functions are used to extract values or sub-object from a JSON string. The functions accept a JSON Pointer argument which specifies the key whose value will be extracted. For an overview of JSON Pointer refer to our documentation and/or the official standard . For the rest of this page we will assume that the variable json contains the following JSON document: { \"key\": 123, \"value\": { \"dyid\": 987, \"uid\": \"987654\", \"keywords\" : [\"insanely\",\"fast\",\"analytics\"], \"tagIdToHits\": { \"map\": { \"1737729\": 32, \"1775582\": 35 } }, \"events\":[ { \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }, { \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } } ] } }","title":"JSON Functions"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#type-parameters","text":"Some of the functions described below accept a type parameter . This parameter is given as a literal string name of one of the supported Firebolt SQL types that correspond to the expected type to be found under the key pointed by the JSON pointer parameter. The JSON type system has fewer types than SQL, therefore not all SQL types are supported by the type parameter","title":"Type parameters"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#supported-types","text":"INT - used for integers as well as JSON boolean DOUBLE - used for real numbers. It will also work with integers, for performance reasons prefer using INT when you know that the values in the JSON document are integers TEXT ARRAY of one of the above types Types that are not supported : DATE , DATETIME , FLOAT - for real numbers always use DOUBLE .","title":"Supported Types"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#json_extract","text":"Takes an expression containing JSON string, a JSON Pointer, and a type parameter, and returns a typed scalar, or an array pointed by the JSON Pointer. If the key pointed by the JSON pointer is not found, or the type of the value under that key is different from the one specified, the function returns NULL Syntax \u200b\u200bJSON_EXTRACT(json, json_pointer_expression, expected_type) Parameter Type Description json TEXT The JSON document from which the value is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the value in the JSON document expected_type Literal String A literal string name of the expected return type. See supported types bellow Return Value If the key pointed by the JSON path exists and its type conforms with the expected_type parameter - the value under that key. Otherwise, return NULL Usage example SELECT JSON_EXTRACT(json,'/value/dyid', 'INT') Returns: 987 JSON_EXTRACT(json, '/value/no_such_key', 'TEXT') Returns: NULL JSON_EXTRACT(json, '/value/data/uid', 'INT') Returns: NULL since the JSON type under that key is a string. JSON_EXTRACT(json,'/value/keywords', 'ARRAY(TEXT)') Returns: [\"insanely\",\"fast\",\"analytics\"]","title":"JSON_EXTRACT"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#json_extract_raw","text":"Returns the scalar or sub-object pointed by the JSON Pointer as a string. Syntax \u200b\u200bJSON_EXTRACT_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the sub-object is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the sub-object in the JSON Return value A string representation of the scalar or sub-object under the specified key, if such key exists. Otherwise NULL Usage Example SELECT JSON_EXTRACT_RAW(json,'/value/dyid') Returns: \"987\" JSON_EXTRACT_RAW(json, '/value/data/tagIdToHits') Returns (as a TEXT): \"map\": { \"1737729\": 32, \"1775582\": 35 }","title":"JSON_EXTRACT_RAW"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#json_extract_array_raw","text":"Returns a string representation of a JSON array pointed by the supplied JSON pointer. This function is useful when working with heterogeneously typed arrays and arrays containing JSON objects in which case each object will be further processed by functions such as TRANSFORM . Syntax \u200b\u200bJSON_EXTRACT_ARRAY_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the array is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the array in the JSON Return value A Firebolt array whose elements are string representations of the scalars or objects contained in the JSON array under the specified key, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_ARRAY_RAW(json, '/value/events') Returns (as an array of TEXT): [ '{ \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }', '{ \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } }' ] Note that the single quotes denote that these are SQL strings, within a Firebolt array. The exact output may vary depending on the selected output format.","title":"JSON_EXTRACT_ARRAY_RAW"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#json_extract_keys","text":"Returns an array of strings containing the keys under the (sub)-object pointed by the JSON Pointer. Syntax JSON_EXTRACT_KEYS(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from the keys are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose keys are to be extracted Return value A Firebolt array of strings consisting the keys of JSON (sub)-object, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_KEYS(json, 'value') FROM RAW_JSON Returns: [\"dyid\",\"uid\",\"keywords\",\"tagIdToHits\",\"events\"]","title":"JSON_EXTRACT_KEYS"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/#json_extract_values","text":"Returns an array of string representations, each element containing the value (scalar or sub-object) pointed by the JSON Pointer. Syntax \u200b\u200bJSON_EXTRACT_VALUES(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the values are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose values are to be extracted Return value A Firebolt array of strings consisting the values of the JSON (sub)-object, if such key exists. Otherwise NULL Usage example select JSON_EXTRACT_VALUES(j, 'value') FROM RAW_JSON Returns: [ '\"987\"', '\"987654\"', '[\"insanely\",\"fast\",\"analytics\"]', '{\"map\":{\"1737729\":32,\"1775582\":35}}', '[{\"EventId\":547,\"EventProperties\":{\"UserName\":\"John Doe\",\"Successful\":true}},{\"EventId\":548,\"EventProperties\":{\"ProductID\":\"xy123\",\"items\":2}}]' ] Here, as with JSON_EXTRACT_KEYS , the single quotes are to illustrate that it is a Firebolt array whose elements are SQL strings. The results as a whole is not a JSON string. Each element of the result is.","title":"JSON_EXTRACT_VALUES"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/json-pointer/","text":"JSON Pointer This page describes the syntax and semantics of JSON Pointer, a way to access specific elements in a JSON document. For a formal specification see RFC6901 . A JSON Pointer is a string starting with / (slash) denoting the root of the JSON document, followed by a sequence of property names (keys) or zero-based ordinal numbers, separated but slashes. It treats the JSON document as a typical filesystem. In the case of ordinal number, they can be used to address the n-th property, or the n-th an element of an array. The tilde ~ and slash / characters have special meanings, therefore if a property name contains those characters they should be escaped. For a literal tilde ~ use ~0 For a literal slash / use ~1 Let's consider the following JSON document: { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } Here's is a list of JSON Pointers and the results of their evaluation Pointer Result Notes / { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } The whole document /key 123 /key~0with~0tilde 2 Note the use of escaping /key~1with~1slash 3 Note the use of escaping /2 3 Access by position /value/keywords/1 \"fast\" Accessing an array element","title":"JSON Pointer"},{"location":"sql-functions-reference/semi-structured-functions/json-functions/json-pointer/#json-pointer","text":"This page describes the syntax and semantics of JSON Pointer, a way to access specific elements in a JSON document. For a formal specification see RFC6901 . A JSON Pointer is a string starting with / (slash) denoting the root of the JSON document, followed by a sequence of property names (keys) or zero-based ordinal numbers, separated but slashes. It treats the JSON document as a typical filesystem. In the case of ordinal number, they can be used to address the n-th property, or the n-th an element of an array. The tilde ~ and slash / characters have special meanings, therefore if a property name contains those characters they should be escaped. For a literal tilde ~ use ~0 For a literal slash / use ~1 Let's consider the following JSON document: { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } Here's is a list of JSON Pointers and the results of their evaluation Pointer Result Notes / { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } The whole document /key 123 /key~0with~0tilde 2 Note the use of escaping /key~1with~1slash 3 Note the use of escaping /2 3 Access by position /value/keywords/1 \"fast\" Accessing an array element","title":"JSON Pointer"},{"location":"sql-reference/commands/","text":"Commands reference","title":"Commands reference"},{"location":"sql-reference/commands/#commands-reference","text":"","title":"Commands reference"},{"location":"sql-reference/commands/ddl-commands/","text":"DDL commands Firebolt supports these DDL (data definition language) commands so that you can create, manipulate, and modify objects in your tables. The commands include: ALTER CREATE START ENGINE STOP ENGINE ATTACH ENGINE DETACH ENGINE (deprecated) DESCRIBE DROP SHOW ALTER The ALTER command enables you to edit the configuration of an engine, and drop data from a fact table, Read more about these topics: ALTER ENGINE ALTER ENGINE DROP PARTITION ALTER ENGINE The ALTER ENGINE command enables you to update the engine configuration. Syntax ALTER ENGINE <engine_name> SET [SCALE TO <scale> [ABORT = TRUE|FALSE ]] [SPEC = <spec> [ABORT = TRUE|FALSE]] [AUTO_STOP = <minutes] [RENAME TO <new_name>] [WARMUP = <warmup_method>] Parameters Parameter Description Mandatory? Y/N <engine_name> Name of the engine to be altered. Y SCALE = <scale> Valid scale numbers include any INT between 1 to 128. ABORT is an optional parameter (default=false): ABORT=FALSE means that currently running queries aren\u2019t aborted. The old engine only terminates once the new engine scale is ready, and the running queries are complete. ABORT=TRUE means that once the new engine is ready, the old engine is terminated, and running queries in it are aborted. N SPEC = <spec> Indicates the EC2 instance type, for example, 'm5.xlarge' ABORT is an optional parameter (default=false): ABORT=FALSE means that currently running queries aren\u2019t aborted. The old engine only terminates once the new engine scale is ready, and the running queries are complete. ABORT=TRUE means that once the new engine is ready, the old engine is terminated, and running queries in it are aborted. N AUTO_STOP = <minutes> The number of minutes after which the engine automatically stops, where 0 indicates that AUTO_STOP is disabled. N RENAME TO <new_name> Indicates the new name for the engine. For example: RENAME TO new_engine_name N WARMUP =<warmup_method> The warmup method that should be used, the following options are supported: MINIMAL On-demand loading (both indexes and tables' data). PRELOAD_INDEXES Load indexes only (default). PRELOAD_ALL_DATA Full data auto-load (both indexes and table data - full warmup). N Example: Change engine scale ALTER ENGINE my_engine SET SCALE TO 1 ALTER TABLE DROP PARTITION The ALTER TABLE DROP PARTITION enables you to delete data from a fact table by dropping a partition. Syntax ALTER TABLE <table_name> DROP PARTITION <partition_expr> Parameters Parameter Description Mandatory? Y/N <table_name> Name of the table to be altered. Y <partition_expr> The partition expression. The following expressions are supported: Literal (or a comma-separated list of literals) A function applied on a literal. The following functions are supported: EXTRACT , and DATE_FORMAT A combination of the 2 above The partitions that match the provided expression will be dropped. In-case a partition key is composed of multiple values - all the values must be provided to perform the drop. Y For usage examples and additional details read more here . CREATE The CREATE command enables you to create an engine, a database, an external table , a Firebolt fact or a dimension table, view, join index , and aggregating index . GENERATE is used to populate an aggregating index once created. Read more about these topics: For engines, read more here For databases, read more here . For external tables, read more here . For external tables based on an AWS Glue table, read more here . For Firebolt tables, read more here . For views, read more here . For join index, read more here . For aggregating index, read more here . CREATE ENGINE Syntax CREATE ENGINE [IF NOT EXISTS] <engine_name> [WITH <properties>] Where <properties> are: REGION = '<aws_region>' ENGINE_TYPE = <type> SPEC = '<spec>' SCALE = <scale> AUTO_STOP = <minutes> WARMUP = [ MINIMAL | PRELOAD_INDEXES | PRELOAD_ALL_DATA ] Parameters Parameter Description Mandatory? Y/N <engine_name> An identifier that specifies the name of the engine. For example: my_engine Y REGION = '<aws_region>' The AWS region in which the engine runs. If not specified, 'us-west-2' is used as default. N ENGINE_TYPE = <type> The engine type. The <type> can have one of the following values: GENERAL_PURPOSE DATA_ANALYTICS If not specified - GENERAL_PURPOSE is used as default. Usage example: CREATE ENGINE ... ENGINE_TYPE = GENERAL_PURPOSE N SPEC = '<spec>' The AWS EC2 instance type, for example, 'm5d.xlarge' . If not specified, 'i3.4xlarge' is used as default. N SCALE = <scale> Specifies the scale of the engine. The scale can be any INT between 1 to 128. If not specified, 2 is used as default. N AUTO_STOP = <minutes> Indicates the amount of time (in minutes) after which the engine automatically stops. The default value is 20. Setting the minutes to 0 indicates that AUTO_STOP is disabled. N WARMUP = <warmup_method> The warmup method that should be used, the following options are supported: MINIMAL On-demand loading (both indexes and tables' data). PRELOAD_INDEXES Load indexes only (default). PRELOAD_ALL_DATA Full data auto-load (both indexes and table data - full warmup). N Example - create an engine with (non-default) properties: CREATE ENGINE my_engine WITH SPEC = 'c5d.4xlarge' SCALE = 8 CREATE DATABASE Syntax CREATE DATABASE [IF NOT EXISTS] <database_name> [WITH <properties>] Where <properties> are: REGION = '<aws_region> ATTACHED_ENGINES = ( '<engine_name>' [, ... ] ) DEFAULT_ENGINE = 'engine_name' DESCRIPTION = 'description' Parameters Parameter Description Mandatory? Y/N <database_name> An identifier that specifies the name of the database. For example: my_database Y REGION = '<aws_region>' The AWS region in which the database is configured. If not specified, 'us-west-2' is being used as default. N ATTACHED_ENGINES = ( <engine_name> [ ... ] ) A list of engine names, for example: ATTACHED_ENGINES = (my_engine_1 my_engine_2) . The specified engines must be detached from any other databases first. N DEFAULT_ENGINE = engine_name An identifier that specifies the name of the default engine. If not specified, the first engine in the attached engines list will be used as default. If a default engine is specified without specifying the list of attached engines or if the default engine is not in that list, the default engine will be both attached to the database and used as the default engine. N DESCRIPTION = 'description' The engine's description (up to 64 characters). N Example - create a database with (non-default) properties: CREATE DATABASE IF NOT EXISTS my_db WITH region = 'us-east-1' description = 'Being used for testing' CREATE EXTERNAL TABLE Syntax CREATE EXTERNAL TABLE [IF NOT EXISTS] <external_table_name> ( <column_name> <column_type> [, <column_name2> <column_type2> [, ...n] ] [, <partition_column_name> <partition_column_type> PARTITION('<regex>') ) [CREDENTIALS = (<awsCredentials>)] URL = 's3://<bucket>[/<folder>][/...]/' OBJECT_PATTERN = '<object_pattern>'[, '<object_pattern>'[, ...n]]] TYPE = (<type> [typeOptions]) [COMPRESSION = <compression_type>] Parameters Parameter Description <external_table_name> An \u200bidentifier\u200b\u200b that specifies the name of the external table. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_name> An identifier that specifies the name of the column. This name should be unique within the table. <column_type> Specifies the data type for the column. <partition_column_name> Extract partition data CREDENTIALS CREDENTIALS URL and OBJECT_PATTERN URL & OBJECT_PATTERN TYPE TYPE COMPRESSION COMPRESSION All Firebolt identifiers are case insensitive unless double-quotes are used. For more information, please see Identifier requirements . Extract partition data When data is partitioned, the partition columns are not stored in the S3 files. Instead, they can be extracted from the file path within the bucket. \u200b Syntax [<column_name> <column_type> PARTITION('<regex>')] Parameters Parameter Description \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<partition_column_name> An identifier that specifies the name of the column as it would appear in the external table (should match the hive partition name). As with any column name, this name should be unique within the table. <partition_column_type> Specifies the data type for the column. '<regex>' The regex for extracting the partition value out of the file path in S3. Guidelines for creating the regex: You do not have to reference all the partitions in your data; you can specify only the columns that you wish to include in the external table. You can extract the column value from the file name, but not from its path. For each column, a regular expression that contains a capturing group must be specified, so Firebolt can treat the captured string as the column value. When the column data type is date , Firebolt expects three capturing groups in the order of year, month, and day. The regular expression is matched against the file path, not including the s3://bucket_name/ prefix. Firebolt tries to convert the captured string to the specified type. If the type conversion fails, the value is treated as NULL. Tip In most cases, the easiest way to build a regular expression is as follows: Count the number of folders in the path, not including the bucket name. Concatenate the string [^\\/]+\\/ according to the number of folders. Prefix the regex with an additional [^\\/]+ for the file name. Wrap the [^\\/]+ in the right folder with a capturing group parenthesis, i.e ([^\\/]+). See the examples below for both hive-compatible and non-compatible partitions extractions. Here is a good explanation about matching groups , and an online tool to test your regular expressions. Example 1 - extract hive-compatible partitions: \\ Consider the following layout of files in a bucket - data is partitioned according to client type, year, and month, with multiple parquet files in each partition. The parquet files don't contain the corresponding columns, but the columns can be extracted, along with their values, by parsing the file paths, as we will see in the next section. s3://my_bucket/c_type=xyz/year=2018/month=01/part-00001.parquet s3://my_bucket/c_type=xyz/year=2018/month=01/part-00002.parquet ... s3://my_bucket/c_type=abc/year=2018/month=01/part-00001.parquet s3://my_bucket/c_type=abc/year=2018/month=01/part-00002.parquet Creating an external table using the following format: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT, c_type TEXT PARTITION('[^\\/]+\\/c_type=([^\\/]+)\\/[^\\/]+\\/[^\\/]+') ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET) Results with an external table in the following structure: c_id c_name c_type 1 name_a xyz 2 name_b xyz ... ... ... 100 name_abc xyz Example 2 - extract non-hive compatible partitions: In some cases, your S3 files may be organized in partitions that do not use the = format. For example, consider this layout: s3://my_bucket/xyz/2018/01/part-00001.parquet s3://my_bucket/xyz/2018/01/part-00002.parquet ... s3://my_bucket/abc/2018/01/part-00001.parquet s3://my_bucket/abc/2018/01/part-00002.parquet In this case, you can use the advanced PARTITION(<regex>) column definition to create the columns and extract their values. To create the same external table as we did in the hive-compatible case, use the following command: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT, c_type TEXT PARTITION('[^\\/]+\\/([^\\/]+)\\/[^\\/]+\\/[^\\/]+') ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET) As in the previous example, the values for the columns c_id and c_name are extracted from the record in the parquet file, and the values for the c_type column is extracted from the file path, according to the specified type and regular expression. The partition values can be extracted during the INSERT INTO command as well. Read more here . CREDENTIALS The credentials for accessing your AWS S3. Firebolt enables using either access key & secret or IAM role. Syntax - Authenticating using an access key & secret CREDENTIALS = (AWS_KEY_ID = '<aws_key_id>' AWS_SECRET_KEY = '<aws_secret_key>' ) Parameter Description Data type <aws_key_id> The AWS access key ID for the authorized app (Firebolt) TEXT <aws_secret_key> The AWS secret access key for the app (Firebolt) TEXT In case you don't have the access key and secret to access your S3 bucket, read more here on how to obtain them. Syntax - Authenticating using IAM role Read more on how to configure the AWS role here . CREDENTIALS = (AWS_ROLE_ARN = '<role_arn>' [AWS_ROLE_EXTERNAL_ID = '<external_id>']) Parameter Description Data type '<role_arn>' The arn_role you created in order to enable access to the required bucket. TEXT '<external_id>' Optional. This is an optional external ID that you can configure in AWS when creating the role. Specify this only if you use the external ID. TEXT URL and OBJECT_PATTERN The URL and OBJECT_PATTERN parameters are used together, to match the set of files from within the specified bucket that you wish to include as the data for the external table. Syntax URL = 's3://<bucket>[/<folder>][/...]/' OBJECT_PATTERN = '<object_pattern>'[, '<object_pattern>'[, ...n]]] Parameters Description Data type URL This is the URL of the specific bucket and path within the bucket where the relevant files are located (common path prefix). TEXT OBJECT_PATTERN Specify the data patterns to be found in your data source. For example, *.parquet indicates that all parquet files should be found. TEXT The following wildcards are supported: '*' matches any sequence of characters '?' matches any single character [SET] matches any single character in the specified set [!SET] matches any character, not in the specified set. Example: In the following layout of objects in a bucket, the data is partitioned according to client type, year, and month, with multiple parquet files in each partition. The examples demonstrate how choosing both URL and OBJECT_PATTERN impacts the objects that are retrieved from S3. s3://bucket/c_type=c_type=xyz/year=2018/month=01/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2018/month=01/part-00002.parquet ... s3://bucket/c_type=c_type=xyz/year=2018/month=12/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2018/month=12/part-00002.parquet ... s3://bucket/c_type=c_type=xyz/year=2019/month=01/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2019/month=01/part-00002.parquet ... s3://bucket/c_type=c_type=xyz/year=2020/month=01/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2020/month=01/part-00002.parquet ... s3://bucket/c_type=c_type=abc/year=2018/month=01/part-00001.parquet s3://bucket/c_type=c_type=abc/year=2018/month=01/part-00002.parquet ... Following are some common use cases for URL and object pattern combinations: Use cases Syntax Get all files for file type xyz URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = '*' URL = 's3://bucket/' OBJECT_PATTERN = 'c_type=xyz/*' Get one specific file: c_type=xyz/year=2018/month=01/part-00001.parquet URL = 's3://bucket/c_type=xyz/year=2018/month=01/' OBJECT_PATTERN = 'c_type=xyz/year=2018/month=01/part-00001.parquet' URL = 's3://bucket/c_type=xyz/year=2018/month=01/' OBJECT_PATTERN = '*/part-00001.parquet' As can be seen in this example, the \u200b URL \u200b is used to get only the minimal set of files (c_type files in the bucket from January 2018), and then from within those matching files, the \u200b OBJECT_PATTERN \u200b\u200b is matched against the full path of the file (without the bucket name). Get all parquet files for type xyz URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = '*.parquet' Get all client data (for all types) from 2018 and 2019 URL = 's3://bucket/' OBJECT_PATTERN = ' /year=2018/ ', ' /year=2019/ ' Get all files for type xyz, from the past 3 months URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = ' /year=2019/month=12/ ', ' /year=2020/month=01/ ', ' /year=2020/month=02/ ' No files matched, since the URL parameter does not support wildcards. URL = 's3://bucket/' OBJECT_PATTERN = ' /year=2019/month=12/ ', ' /year=2020/month=01/ ', ' /year=2020/month=02/ ' Get all files of type xyz from the first six months of 2019 URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = '/year=2019/month=0[1-6]' TYPE Specifies the type of the files in S3. The following types and type options are supported. TYPE = (CSV [SKIP_HEADER_ROWS = {1|0}]) With TYPE = (CSV SKIP_HEADER_ROWS = 1) , Firebolt assumes that the first row in each file read from S3 is a header row and skips it when ingesting data. When set to 0 , which is the default if not specified, Firebolt ingests the first row as data. * TYPE = (JSON [PARSE_AS_TEXT = {'TRUE'|'FALSE'}]) With TYPE = (JSON PARSE_AS_TEXT = 'TRUE') , Firebolt ingests each JSON object literal in its entirety into a single column of type TEXT . With TYPE = (JSON PARSE_AS_TEXT = 'FALSE') , Firebolt expects each key in a JSON object literal to map to a column in the table definition. During ingestion, Firebolt inserts the key's value into the corresponding column. * TYPE = (ORC) * TYPE = (PARQUET) * TYPE = (TSV) Example Creating an external table that reads parquet files from S3 is being done with the following command: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET) COMPRESSION Specifies the compression type of the files in S3. Syntax [COMPRESSION = <compression_type>] Parameters Description <compression_type> An identifier specifies the compression type. GZIP is supported. Example The example below creates an external table to ingest parquet files from S3 that are compressed using gzip. The credentials for an IAM user with access to the bucket are provided. CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = 'AKIAIOSFODNN7EXAMPLE' AWS_SECRET_KEY = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') URL = 's3://mybucket/' OBJECT_PATTERN= '*.parquet.gz' TYPE = (PARQUET) COMPRESSION = GZIP CREATE EXTERNAL TABLE based on an AWS Glue table Syntax CREATE EXTERNAL TABLE [IF NOT EXISTS] <external_table_name> CREDENTIALS = ( AWS_KEY_ID = '<aws_key_id>' AWS_SECRET_KEY = '<aws_secret_key>' ) META_STORE = (TYPE='Glue' DATABASE_NAME=<db_name> TABLE_NAME=<table_name>) In order to access Glue, make sure to allow the following actions in the AWS Policy: \"s3:GetObject\" \"s3:GetObjectVersion\" \"s3:GetBucketLocation\" \"s3:ListBucket\" \"glue:GetTables\" Click here to download a templated policy you can use. Make sure to replace: <bucket> and <prefix> with the actual AWS S3 bucket name path and prefix where the AWS Glue data is stored. <db_name> with the name of the AWS Glue database. Parameter Description Data type <external_table_name> An \u200bidentifier\u200b\u200b that specifies the name of the external table. This name should be unique within the database. <db_name> The name of the database in AWS Glue TEXT <table_name> The name of the table in AWS Glue TEXT '<aws_key_id>' The AWS access key ID for the authorized app (Firebolt) TEXT '<aws_secret_key>' The AWS secret access key for the app (Firebolt) TEXT Example An external table based on an AWS Glue table 'glue_table' in 'glue_db' database: CREATE EXTERNAL TABLE IF NOT EXISTS my_external_table CREDENTIALS = ( AWS_KEY_ID = 'AKIAIOSFODNN7EXAMPLE' AWS_SECRET_KEY = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' ) META_STORE = (TYPE='Glue' DATABASE_NAME='glue_db' TABLE_NAME='glue_table') CREATE FACT / DIMENSION TABLE Creates a new FACT/DIMENSION table in the current database. Firebolt also supports creating a table as select (also referred to as CTAS) - read more here . Fact table syntax CREATE FACT TABLE [IF NOT EXISTS] <table_name> ( <column_name> <column_type> [constraints] [, <column_name2> <column_type2> [constraints] [, ...n]] ) PRIMARY INDEX <column_name>[, <column_name>[, ...n]] [PARTITION BY <column_name>[, <column_name>[, ...n]]] Partitions are only supported on FACT tables. Dimension table syntax CREATE DIMENSION TABLE [IF NOT EXISTS] <table_name> ( <column_name> <column_type> [constraints] [, <column_name2> <column_type2> [constraints] [, ...n]] ) [PRIMARY INDEX <column_name>[, <column_name>[, ...n]]] Parameter Description <table_name> An \u200bidentifier\u200b\u200b that specifies the name of the table. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_name> An identifier that specifies the name of the column. This name should be unique within the table. <column_type> Specifies the data type for the column. All identifiers are case insensitive unless double-quotes are used. For more information, please see our identifier requirements page . Read more on Column constraints & default expression PRIMARY INDEX specifier PARTITION BY specifier Column constraints & default expression Syntax <column_name> <column_type> [UNIQUE] [NULL|NOT NULL] [DEFAULT <expr>] Firebolt supports the following column constraints: Constraint Description Default value DEFAULT <expr> Determines the default value that is used instead of NULL value is inserted. NULL | NOT NULL Determines if the column may or may not contain NULLs. NOT NULL UNIQUE This is an optimization hint to tell Firebolt that this column will be queried for unique values, such as through a COUNT(DISTINCT) function. This will not raise an error if a non-unique value is added to the column. Note that nullable columns can not be used in Firebolt indexes (Primary, Aggregating, or Join indexes). Example - Creating a table with nulls and not nulls: This example illustrates different use cases for column definitions and INSERT statements: Explicit NULL insert: a direct insertion of a NULL value into a particular column. Implicit insert: an INSERT statement with missing values for a particular column. The example uses a fact table in which to insert different values. First, we create the fact table as follows: CREATE FACT TABLE t1 ( col1 INT NULL , col2 INT NOT NULL UNIQUE, col3 INT NULL DEFAULT 1, col4 INT NOT NULL DEFAULT 1, col5 TEXT ) PRIMARY INDEX col2; Once we've created the table, we can manipulate the values with different INSERT statements. Following are detailed descriptions of different examples of these: INSERT statement Results and explanation INSERT INTO t1 VALUES (1,1,1,1,1) 1 is inserted into each column INSERT INTO t1 VALUES (NULL,1,1,1,1) col1 is NULL , and this is an explicit NULL insert, so NULL is inserted successfully. INSERT INTO t1 (col2,col3,col4,col5) VALUES (1,1,1,1) This is an example of explicit and implicit INSERT statements. col1 is NULL, which is an implicit insert, as a default expression was not specified. In this case, col1 is treated as NULL DEFAULT NULL, so Firebolt inserts NULL. INSERT INTO t1 VALUES (1,NULL,1,1,1) INSERT INTO t1 (col1,col3,col4,col5) VALUES (1,1,1,1) The behavior here depends on the column type. For both cases, a \u201cnull mismatch\u201d event occurs. In the original table creation, col2 receives a NOT NULL value. Since a default expression is not specified, both of these INSERT statements try to insert NOT NULL DEFAULT NULL into col2. This means that there is an implicit attempt to insert NULL in both cases. In this particular case, the data type for col4 is INT. Because NOT NULL is configured on col4 as well, it cannot accept NULL values. If the data type for col4 was TEXT, for example, the result would have been an insert of '' . INSERT INTO t1 VALUES (1,1,NULL,1,1) col3 is NULL DEFAULT 1, and this is an explicit insert. NULL is inserted INSERT INTO t1 (col1,col2,col4,col5) VALUES (1,1,1,1) col3 is NULL DEFAULT 1 . This is an implicit insert, and a default expression is specified, so 1 is inserted INSERT INTO t1 VALUES (1,1,1,NULL,1) col4 is NOT NULL DEFAULT 1 , and this is an explicit insert. Therefore, a \u201cnull mismatch\u201d event occurs. In this particular case, since the data type for col4 is INT, the result is an error. If the data type for col4 was TEXT, for example, the result would have been an insert of '' . INSERT INTO t1 (col1,col2,col3,col5) VALUES (1,1,1,1) col4 is NOT NULL DEFAULT 1 , and this is an implicit insert. Therefore, the default expression is used, and 1 is inserted INSERT INTO t1 VALUES (1,1,1,1,NULL) INSERT INTO t1 (col1,col2,col3,col4) VALUES (1,1,1,1) The nullability and default expression for col5 were not specified. In this case, Firebolt treats col5 as NOT NULL DEFAULT NULL . For the explicit insert, Firebolt attempts to insert NULL into a NOT NULL int column, and a \u201cnull mismatch\u201d event results. For the implicit insert, Firebolt resorts to the default, and again, attempts to insert NULL. Similar to the explicit NULL case - an empty value '' is inserted. PRIMARY INDEX The PRIMARY INDEX is a sparse index containing sorted data based on the indexed field. This index clusters and sorts data as it is ingested, without affecting data scan performance. Notice The PRIMARY INDEX is mandatory for FACT tables and optional for DIMENSION tables. Primary index syntax PRIMARY INDEX <column_name>[, <column_name>[, ...n]] The following table describes the primary index parameters: Parameter. Description Mandatory? <column_name> Specifies the name of the column in the Firebolt table which composes the index. At least one column must be used for configuring the index. Y PARTITION BY The PARTITION BY specifier contains the definition of the columns by which the table will be split into physical parts. Those columns are considered to be the partition key of the table. The key can be any of the table columns which is not nullable. Only FACT tables can be partitioned. When the partition key is set with multiple columns, all will be used as the partition boundaries. Partition syntax PARTITION BY <column_name>[, <column_name>[, ...n]] Read more on how to work with partitions here . CTAS - CREATE FACT / DIMENSION TABLE AS SELECT Creates a table and loads data into it based on the SELECT query. The table column names and types are automatically inferred based on the output columns of the SELECT . When specifying explicit column names those override the column names inferred from the SELECT . CREATE FACT TABLE AS SELECT syntax CREATE FACT TABLE <table_name> [(<column_name>[, ...n] )] PRIMARY INDEX <column_name>[, <column_name>[, ...n]] [PARTITION BY <column_name>[, <column_name>[, ...n]]] AS <select_query> CREATE DIMENSION TABLE AS SELECT syntax CREATE DIMENSION TABLE <table_name> [(<column_name>[, ...n] )] [PRIMARY INDEX <column_name>[, <column_name>[, ...n]]] AS <select_query> General parameters Parameter Description <table_name> An \u200bidentifier\u200b\u200b that specifies the name of the external table. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_name> An identifier that specifies the name of the column. This name should be unique within the table. <select_query > Any valid select query Read more on the primary index specifier here . CREATE VIEW Views allow you to use a query as if it were a table. Views are useful to filter, focus and simplify a database for users. They provide a level of abstraction that can make subqueries easier to write, especially for commonly referenced subsets of data. A view in Firebolt executes its query each time the view is referenced. In other words, the view results are not stored for future usage, and therefore using views does not provide a performance advantage. Syntax CREATE VIEW [IF NOT EXISTS] <name> [(<column_list>)] AS SELECT <select_statement> Parameters Parameter Description <name> An \u200bidentifier\u200b\u200b that specifies the name of the view. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_list> An optional list of column names to be used for columns of the view. If not given, the column names are deduced from the query. <select_statement> The select statement for creating the view Example CREATE VIEW fob_shipments AS SELECT l_shipmode, l_shipdate, l_linestatus, l_orderkey, FROM lineitem WHERE l_shipdate > '1990-01-01' AND l_shipmode = 'FOB' CREATE JOIN INDEX Join indexes can accelerate queries that use JOIN operations on dimension tables. Under certain circumstances, a join index can significantly reduce the compute requirements required to perform a join at query runtime. Syntax CREATE JOIN INDEX [IF NOT EXISTS] <unique_join_index_name> ON <dimension_table_name> ( <unique_join_key_column>, <dimension_column>[, ...n] ) General parameters Parameter Description <unique_join_index_name> A unique name for the join index. <dimension_table_name> The name of the dimension table on which the index is configured. <unique_join_key_column> The column name that is used in the join\u2019s ON clause. <dimension_column> The column name which is being loaded into memory from the dimension table. More than one column can be specified. For better performance, whenever possible, use the UNIQUE column attribute in the dimension table definition for the column that is used as the join key in queries. Since the join index is loaded into engine RAM, make sure to choose only the subset of dimension table columns that appear in queries that use the join. Example: Create join index with specific columns In the following example, we create a join index on the dimension table my_dim , and store the columns email and country in the index: CREATE DIMENSION TABLE my_dim( my_dim_id BIGINT UNIQUE, email TEXT, country TEXT, city TEXT, cellolar TEXT) PRIMARY INDEX my_dim_id; CREATE JOIN INDEX my_dim_join_idx ON my_dim (my_dim_id, email, country); CREATE AGGREGATING INDEX Creating an aggregating index can be done as follows: For an empty table - use the following syntax . For a table already populated with data - use the following syntax . Syntax for aggregating index on an empty table CREATE AGGREGATING INDEX <agg_index_name> ON <fact_table_name> ( <key_column>[, ...n], <aggregation>[, ...n] ); Click here to read about the different parameters. The index is being populated automatically as data is being loaded to the table. Syntax for aggregating index on a populated table CREATE AND GENERATE AGGREGATING INDEX <agg_index_name> ON <fact_table_name> ( <key_column>[, ...n], <aggregation>[, ...n] ); Generating the index after data was loaded to the table is a memory-heavy operation. Parameters Parameter Description <agg_index_name> Specifies a unique name for the index <fact_table_name> Specifies the name of the fact table referenced by this index <key_column> Specifies column name from the <fact_table_name> used for the index <aggregation> Specifies one or more aggregation functions to be applied on a <key_column> , such as SUM , COUNT , AVG , and more. Example: Create an aggregating index In the following example, we create an aggregating index on the fact table my_fact , to be used in the following query: SELECT product_name, count(DISTINCT source), sum(amount) FROM my_fact GROUP BY product_name; The aggregating index is being created as follows: CREATE AGGREGATING INDEX my_fact_agg_idx ON my_fact ( product_name, count(distinct source), sum(amount) ); In order to enjoy the performance boost provided by the index - all the columns/measurements in the query should exists in the aggregating index. START ENGINE The START ENGINE command enables you to start a stopped engine. Syntax START ENGINE <engine_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to be started Y STOP ENGINE The STOP ENGINE command enables you to stop a running engine. Syntax STOP ENGINE <engine_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to be stopped Y ATTACH ENGINE The ATTACH ENGINE command enables you to attach an engine to a database. Syntax ATTACH ENGINE <engine_name> TO <database_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to attach. Y <database_name> The name of the database to attach engine <engine_name> to. Y DETACH ENGINE (deprecated) Deprecated. Avoid using this statement and use DROP ENGINE instead. Allows you to detach an engine from a database. Syntax DETACH ENGINE <engine_name> FROM <database_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to detach. Y <database_name> The name of the database to detach engine <engine_name> from. Y DESCRIBE Lists all columns and data types for the table. Once the results are displayed, you can also export them to CSV or JSON. Syntax DESCRIBE <table_name> Example The following lists all columns and data types for the table named prices : DESCRIBE prices Returns: +------------+-------------+-----------+----------+ | table_name | column_name | data_type | nullable | +------------+-------------+-----------+----------+ | prices | item | text | 0 | | prices | num | int | 0 | +------------+-------------+-----------+----------+ DROP DROP drops or removes the specified object from Firebolt. Firebolt supports dropping a table and view. Read more on these topics: For dropping an engine, read here . For dropping an index, read here . For dropping a table, read here . For dropping a database, read here For dropping a view, read here . DROP ENGINE Syntax DROP ENGINE [IF EXISTS] <engine_name> Parameter Description <engine_name> The name of the engine to be deleted. DROP INDEX Syntax DROP [AGGREGATING | JOIN] INDEX [IF EXISTS] <index_name> Parameter Description <index_name> The name of the index to be deleted. DROP TABLE Syntax DROP TABLE [IF EXISTS] <table_name> Parameter Description <table_name> The name of the table to be deleted. Notice For external tables, this means that the table is removed from Firebolt, but not from the source. DROP DATABASE Syntax Deletes the database and all of its tables and attached engines. DROP DATABASE [IF EXISTS] <database_name> Parameter Description <database_name> The name of the database to be deleted DROP VIEW Syntax DROP VIEW [IF EXISTS] <view_name> Parameter Description <view_name> The name of the view to be deleted. SHOW SHOW can list several objects and their details. SHOW CACHE Returns the current SSD usage ( ssd_usage ) for the current engine. SHOW CACHE returns values at the engine level, not by each node. Syntax SHOW CACHE; The results of SHOW CACHE are formatted as follows: <ssd_used> / <ssd _ available> GB ( <percent_utilization> %) These components are defined as follows: Component Description <ssd_used> The amount of storage currently used on your engine. This data includes storage that Firebolt reserves for internal usage. <ssd _ available> The amount of available storage on your engine. <percent_utilization> The percent of used storage as compared to available storage. Example returned output is shown below. | ssd_usage | +-----------------------+ | 3.82/73.28 GB (5.22%) | SHOW COLUMNS Lists columns and their properties for a specified table. Returns <table_name> , <column_name> , <data_type> , and nullable for each column. Syntax SHOW COLUMNS <table_name>; Parameter Description <table_name> The name of the table to be analyzed. Example SHOW COLUMNS prices; Returns : ------------+-------------+-----------+----------+ | table_name | column_name | data_type | nullable | +------------+-------------+-----------+----------+ | prices | item | text | 0 | | prices | num | int | 0 | +------------+-------------+-----------+----------+ SHOW DATABASES Lists databases in the current Firebolt account. Returns name , region , attached_engines , created_on , created_by , and errors for each database. Syntax SHOW DATABASES; Example SHOW DATABASES; Returns : +---------------+-----------+-------------------------------------+-----------------------------+---------------+--------+ | database_name | region | attached_engines | created_on | created_by | errors | +---------------+-----------+-------------------------------------+-----------------------------+---------------+--------+ | Tutorial1 | us-east-1 | Tutorial1_general_purpose (default) | 2021-09-30T21:25:45.401405Z | someone | - | +---------------+-----------+-------------------------------------+-----------------------------+---------------+--------+ SHOW DATABASE Shows the status for the specified database. These are the same metadata fields as SHOW DATABASES . SHOW DATABASE <database_name>; Parameter Description < database_name> The name of the database to be analyzed. SHOW ENGINES Lists all engines in the current Firebolt account. Returns engine_name , region , spec , scale , status , and attached_to for each engine. Syntax SHOW ENGINES; Example SHOW ENGINES; Returns : +--------------------+-----------+-------------+-------+---------+-------------+ | engine_name | region | spec | scale | status | attached_to | +--------------------+-----------+-------------+-------+---------+-------------+ | Tutorial_analytics | us-east-1 | r5d.4xlarge | 2 | Stopped | Tutorial | +--------------------+-----------+-------------+-------+---------+-------------+ SHOW INDEXES Lists all indexes defined in the current database. Returns index_name , table_name , type (primary, aggregating, or join), the index expression , and the size_compressed . Syntax SHOW INDEXES; Example SHOW INDEXES; Returns: +------------------------+----------------+-------------+----------------------------------------------------------------------------------------------------------+-----------------+-------------------+-------------------+--------------------+ | index_name | table_name | type | expression | size_compressed | size_uncompressed | compression_ratio | number_of_segments | +------------------------+----------------+-------------+----------------------------------------------------------------------------------------------------------+-----------------+-------------------+-------------------+--------------------+ | primary_lineitem | lineitem | primary | [\"l_orderkey\",\"l_linenumber\"] | N/A | N/A | N/A | N/A | | primary_partition_test | partition_test | primary | [\"store_id\",\"product_id\"] | N/A | N/A | N/A | N/A | | agg_lineitem | lineitem | aggregating | [\"\\\"l_suppkey\\\"\",\"\\\"l_partkey\\\"\",\"SUM(\\\"l_quantity\\\")\",\"SUM(\\\"l_extendedprice\\\")\",\"AVG(\\\"l_discount\\\")\"] | N/A | N/A | N/A | 8 | +------------------------+----------------+-------------+----------------------------------------------------------------------------------------------------------+-----------------+-------------------+-------------------+--------------------+ SHOW TABLES Lists all tables defined in the current database. Returns table_name , state , table_type , column_count , primary_index , and schema (the CREATE [EXTERNAL|FACT|DIMENSION] TABLE statement for the table). Syntax SHOW TABLES; Example SHOW TABLES; Returns : +----------------+-------+------------+--------------+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------+-------------------+-------------------+--------------------+ | table_name | state | table_type | column_count | primary_index | schema | number_of_rows | size | size_uncompressed | compression_ratio | number_of_segments | +----------------+-------+------------+--------------+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------+-------------------+-------------------+--------------------+ | ex_lineitem | Valid | EXTERNAL | 16 | N/A | CREATE EXTERNAL TABLE IF NOT EXISTS \"ex_lineitem\" (\"l_orderkey\" long NOT NULL, \"l_partkey\" long NOT NULL, \"l_suppkey\" long NOT NULL, \"l_linenumber\" int NOT NULL, \"l_quantity\" long NOT NULL, \"l_extendedprice\" long NOT NULL, \"l_discount\" long NOT NULL, \"l_tax\" long NOT NULL, \"l_returnflag\" text NOT NULL, \"l_linestatus\" text NOT NULL, \"l_shipdate\" text NOT NULL, \"l_commitdate\" text NOT NULL, \"l_receiptdate\" text NOT NULL, \"l_shipinstruct\" text NOT NULL, \"l_shipmode\" text NOT NULL, \"l_comment\" text NOT NULL) \"URL\" = 's3://firebolt-publishing-public/samples/tpc-h/parquet/lineitem/' \"OBJECT_PATTERN\" = '*.parquet' \"TYPE\" = (\"PARQUET\") | N/A | N/A | N/A | N/A | N/A | | lineitem | Valid | FACT | 16 | [l_orderkey, l_linenumber] | CREATE FACT TABLE IF NOT EXISTS \"lineitem\" (\"l_orderkey\" long NOT NULL, \"l_partkey\" long NOT NULL, \"l_suppkey\" long NOT NULL, \"l_linenumber\" int NOT NULL, \"l_quantity\" long NOT NULL, \"l_extendedprice\" long NOT NULL, \"l_discount\" long NOT NULL, \"l_tax\" long NOT NULL, \"l_returnflag\" text NOT NULL, \"l_linestatus\" text NOT NULL, \"l_shipdate\" text NOT NULL, \"l_commitdate\" text NOT NULL, \"l_receiptdate\" text NOT NULL, \"l_shipinstruct\" text NOT NULL, \"l_shipmode\" text NOT NULL, \"l_comment\" text NOT NULL) PRIMARY INDEX \"l_orderkey\", \"l_linenumber\" | N/A | N/A | N/A | N/A | 6 | | partition_test | Valid | FACT | 5 | [store_id, product_id] | CREATE FACT TABLE IF NOT EXISTS \"partition_test\" (\"transaction_id\" long NOT NULL, \"transaction_date\" timestamp NOT NULL, \"store_id\" int NOT NULL, \"product_id\" int NOT NULL, \"units_sold\" int NOT NULL) PRIMARY INDEX \"store_id\", \"product_id\" PARTITION BY EXTRACT(YEAR FROM \"transaction_date\") | N/A | N/A | N/A | N/A | N/A | | Insert_test | Valid | DIMENSION | 3 | N/A | CREATE DIMENSION TABLE IF NOT EXISTS \"Insert_test\" (\"name\" long NOT NULL, \"number\" int NOT NULL, \"other_name\" long NOT NULL) | N/A | N/A | N/A | N/A | 2 | +----------------+-------+------------+--------------+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------+-------------------+-------------------+--------------------+","title":"DDL commands"},{"location":"sql-reference/commands/ddl-commands/#ddl-commands","text":"Firebolt supports these DDL (data definition language) commands so that you can create, manipulate, and modify objects in your tables. The commands include: ALTER CREATE START ENGINE STOP ENGINE ATTACH ENGINE DETACH ENGINE (deprecated) DESCRIBE DROP SHOW","title":"DDL commands"},{"location":"sql-reference/commands/ddl-commands/#alter","text":"The ALTER command enables you to edit the configuration of an engine, and drop data from a fact table, Read more about these topics: ALTER ENGINE ALTER ENGINE DROP PARTITION","title":"ALTER"},{"location":"sql-reference/commands/ddl-commands/#alter-engine","text":"The ALTER ENGINE command enables you to update the engine configuration. Syntax ALTER ENGINE <engine_name> SET [SCALE TO <scale> [ABORT = TRUE|FALSE ]] [SPEC = <spec> [ABORT = TRUE|FALSE]] [AUTO_STOP = <minutes] [RENAME TO <new_name>] [WARMUP = <warmup_method>] Parameters Parameter Description Mandatory? Y/N <engine_name> Name of the engine to be altered. Y SCALE = <scale> Valid scale numbers include any INT between 1 to 128. ABORT is an optional parameter (default=false): ABORT=FALSE means that currently running queries aren\u2019t aborted. The old engine only terminates once the new engine scale is ready, and the running queries are complete. ABORT=TRUE means that once the new engine is ready, the old engine is terminated, and running queries in it are aborted. N SPEC = <spec> Indicates the EC2 instance type, for example, 'm5.xlarge' ABORT is an optional parameter (default=false): ABORT=FALSE means that currently running queries aren\u2019t aborted. The old engine only terminates once the new engine scale is ready, and the running queries are complete. ABORT=TRUE means that once the new engine is ready, the old engine is terminated, and running queries in it are aborted. N AUTO_STOP = <minutes> The number of minutes after which the engine automatically stops, where 0 indicates that AUTO_STOP is disabled. N RENAME TO <new_name> Indicates the new name for the engine. For example: RENAME TO new_engine_name N WARMUP =<warmup_method> The warmup method that should be used, the following options are supported: MINIMAL On-demand loading (both indexes and tables' data). PRELOAD_INDEXES Load indexes only (default). PRELOAD_ALL_DATA Full data auto-load (both indexes and table data - full warmup). N Example: Change engine scale ALTER ENGINE my_engine SET SCALE TO 1","title":"ALTER ENGINE"},{"location":"sql-reference/commands/ddl-commands/#alter-table-drop-partition","text":"The ALTER TABLE DROP PARTITION enables you to delete data from a fact table by dropping a partition. Syntax ALTER TABLE <table_name> DROP PARTITION <partition_expr> Parameters Parameter Description Mandatory? Y/N <table_name> Name of the table to be altered. Y <partition_expr> The partition expression. The following expressions are supported: Literal (or a comma-separated list of literals) A function applied on a literal. The following functions are supported: EXTRACT , and DATE_FORMAT A combination of the 2 above The partitions that match the provided expression will be dropped. In-case a partition key is composed of multiple values - all the values must be provided to perform the drop. Y For usage examples and additional details read more here .","title":"ALTER TABLE DROP PARTITION"},{"location":"sql-reference/commands/ddl-commands/#create","text":"The CREATE command enables you to create an engine, a database, an external table , a Firebolt fact or a dimension table, view, join index , and aggregating index . GENERATE is used to populate an aggregating index once created. Read more about these topics: For engines, read more here For databases, read more here . For external tables, read more here . For external tables based on an AWS Glue table, read more here . For Firebolt tables, read more here . For views, read more here . For join index, read more here . For aggregating index, read more here .","title":"CREATE"},{"location":"sql-reference/commands/ddl-commands/#create-engine","text":"Syntax CREATE ENGINE [IF NOT EXISTS] <engine_name> [WITH <properties>] Where <properties> are: REGION = '<aws_region>' ENGINE_TYPE = <type> SPEC = '<spec>' SCALE = <scale> AUTO_STOP = <minutes> WARMUP = [ MINIMAL | PRELOAD_INDEXES | PRELOAD_ALL_DATA ] Parameters Parameter Description Mandatory? Y/N <engine_name> An identifier that specifies the name of the engine. For example: my_engine Y REGION = '<aws_region>' The AWS region in which the engine runs. If not specified, 'us-west-2' is used as default. N ENGINE_TYPE = <type> The engine type. The <type> can have one of the following values: GENERAL_PURPOSE DATA_ANALYTICS If not specified - GENERAL_PURPOSE is used as default. Usage example: CREATE ENGINE ... ENGINE_TYPE = GENERAL_PURPOSE N SPEC = '<spec>' The AWS EC2 instance type, for example, 'm5d.xlarge' . If not specified, 'i3.4xlarge' is used as default. N SCALE = <scale> Specifies the scale of the engine. The scale can be any INT between 1 to 128. If not specified, 2 is used as default. N AUTO_STOP = <minutes> Indicates the amount of time (in minutes) after which the engine automatically stops. The default value is 20. Setting the minutes to 0 indicates that AUTO_STOP is disabled. N WARMUP = <warmup_method> The warmup method that should be used, the following options are supported: MINIMAL On-demand loading (both indexes and tables' data). PRELOAD_INDEXES Load indexes only (default). PRELOAD_ALL_DATA Full data auto-load (both indexes and table data - full warmup). N Example - create an engine with (non-default) properties: CREATE ENGINE my_engine WITH SPEC = 'c5d.4xlarge' SCALE = 8","title":"CREATE ENGINE"},{"location":"sql-reference/commands/ddl-commands/#create-database","text":"Syntax CREATE DATABASE [IF NOT EXISTS] <database_name> [WITH <properties>] Where <properties> are: REGION = '<aws_region> ATTACHED_ENGINES = ( '<engine_name>' [, ... ] ) DEFAULT_ENGINE = 'engine_name' DESCRIPTION = 'description' Parameters Parameter Description Mandatory? Y/N <database_name> An identifier that specifies the name of the database. For example: my_database Y REGION = '<aws_region>' The AWS region in which the database is configured. If not specified, 'us-west-2' is being used as default. N ATTACHED_ENGINES = ( <engine_name> [ ... ] ) A list of engine names, for example: ATTACHED_ENGINES = (my_engine_1 my_engine_2) . The specified engines must be detached from any other databases first. N DEFAULT_ENGINE = engine_name An identifier that specifies the name of the default engine. If not specified, the first engine in the attached engines list will be used as default. If a default engine is specified without specifying the list of attached engines or if the default engine is not in that list, the default engine will be both attached to the database and used as the default engine. N DESCRIPTION = 'description' The engine's description (up to 64 characters). N Example - create a database with (non-default) properties: CREATE DATABASE IF NOT EXISTS my_db WITH region = 'us-east-1' description = 'Being used for testing'","title":"CREATE DATABASE"},{"location":"sql-reference/commands/ddl-commands/#create-external-table","text":"Syntax CREATE EXTERNAL TABLE [IF NOT EXISTS] <external_table_name> ( <column_name> <column_type> [, <column_name2> <column_type2> [, ...n] ] [, <partition_column_name> <partition_column_type> PARTITION('<regex>') ) [CREDENTIALS = (<awsCredentials>)] URL = 's3://<bucket>[/<folder>][/...]/' OBJECT_PATTERN = '<object_pattern>'[, '<object_pattern>'[, ...n]]] TYPE = (<type> [typeOptions]) [COMPRESSION = <compression_type>]","title":"CREATE EXTERNAL TABLE"},{"location":"sql-reference/commands/ddl-commands/#parameters","text":"Parameter Description <external_table_name> An \u200bidentifier\u200b\u200b that specifies the name of the external table. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_name> An identifier that specifies the name of the column. This name should be unique within the table. <column_type> Specifies the data type for the column. <partition_column_name> Extract partition data CREDENTIALS CREDENTIALS URL and OBJECT_PATTERN URL & OBJECT_PATTERN TYPE TYPE COMPRESSION COMPRESSION All Firebolt identifiers are case insensitive unless double-quotes are used. For more information, please see Identifier requirements .","title":"Parameters"},{"location":"sql-reference/commands/ddl-commands/#extract-partition-data","text":"When data is partitioned, the partition columns are not stored in the S3 files. Instead, they can be extracted from the file path within the bucket. \u200b Syntax [<column_name> <column_type> PARTITION('<regex>')] Parameters Parameter Description \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<partition_column_name> An identifier that specifies the name of the column as it would appear in the external table (should match the hive partition name). As with any column name, this name should be unique within the table. <partition_column_type> Specifies the data type for the column. '<regex>' The regex for extracting the partition value out of the file path in S3. Guidelines for creating the regex: You do not have to reference all the partitions in your data; you can specify only the columns that you wish to include in the external table. You can extract the column value from the file name, but not from its path. For each column, a regular expression that contains a capturing group must be specified, so Firebolt can treat the captured string as the column value. When the column data type is date , Firebolt expects three capturing groups in the order of year, month, and day. The regular expression is matched against the file path, not including the s3://bucket_name/ prefix. Firebolt tries to convert the captured string to the specified type. If the type conversion fails, the value is treated as NULL. Tip In most cases, the easiest way to build a regular expression is as follows: Count the number of folders in the path, not including the bucket name. Concatenate the string [^\\/]+\\/ according to the number of folders. Prefix the regex with an additional [^\\/]+ for the file name. Wrap the [^\\/]+ in the right folder with a capturing group parenthesis, i.e ([^\\/]+). See the examples below for both hive-compatible and non-compatible partitions extractions. Here is a good explanation about matching groups , and an online tool to test your regular expressions. Example 1 - extract hive-compatible partitions: \\ Consider the following layout of files in a bucket - data is partitioned according to client type, year, and month, with multiple parquet files in each partition. The parquet files don't contain the corresponding columns, but the columns can be extracted, along with their values, by parsing the file paths, as we will see in the next section. s3://my_bucket/c_type=xyz/year=2018/month=01/part-00001.parquet s3://my_bucket/c_type=xyz/year=2018/month=01/part-00002.parquet ... s3://my_bucket/c_type=abc/year=2018/month=01/part-00001.parquet s3://my_bucket/c_type=abc/year=2018/month=01/part-00002.parquet Creating an external table using the following format: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT, c_type TEXT PARTITION('[^\\/]+\\/c_type=([^\\/]+)\\/[^\\/]+\\/[^\\/]+') ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET) Results with an external table in the following structure: c_id c_name c_type 1 name_a xyz 2 name_b xyz ... ... ... 100 name_abc xyz Example 2 - extract non-hive compatible partitions: In some cases, your S3 files may be organized in partitions that do not use the = format. For example, consider this layout: s3://my_bucket/xyz/2018/01/part-00001.parquet s3://my_bucket/xyz/2018/01/part-00002.parquet ... s3://my_bucket/abc/2018/01/part-00001.parquet s3://my_bucket/abc/2018/01/part-00002.parquet In this case, you can use the advanced PARTITION(<regex>) column definition to create the columns and extract their values. To create the same external table as we did in the hive-compatible case, use the following command: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT, c_type TEXT PARTITION('[^\\/]+\\/([^\\/]+)\\/[^\\/]+\\/[^\\/]+') ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET) As in the previous example, the values for the columns c_id and c_name are extracted from the record in the parquet file, and the values for the c_type column is extracted from the file path, according to the specified type and regular expression. The partition values can be extracted during the INSERT INTO command as well. Read more here .","title":"Extract partition data"},{"location":"sql-reference/commands/ddl-commands/#credentials","text":"The credentials for accessing your AWS S3. Firebolt enables using either access key & secret or IAM role.","title":"CREDENTIALS"},{"location":"sql-reference/commands/ddl-commands/#syntax-authenticating-using-an-access-key-secret","text":"CREDENTIALS = (AWS_KEY_ID = '<aws_key_id>' AWS_SECRET_KEY = '<aws_secret_key>' ) Parameter Description Data type <aws_key_id> The AWS access key ID for the authorized app (Firebolt) TEXT <aws_secret_key> The AWS secret access key for the app (Firebolt) TEXT In case you don't have the access key and secret to access your S3 bucket, read more here on how to obtain them.","title":"Syntax - Authenticating using an access key &amp; secret"},{"location":"sql-reference/commands/ddl-commands/#syntax-authenticating-using-iam-role","text":"Read more on how to configure the AWS role here . CREDENTIALS = (AWS_ROLE_ARN = '<role_arn>' [AWS_ROLE_EXTERNAL_ID = '<external_id>']) Parameter Description Data type '<role_arn>' The arn_role you created in order to enable access to the required bucket. TEXT '<external_id>' Optional. This is an optional external ID that you can configure in AWS when creating the role. Specify this only if you use the external ID. TEXT","title":"Syntax - Authenticating using IAM role"},{"location":"sql-reference/commands/ddl-commands/#url-and-object_pattern","text":"The URL and OBJECT_PATTERN parameters are used together, to match the set of files from within the specified bucket that you wish to include as the data for the external table. Syntax URL = 's3://<bucket>[/<folder>][/...]/' OBJECT_PATTERN = '<object_pattern>'[, '<object_pattern>'[, ...n]]] Parameters Description Data type URL This is the URL of the specific bucket and path within the bucket where the relevant files are located (common path prefix). TEXT OBJECT_PATTERN Specify the data patterns to be found in your data source. For example, *.parquet indicates that all parquet files should be found. TEXT The following wildcards are supported: '*' matches any sequence of characters '?' matches any single character [SET] matches any single character in the specified set [!SET] matches any character, not in the specified set. Example: In the following layout of objects in a bucket, the data is partitioned according to client type, year, and month, with multiple parquet files in each partition. The examples demonstrate how choosing both URL and OBJECT_PATTERN impacts the objects that are retrieved from S3. s3://bucket/c_type=c_type=xyz/year=2018/month=01/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2018/month=01/part-00002.parquet ... s3://bucket/c_type=c_type=xyz/year=2018/month=12/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2018/month=12/part-00002.parquet ... s3://bucket/c_type=c_type=xyz/year=2019/month=01/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2019/month=01/part-00002.parquet ... s3://bucket/c_type=c_type=xyz/year=2020/month=01/part-00001.parquet s3://bucket/c_type=c_type=xyz/year=2020/month=01/part-00002.parquet ... s3://bucket/c_type=c_type=abc/year=2018/month=01/part-00001.parquet s3://bucket/c_type=c_type=abc/year=2018/month=01/part-00002.parquet ... Following are some common use cases for URL and object pattern combinations: Use cases Syntax Get all files for file type xyz URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = '*' URL = 's3://bucket/' OBJECT_PATTERN = 'c_type=xyz/*' Get one specific file: c_type=xyz/year=2018/month=01/part-00001.parquet URL = 's3://bucket/c_type=xyz/year=2018/month=01/' OBJECT_PATTERN = 'c_type=xyz/year=2018/month=01/part-00001.parquet' URL = 's3://bucket/c_type=xyz/year=2018/month=01/' OBJECT_PATTERN = '*/part-00001.parquet' As can be seen in this example, the \u200b URL \u200b is used to get only the minimal set of files (c_type files in the bucket from January 2018), and then from within those matching files, the \u200b OBJECT_PATTERN \u200b\u200b is matched against the full path of the file (without the bucket name). Get all parquet files for type xyz URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = '*.parquet' Get all client data (for all types) from 2018 and 2019 URL = 's3://bucket/' OBJECT_PATTERN = ' /year=2018/ ', ' /year=2019/ ' Get all files for type xyz, from the past 3 months URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = ' /year=2019/month=12/ ', ' /year=2020/month=01/ ', ' /year=2020/month=02/ ' No files matched, since the URL parameter does not support wildcards. URL = 's3://bucket/' OBJECT_PATTERN = ' /year=2019/month=12/ ', ' /year=2020/month=01/ ', ' /year=2020/month=02/ ' Get all files of type xyz from the first six months of 2019 URL = 's3://bucket/c_type=xyz/' OBJECT_PATTERN = '/year=2019/month=0[1-6]'","title":"URL and OBJECT_PATTERN"},{"location":"sql-reference/commands/ddl-commands/#type","text":"Specifies the type of the files in S3. The following types and type options are supported. TYPE = (CSV [SKIP_HEADER_ROWS = {1|0}]) With TYPE = (CSV SKIP_HEADER_ROWS = 1) , Firebolt assumes that the first row in each file read from S3 is a header row and skips it when ingesting data. When set to 0 , which is the default if not specified, Firebolt ingests the first row as data. * TYPE = (JSON [PARSE_AS_TEXT = {'TRUE'|'FALSE'}]) With TYPE = (JSON PARSE_AS_TEXT = 'TRUE') , Firebolt ingests each JSON object literal in its entirety into a single column of type TEXT . With TYPE = (JSON PARSE_AS_TEXT = 'FALSE') , Firebolt expects each key in a JSON object literal to map to a column in the table definition. During ingestion, Firebolt inserts the key's value into the corresponding column. * TYPE = (ORC) * TYPE = (PARQUET) * TYPE = (TSV) Example Creating an external table that reads parquet files from S3 is being done with the following command: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET)","title":"TYPE"},{"location":"sql-reference/commands/ddl-commands/#compression","text":"Specifies the compression type of the files in S3. Syntax [COMPRESSION = <compression_type>] Parameters Description <compression_type> An identifier specifies the compression type. GZIP is supported. Example The example below creates an external table to ingest parquet files from S3 that are compressed using gzip. The credentials for an IAM user with access to the bucket are provided. CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = 'AKIAIOSFODNN7EXAMPLE' AWS_SECRET_KEY = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY') URL = 's3://mybucket/' OBJECT_PATTERN= '*.parquet.gz' TYPE = (PARQUET) COMPRESSION = GZIP","title":"COMPRESSION"},{"location":"sql-reference/commands/ddl-commands/#create-external-table-based-on-an-aws-glue-table","text":"Syntax CREATE EXTERNAL TABLE [IF NOT EXISTS] <external_table_name> CREDENTIALS = ( AWS_KEY_ID = '<aws_key_id>' AWS_SECRET_KEY = '<aws_secret_key>' ) META_STORE = (TYPE='Glue' DATABASE_NAME=<db_name> TABLE_NAME=<table_name>) In order to access Glue, make sure to allow the following actions in the AWS Policy: \"s3:GetObject\" \"s3:GetObjectVersion\" \"s3:GetBucketLocation\" \"s3:ListBucket\" \"glue:GetTables\" Click here to download a templated policy you can use. Make sure to replace: <bucket> and <prefix> with the actual AWS S3 bucket name path and prefix where the AWS Glue data is stored. <db_name> with the name of the AWS Glue database. Parameter Description Data type <external_table_name> An \u200bidentifier\u200b\u200b that specifies the name of the external table. This name should be unique within the database. <db_name> The name of the database in AWS Glue TEXT <table_name> The name of the table in AWS Glue TEXT '<aws_key_id>' The AWS access key ID for the authorized app (Firebolt) TEXT '<aws_secret_key>' The AWS secret access key for the app (Firebolt) TEXT Example An external table based on an AWS Glue table 'glue_table' in 'glue_db' database: CREATE EXTERNAL TABLE IF NOT EXISTS my_external_table CREDENTIALS = ( AWS_KEY_ID = 'AKIAIOSFODNN7EXAMPLE' AWS_SECRET_KEY = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' ) META_STORE = (TYPE='Glue' DATABASE_NAME='glue_db' TABLE_NAME='glue_table')","title":"CREATE EXTERNAL TABLE based on an AWS Glue table"},{"location":"sql-reference/commands/ddl-commands/#create-fact-dimension-table","text":"Creates a new FACT/DIMENSION table in the current database. Firebolt also supports creating a table as select (also referred to as CTAS) - read more here . Fact table syntax CREATE FACT TABLE [IF NOT EXISTS] <table_name> ( <column_name> <column_type> [constraints] [, <column_name2> <column_type2> [constraints] [, ...n]] ) PRIMARY INDEX <column_name>[, <column_name>[, ...n]] [PARTITION BY <column_name>[, <column_name>[, ...n]]] Partitions are only supported on FACT tables. Dimension table syntax CREATE DIMENSION TABLE [IF NOT EXISTS] <table_name> ( <column_name> <column_type> [constraints] [, <column_name2> <column_type2> [constraints] [, ...n]] ) [PRIMARY INDEX <column_name>[, <column_name>[, ...n]]] Parameter Description <table_name> An \u200bidentifier\u200b\u200b that specifies the name of the table. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_name> An identifier that specifies the name of the column. This name should be unique within the table. <column_type> Specifies the data type for the column. All identifiers are case insensitive unless double-quotes are used. For more information, please see our identifier requirements page .","title":"CREATE FACT / DIMENSION TABLE"},{"location":"sql-reference/commands/ddl-commands/#read-more-on","text":"Column constraints & default expression PRIMARY INDEX specifier PARTITION BY specifier","title":"Read more on"},{"location":"sql-reference/commands/ddl-commands/#column-constraints-default-expression","text":"Syntax <column_name> <column_type> [UNIQUE] [NULL|NOT NULL] [DEFAULT <expr>] Firebolt supports the following column constraints: Constraint Description Default value DEFAULT <expr> Determines the default value that is used instead of NULL value is inserted. NULL | NOT NULL Determines if the column may or may not contain NULLs. NOT NULL UNIQUE This is an optimization hint to tell Firebolt that this column will be queried for unique values, such as through a COUNT(DISTINCT) function. This will not raise an error if a non-unique value is added to the column. Note that nullable columns can not be used in Firebolt indexes (Primary, Aggregating, or Join indexes). Example - Creating a table with nulls and not nulls: This example illustrates different use cases for column definitions and INSERT statements: Explicit NULL insert: a direct insertion of a NULL value into a particular column. Implicit insert: an INSERT statement with missing values for a particular column. The example uses a fact table in which to insert different values. First, we create the fact table as follows: CREATE FACT TABLE t1 ( col1 INT NULL , col2 INT NOT NULL UNIQUE, col3 INT NULL DEFAULT 1, col4 INT NOT NULL DEFAULT 1, col5 TEXT ) PRIMARY INDEX col2; Once we've created the table, we can manipulate the values with different INSERT statements. Following are detailed descriptions of different examples of these: INSERT statement Results and explanation INSERT INTO t1 VALUES (1,1,1,1,1) 1 is inserted into each column INSERT INTO t1 VALUES (NULL,1,1,1,1) col1 is NULL , and this is an explicit NULL insert, so NULL is inserted successfully. INSERT INTO t1 (col2,col3,col4,col5) VALUES (1,1,1,1) This is an example of explicit and implicit INSERT statements. col1 is NULL, which is an implicit insert, as a default expression was not specified. In this case, col1 is treated as NULL DEFAULT NULL, so Firebolt inserts NULL. INSERT INTO t1 VALUES (1,NULL,1,1,1) INSERT INTO t1 (col1,col3,col4,col5) VALUES (1,1,1,1) The behavior here depends on the column type. For both cases, a \u201cnull mismatch\u201d event occurs. In the original table creation, col2 receives a NOT NULL value. Since a default expression is not specified, both of these INSERT statements try to insert NOT NULL DEFAULT NULL into col2. This means that there is an implicit attempt to insert NULL in both cases. In this particular case, the data type for col4 is INT. Because NOT NULL is configured on col4 as well, it cannot accept NULL values. If the data type for col4 was TEXT, for example, the result would have been an insert of '' . INSERT INTO t1 VALUES (1,1,NULL,1,1) col3 is NULL DEFAULT 1, and this is an explicit insert. NULL is inserted INSERT INTO t1 (col1,col2,col4,col5) VALUES (1,1,1,1) col3 is NULL DEFAULT 1 . This is an implicit insert, and a default expression is specified, so 1 is inserted INSERT INTO t1 VALUES (1,1,1,NULL,1) col4 is NOT NULL DEFAULT 1 , and this is an explicit insert. Therefore, a \u201cnull mismatch\u201d event occurs. In this particular case, since the data type for col4 is INT, the result is an error. If the data type for col4 was TEXT, for example, the result would have been an insert of '' . INSERT INTO t1 (col1,col2,col3,col5) VALUES (1,1,1,1) col4 is NOT NULL DEFAULT 1 , and this is an implicit insert. Therefore, the default expression is used, and 1 is inserted INSERT INTO t1 VALUES (1,1,1,1,NULL) INSERT INTO t1 (col1,col2,col3,col4) VALUES (1,1,1,1) The nullability and default expression for col5 were not specified. In this case, Firebolt treats col5 as NOT NULL DEFAULT NULL . For the explicit insert, Firebolt attempts to insert NULL into a NOT NULL int column, and a \u201cnull mismatch\u201d event results. For the implicit insert, Firebolt resorts to the default, and again, attempts to insert NULL. Similar to the explicit NULL case - an empty value '' is inserted.","title":"Column constraints &amp; default expression"},{"location":"sql-reference/commands/ddl-commands/#primary-index","text":"The PRIMARY INDEX is a sparse index containing sorted data based on the indexed field. This index clusters and sorts data as it is ingested, without affecting data scan performance. Notice The PRIMARY INDEX is mandatory for FACT tables and optional for DIMENSION tables. Primary index syntax PRIMARY INDEX <column_name>[, <column_name>[, ...n]] The following table describes the primary index parameters: Parameter. Description Mandatory? <column_name> Specifies the name of the column in the Firebolt table which composes the index. At least one column must be used for configuring the index. Y","title":"PRIMARY INDEX"},{"location":"sql-reference/commands/ddl-commands/#partition-by","text":"The PARTITION BY specifier contains the definition of the columns by which the table will be split into physical parts. Those columns are considered to be the partition key of the table. The key can be any of the table columns which is not nullable. Only FACT tables can be partitioned. When the partition key is set with multiple columns, all will be used as the partition boundaries. Partition syntax PARTITION BY <column_name>[, <column_name>[, ...n]] Read more on how to work with partitions here .","title":"PARTITION BY"},{"location":"sql-reference/commands/ddl-commands/#ctas-create-fact-dimension-table-as-select","text":"Creates a table and loads data into it based on the SELECT query. The table column names and types are automatically inferred based on the output columns of the SELECT . When specifying explicit column names those override the column names inferred from the SELECT . CREATE FACT TABLE AS SELECT syntax CREATE FACT TABLE <table_name> [(<column_name>[, ...n] )] PRIMARY INDEX <column_name>[, <column_name>[, ...n]] [PARTITION BY <column_name>[, <column_name>[, ...n]]] AS <select_query> CREATE DIMENSION TABLE AS SELECT syntax CREATE DIMENSION TABLE <table_name> [(<column_name>[, ...n] )] [PRIMARY INDEX <column_name>[, <column_name>[, ...n]]] AS <select_query>","title":"CTAS - CREATE FACT / DIMENSION TABLE AS SELECT"},{"location":"sql-reference/commands/ddl-commands/#general-parameters","text":"Parameter Description <table_name> An \u200bidentifier\u200b\u200b that specifies the name of the external table. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_name> An identifier that specifies the name of the column. This name should be unique within the table. <select_query > Any valid select query Read more on the primary index specifier here .","title":"General parameters"},{"location":"sql-reference/commands/ddl-commands/#create-view","text":"Views allow you to use a query as if it were a table. Views are useful to filter, focus and simplify a database for users. They provide a level of abstraction that can make subqueries easier to write, especially for commonly referenced subsets of data. A view in Firebolt executes its query each time the view is referenced. In other words, the view results are not stored for future usage, and therefore using views does not provide a performance advantage. Syntax CREATE VIEW [IF NOT EXISTS] <name> [(<column_list>)] AS SELECT <select_statement>","title":"CREATE VIEW"},{"location":"sql-reference/commands/ddl-commands/#parameters_1","text":"Parameter Description <name> An \u200bidentifier\u200b\u200b that specifies the name of the view. This name should be unique within the database. \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<column_list> An optional list of column names to be used for columns of the view. If not given, the column names are deduced from the query. <select_statement> The select statement for creating the view Example CREATE VIEW fob_shipments AS SELECT l_shipmode, l_shipdate, l_linestatus, l_orderkey, FROM lineitem WHERE l_shipdate > '1990-01-01' AND l_shipmode = 'FOB'","title":"Parameters"},{"location":"sql-reference/commands/ddl-commands/#create-join-index","text":"Join indexes can accelerate queries that use JOIN operations on dimension tables. Under certain circumstances, a join index can significantly reduce the compute requirements required to perform a join at query runtime. Syntax CREATE JOIN INDEX [IF NOT EXISTS] <unique_join_index_name> ON <dimension_table_name> ( <unique_join_key_column>, <dimension_column>[, ...n] ) General parameters Parameter Description <unique_join_index_name> A unique name for the join index. <dimension_table_name> The name of the dimension table on which the index is configured. <unique_join_key_column> The column name that is used in the join\u2019s ON clause. <dimension_column> The column name which is being loaded into memory from the dimension table. More than one column can be specified. For better performance, whenever possible, use the UNIQUE column attribute in the dimension table definition for the column that is used as the join key in queries. Since the join index is loaded into engine RAM, make sure to choose only the subset of dimension table columns that appear in queries that use the join. Example: Create join index with specific columns In the following example, we create a join index on the dimension table my_dim , and store the columns email and country in the index: CREATE DIMENSION TABLE my_dim( my_dim_id BIGINT UNIQUE, email TEXT, country TEXT, city TEXT, cellolar TEXT) PRIMARY INDEX my_dim_id; CREATE JOIN INDEX my_dim_join_idx ON my_dim (my_dim_id, email, country);","title":"CREATE JOIN INDEX"},{"location":"sql-reference/commands/ddl-commands/#create-aggregating-index","text":"Creating an aggregating index can be done as follows: For an empty table - use the following syntax . For a table already populated with data - use the following syntax .","title":"CREATE AGGREGATING INDEX"},{"location":"sql-reference/commands/ddl-commands/#syntax-for-aggregating-index-on-an-empty-table","text":"CREATE AGGREGATING INDEX <agg_index_name> ON <fact_table_name> ( <key_column>[, ...n], <aggregation>[, ...n] ); Click here to read about the different parameters. The index is being populated automatically as data is being loaded to the table.","title":"Syntax for aggregating index on an empty table"},{"location":"sql-reference/commands/ddl-commands/#syntax-for-aggregating-index-on-a-populated-table","text":"CREATE AND GENERATE AGGREGATING INDEX <agg_index_name> ON <fact_table_name> ( <key_column>[, ...n], <aggregation>[, ...n] ); Generating the index after data was loaded to the table is a memory-heavy operation.","title":"Syntax for aggregating index on a populated table"},{"location":"sql-reference/commands/ddl-commands/#parameters_2","text":"Parameter Description <agg_index_name> Specifies a unique name for the index <fact_table_name> Specifies the name of the fact table referenced by this index <key_column> Specifies column name from the <fact_table_name> used for the index <aggregation> Specifies one or more aggregation functions to be applied on a <key_column> , such as SUM , COUNT , AVG , and more. Example: Create an aggregating index In the following example, we create an aggregating index on the fact table my_fact , to be used in the following query: SELECT product_name, count(DISTINCT source), sum(amount) FROM my_fact GROUP BY product_name; The aggregating index is being created as follows: CREATE AGGREGATING INDEX my_fact_agg_idx ON my_fact ( product_name, count(distinct source), sum(amount) ); In order to enjoy the performance boost provided by the index - all the columns/measurements in the query should exists in the aggregating index.","title":"Parameters"},{"location":"sql-reference/commands/ddl-commands/#start-engine","text":"The START ENGINE command enables you to start a stopped engine. Syntax START ENGINE <engine_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to be started Y","title":"START ENGINE"},{"location":"sql-reference/commands/ddl-commands/#stop-engine","text":"The STOP ENGINE command enables you to stop a running engine. Syntax STOP ENGINE <engine_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to be stopped Y","title":"STOP ENGINE"},{"location":"sql-reference/commands/ddl-commands/#attach-engine","text":"The ATTACH ENGINE command enables you to attach an engine to a database. Syntax ATTACH ENGINE <engine_name> TO <database_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to attach. Y <database_name> The name of the database to attach engine <engine_name> to. Y","title":"ATTACH ENGINE"},{"location":"sql-reference/commands/ddl-commands/#detach-engine-deprecated","text":"Deprecated. Avoid using this statement and use DROP ENGINE instead. Allows you to detach an engine from a database. Syntax DETACH ENGINE <engine_name> FROM <database_name> Parameter Description Mandatory? Y/N <engine_name> The name of the engine to detach. Y <database_name> The name of the database to detach engine <engine_name> from. Y","title":"DETACH ENGINE (deprecated)"},{"location":"sql-reference/commands/ddl-commands/#describe","text":"Lists all columns and data types for the table. Once the results are displayed, you can also export them to CSV or JSON. Syntax DESCRIBE <table_name> Example The following lists all columns and data types for the table named prices : DESCRIBE prices Returns: +------------+-------------+-----------+----------+ | table_name | column_name | data_type | nullable | +------------+-------------+-----------+----------+ | prices | item | text | 0 | | prices | num | int | 0 | +------------+-------------+-----------+----------+","title":"DESCRIBE"},{"location":"sql-reference/commands/ddl-commands/#drop","text":"DROP drops or removes the specified object from Firebolt. Firebolt supports dropping a table and view. Read more on these topics: For dropping an engine, read here . For dropping an index, read here . For dropping a table, read here . For dropping a database, read here For dropping a view, read here .","title":"DROP"},{"location":"sql-reference/commands/ddl-commands/#drop-engine","text":"Syntax DROP ENGINE [IF EXISTS] <engine_name> Parameter Description <engine_name> The name of the engine to be deleted.","title":"DROP ENGINE"},{"location":"sql-reference/commands/ddl-commands/#drop-index","text":"Syntax DROP [AGGREGATING | JOIN] INDEX [IF EXISTS] <index_name> Parameter Description <index_name> The name of the index to be deleted.","title":"DROP INDEX"},{"location":"sql-reference/commands/ddl-commands/#drop-table","text":"Syntax DROP TABLE [IF EXISTS] <table_name> Parameter Description <table_name> The name of the table to be deleted. Notice For external tables, this means that the table is removed from Firebolt, but not from the source.","title":"DROP TABLE"},{"location":"sql-reference/commands/ddl-commands/#drop-database","text":"Syntax Deletes the database and all of its tables and attached engines. DROP DATABASE [IF EXISTS] <database_name> Parameter Description <database_name> The name of the database to be deleted","title":"DROP DATABASE"},{"location":"sql-reference/commands/ddl-commands/#drop-view","text":"Syntax DROP VIEW [IF EXISTS] <view_name> Parameter Description <view_name> The name of the view to be deleted.","title":"DROP VIEW"},{"location":"sql-reference/commands/ddl-commands/#show","text":"SHOW can list several objects and their details.","title":"SHOW"},{"location":"sql-reference/commands/ddl-commands/#show-cache","text":"Returns the current SSD usage ( ssd_usage ) for the current engine. SHOW CACHE returns values at the engine level, not by each node. Syntax SHOW CACHE; The results of SHOW CACHE are formatted as follows: <ssd_used> / <ssd _ available> GB ( <percent_utilization> %) These components are defined as follows: Component Description <ssd_used> The amount of storage currently used on your engine. This data includes storage that Firebolt reserves for internal usage. <ssd _ available> The amount of available storage on your engine. <percent_utilization> The percent of used storage as compared to available storage. Example returned output is shown below. | ssd_usage | +-----------------------+ | 3.82/73.28 GB (5.22%) |","title":"SHOW CACHE"},{"location":"sql-reference/commands/ddl-commands/#show-columns","text":"Lists columns and their properties for a specified table. Returns <table_name> , <column_name> , <data_type> , and nullable for each column. Syntax SHOW COLUMNS <table_name>; Parameter Description <table_name> The name of the table to be analyzed. Example SHOW COLUMNS prices; Returns : ------------+-------------+-----------+----------+ | table_name | column_name | data_type | nullable | +------------+-------------+-----------+----------+ | prices | item | text | 0 | | prices | num | int | 0 | +------------+-------------+-----------+----------+","title":"SHOW COLUMNS"},{"location":"sql-reference/commands/ddl-commands/#show-databases","text":"Lists databases in the current Firebolt account. Returns name , region , attached_engines , created_on , created_by , and errors for each database. Syntax SHOW DATABASES; Example SHOW DATABASES; Returns : +---------------+-----------+-------------------------------------+-----------------------------+---------------+--------+ | database_name | region | attached_engines | created_on | created_by | errors | +---------------+-----------+-------------------------------------+-----------------------------+---------------+--------+ | Tutorial1 | us-east-1 | Tutorial1_general_purpose (default) | 2021-09-30T21:25:45.401405Z | someone | - | +---------------+-----------+-------------------------------------+-----------------------------+---------------+--------+","title":"SHOW DATABASES"},{"location":"sql-reference/commands/ddl-commands/#show-database","text":"Shows the status for the specified database. These are the same metadata fields as SHOW DATABASES . SHOW DATABASE <database_name>; Parameter Description < database_name> The name of the database to be analyzed.","title":"SHOW DATABASE"},{"location":"sql-reference/commands/ddl-commands/#show-engines","text":"Lists all engines in the current Firebolt account. Returns engine_name , region , spec , scale , status , and attached_to for each engine. Syntax SHOW ENGINES; Example SHOW ENGINES; Returns : +--------------------+-----------+-------------+-------+---------+-------------+ | engine_name | region | spec | scale | status | attached_to | +--------------------+-----------+-------------+-------+---------+-------------+ | Tutorial_analytics | us-east-1 | r5d.4xlarge | 2 | Stopped | Tutorial | +--------------------+-----------+-------------+-------+---------+-------------+","title":"SHOW ENGINES"},{"location":"sql-reference/commands/ddl-commands/#show-indexes","text":"Lists all indexes defined in the current database. Returns index_name , table_name , type (primary, aggregating, or join), the index expression , and the size_compressed . Syntax SHOW INDEXES; Example SHOW INDEXES; Returns: +------------------------+----------------+-------------+----------------------------------------------------------------------------------------------------------+-----------------+-------------------+-------------------+--------------------+ | index_name | table_name | type | expression | size_compressed | size_uncompressed | compression_ratio | number_of_segments | +------------------------+----------------+-------------+----------------------------------------------------------------------------------------------------------+-----------------+-------------------+-------------------+--------------------+ | primary_lineitem | lineitem | primary | [\"l_orderkey\",\"l_linenumber\"] | N/A | N/A | N/A | N/A | | primary_partition_test | partition_test | primary | [\"store_id\",\"product_id\"] | N/A | N/A | N/A | N/A | | agg_lineitem | lineitem | aggregating | [\"\\\"l_suppkey\\\"\",\"\\\"l_partkey\\\"\",\"SUM(\\\"l_quantity\\\")\",\"SUM(\\\"l_extendedprice\\\")\",\"AVG(\\\"l_discount\\\")\"] | N/A | N/A | N/A | 8 | +------------------------+----------------+-------------+----------------------------------------------------------------------------------------------------------+-----------------+-------------------+-------------------+--------------------+","title":"SHOW INDEXES"},{"location":"sql-reference/commands/ddl-commands/#show-tables","text":"Lists all tables defined in the current database. Returns table_name , state , table_type , column_count , primary_index , and schema (the CREATE [EXTERNAL|FACT|DIMENSION] TABLE statement for the table). Syntax SHOW TABLES; Example SHOW TABLES; Returns : +----------------+-------+------------+--------------+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------+-------------------+-------------------+--------------------+ | table_name | state | table_type | column_count | primary_index | schema | number_of_rows | size | size_uncompressed | compression_ratio | number_of_segments | +----------------+-------+------------+--------------+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------+-------------------+-------------------+--------------------+ | ex_lineitem | Valid | EXTERNAL | 16 | N/A | CREATE EXTERNAL TABLE IF NOT EXISTS \"ex_lineitem\" (\"l_orderkey\" long NOT NULL, \"l_partkey\" long NOT NULL, \"l_suppkey\" long NOT NULL, \"l_linenumber\" int NOT NULL, \"l_quantity\" long NOT NULL, \"l_extendedprice\" long NOT NULL, \"l_discount\" long NOT NULL, \"l_tax\" long NOT NULL, \"l_returnflag\" text NOT NULL, \"l_linestatus\" text NOT NULL, \"l_shipdate\" text NOT NULL, \"l_commitdate\" text NOT NULL, \"l_receiptdate\" text NOT NULL, \"l_shipinstruct\" text NOT NULL, \"l_shipmode\" text NOT NULL, \"l_comment\" text NOT NULL) \"URL\" = 's3://firebolt-publishing-public/samples/tpc-h/parquet/lineitem/' \"OBJECT_PATTERN\" = '*.parquet' \"TYPE\" = (\"PARQUET\") | N/A | N/A | N/A | N/A | N/A | | lineitem | Valid | FACT | 16 | [l_orderkey, l_linenumber] | CREATE FACT TABLE IF NOT EXISTS \"lineitem\" (\"l_orderkey\" long NOT NULL, \"l_partkey\" long NOT NULL, \"l_suppkey\" long NOT NULL, \"l_linenumber\" int NOT NULL, \"l_quantity\" long NOT NULL, \"l_extendedprice\" long NOT NULL, \"l_discount\" long NOT NULL, \"l_tax\" long NOT NULL, \"l_returnflag\" text NOT NULL, \"l_linestatus\" text NOT NULL, \"l_shipdate\" text NOT NULL, \"l_commitdate\" text NOT NULL, \"l_receiptdate\" text NOT NULL, \"l_shipinstruct\" text NOT NULL, \"l_shipmode\" text NOT NULL, \"l_comment\" text NOT NULL) PRIMARY INDEX \"l_orderkey\", \"l_linenumber\" | N/A | N/A | N/A | N/A | 6 | | partition_test | Valid | FACT | 5 | [store_id, product_id] | CREATE FACT TABLE IF NOT EXISTS \"partition_test\" (\"transaction_id\" long NOT NULL, \"transaction_date\" timestamp NOT NULL, \"store_id\" int NOT NULL, \"product_id\" int NOT NULL, \"units_sold\" int NOT NULL) PRIMARY INDEX \"store_id\", \"product_id\" PARTITION BY EXTRACT(YEAR FROM \"transaction_date\") | N/A | N/A | N/A | N/A | N/A | | Insert_test | Valid | DIMENSION | 3 | N/A | CREATE DIMENSION TABLE IF NOT EXISTS \"Insert_test\" (\"name\" long NOT NULL, \"number\" int NOT NULL, \"other_name\" long NOT NULL) | N/A | N/A | N/A | N/A | 2 | +----------------+-------+------------+--------------+----------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------+-------------------+-------------------+--------------------+","title":"SHOW TABLES"},{"location":"sql-reference/commands/dml-commands/","text":"DML commands INSERT INTO Inserts one or more values into a specified table. Specifying column names is optional. The INSERT INTO operation is not atomic. If the operation is interrupted, partial data ingestion may occur. Syntax INSERT INTO <table_name> [(<col1>[, <col2>][, ...])] { <select_statement> | VALUES ([<val1>[, <val2>][, ...]) } Parameter Description <table_name> The target table where values are to be inserted. (<col1>[, <col2>][, ...])] A list of column names from <table_name> for the insertion. If not defined, the columns are deduced from the <select_statement> . <select_statement> --OR-- VALUES ([<val1>[, <val2>][, ...])] You can specify either a SELECT query that determines values to or an explicit list of VALUES to insert. Example: extracting partition values using the INSERT INTO command In some cases, your files, stored on AWS S3, may be organized in partitions. Assume we have files with the following paths in S3: s3://my_bucket/xyz/2018/01/part-00001.parquet s3://my_bucket/xyz/2018/01/part-00002.parquet ... s3://my_bucket/abc/2018/01/part-00001.parquet s3://my_bucket/abc/2018/01/part-00002.parquet Our goal is to extract a value called c_type which is stored in the 2nd path segment of each file path. For example, the following file in S3: s3://my_bucket/xyz/2018/01/part-00001.parquet Has the following c_type value: xyz Assume we have an external table that was created using the following DDL: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET) And, also, a FACT table was created using the following DDL which we want to ingest data into: CREATE FACT TABLE my_table ( c_id INT, c_name TEXT, c_type TEXT ) PRIMARY INDEX c_id We are going to extract the data for c_type from a metadata column in our external table called source_file_name . We can accomplish this by using the following INSERT INTO query: INSERT INTO my_table (c_id, c_name, c_type) SELECT c_id, c_name, SPLIT_PART(source_file_name, '/', 1) AS c_type FROM my_external_table We have specified 1 in the SPLIT_PART function since the bucket name is not included in the source_file_name. Checking and validating INSERT INTO status If an INSERT INTO statement fails or a client is disconnected, Firebolt might ingest an incomplete data set into the target table. Here are some steps you can use to determine the status of an ingestion operation and ensure it ran successfully. This may come in handy if you suspect your statement failed for any reason. Before running any queries, first check to make sure that the INSERT INTO query has completed. This can be done by viewing the information schema through the running_queries view: SELECT * FROM catalog.running_queries You should see results similar to this: If you see your INSERT INTO statement under the QUERY_TEXT column, then that means the operation is still running. You should wait for it to finish before attempting any other steps. If the INSERT INTO statement has completed, check that the external table and the target fact or dimension table are equal. ** The SQL statement below counts the rows in an external table my_extable and a fact or dimension table, fact_or_dim_table . It compares the results and returns that result of the comparison as CountResult . A CountResult of 1 is true, indicating the row count is equal. A value of 0 is false, indicating the row count is not equal and ingestion may be incomplete. SELECT (SELECT COUNT(*) FROM my_extable)=(SELECT COUNT(*) FROM fact_or_dim_table) AS CountResult; This statement assumes the fact or dimension table ingested all of the rows from the external table, and not a partial subset. If the counts are not equal, DROP the incomplete target table, create a new target table, change the INSERT INTO query to use the new target, and then run the query again.","title":"DML commands"},{"location":"sql-reference/commands/dml-commands/#dml-commands","text":"","title":"DML commands"},{"location":"sql-reference/commands/dml-commands/#insert-into","text":"Inserts one or more values into a specified table. Specifying column names is optional. The INSERT INTO operation is not atomic. If the operation is interrupted, partial data ingestion may occur. Syntax INSERT INTO <table_name> [(<col1>[, <col2>][, ...])] { <select_statement> | VALUES ([<val1>[, <val2>][, ...]) } Parameter Description <table_name> The target table where values are to be inserted. (<col1>[, <col2>][, ...])] A list of column names from <table_name> for the insertion. If not defined, the columns are deduced from the <select_statement> . <select_statement> --OR-- VALUES ([<val1>[, <val2>][, ...])] You can specify either a SELECT query that determines values to or an explicit list of VALUES to insert. Example: extracting partition values using the INSERT INTO command In some cases, your files, stored on AWS S3, may be organized in partitions. Assume we have files with the following paths in S3: s3://my_bucket/xyz/2018/01/part-00001.parquet s3://my_bucket/xyz/2018/01/part-00002.parquet ... s3://my_bucket/abc/2018/01/part-00001.parquet s3://my_bucket/abc/2018/01/part-00002.parquet Our goal is to extract a value called c_type which is stored in the 2nd path segment of each file path. For example, the following file in S3: s3://my_bucket/xyz/2018/01/part-00001.parquet Has the following c_type value: xyz Assume we have an external table that was created using the following DDL: CREATE EXTERNAL TABLE my_external_table ( c_id INT, c_name TEXT ) CREDENTIALS = (AWS_KEY_ID = '*****' AWS_SECRET_KEY = '******') URL = 's3://my_bucket/' OBJECT_PATTERN= '*.parquet' TYPE = (PARQUET) And, also, a FACT table was created using the following DDL which we want to ingest data into: CREATE FACT TABLE my_table ( c_id INT, c_name TEXT, c_type TEXT ) PRIMARY INDEX c_id We are going to extract the data for c_type from a metadata column in our external table called source_file_name . We can accomplish this by using the following INSERT INTO query: INSERT INTO my_table (c_id, c_name, c_type) SELECT c_id, c_name, SPLIT_PART(source_file_name, '/', 1) AS c_type FROM my_external_table We have specified 1 in the SPLIT_PART function since the bucket name is not included in the source_file_name.","title":"INSERT INTO"},{"location":"sql-reference/commands/dml-commands/#checking-and-validating-insert-into-status","text":"If an INSERT INTO statement fails or a client is disconnected, Firebolt might ingest an incomplete data set into the target table. Here are some steps you can use to determine the status of an ingestion operation and ensure it ran successfully. This may come in handy if you suspect your statement failed for any reason. Before running any queries, first check to make sure that the INSERT INTO query has completed. This can be done by viewing the information schema through the running_queries view: SELECT * FROM catalog.running_queries You should see results similar to this: If you see your INSERT INTO statement under the QUERY_TEXT column, then that means the operation is still running. You should wait for it to finish before attempting any other steps. If the INSERT INTO statement has completed, check that the external table and the target fact or dimension table are equal. ** The SQL statement below counts the rows in an external table my_extable and a fact or dimension table, fact_or_dim_table . It compares the results and returns that result of the comparison as CountResult . A CountResult of 1 is true, indicating the row count is equal. A value of 0 is false, indicating the row count is not equal and ingestion may be incomplete. SELECT (SELECT COUNT(*) FROM my_extable)=(SELECT COUNT(*) FROM fact_or_dim_table) AS CountResult; This statement assumes the fact or dimension table ingested all of the rows from the external table, and not a partial subset. If the counts are not equal, DROP the incomplete target table, create a new target table, change the INSERT INTO query to use the new target, and then run the query again.","title":"Checking and validating INSERT INTO status"},{"location":"sql-reference/commands/identifier-requirements/","text":"Identifier Requirements Firebolt evaluates unquoted identifiers such as table and column names entirely in lowercase. Therefore the following queries: SELECT my_column FROM my_table SELECT MY_COLUMN FROM MY_TABLE SELECT mY_cOlUmn FROM mY_tAbLe are all equivalent to: SELECT my_column FROM my_table Uppercase identifiers can be retained by enclosing in double-quotes. For example, all of these identifiers are unique: \"COLUMN_NAME\" \"column_name\" \"CoLuMn_NaMe\" Unquoted identifiers in some early Firebolt accounts may be case sensitive. Each identifier name must be at least 1 character long at minimum and at most 255 characters maximum. Firebolt identifiers can refer to the following items: Columns Tables Indexes Databases Views Engines","title":"Identifier Requirements"},{"location":"sql-reference/commands/identifier-requirements/#identifier-requirements","text":"Firebolt evaluates unquoted identifiers such as table and column names entirely in lowercase. Therefore the following queries: SELECT my_column FROM my_table SELECT MY_COLUMN FROM MY_TABLE SELECT mY_cOlUmn FROM mY_tAbLe are all equivalent to: SELECT my_column FROM my_table Uppercase identifiers can be retained by enclosing in double-quotes. For example, all of these identifiers are unique: \"COLUMN_NAME\" \"column_name\" \"CoLuMn_NaMe\" Unquoted identifiers in some early Firebolt accounts may be case sensitive. Each identifier name must be at least 1 character long at minimum and at most 255 characters maximum. Firebolt identifiers can refer to the following items: Columns Tables Indexes Databases Views Engines","title":"Identifier Requirements"},{"location":"sql-reference/commands/operators/","text":"Operators This section describes the operators supported in Firebolt. Arithmetic Notice Precision means that representation of a number is guaranteed to be accurate up to X number digits. In Firebolt, calculations are 6 digits accurate for FLOAT numbers and 15 for DOUBLE PRECISION . This means that calculations have a precision of 6 or 15 respectively and numbers that are truncated. For example: \\ If a number is stored as 1.234567 it is automatically truncated to 1.23456 for FLOAT . When performing arithmetic, the number of leading digits in the output is the product of the leading digits in both inputs. This means that if either or both of the input numbers are larger than 6, then those numbers are first truncated and then the arithmetic is performed. Operator Operator description Example Result + addition SELECT 2 + 3; 5 - subtraction SELECT 2 - 3; -1 * multiplication SELECT 2 * 3; 6 / division (integer division truncates the result) SELECT 4 / 2; 2 % modulo (remainder) SELECT 5 % 4; 1 ^ exponentiation SELECT 2.0 ^ 3.0; 8 Comparison Comparison operators are usually implemented with WHERE clauses. SELECT * FROM Table_Name WHERE Price >= Number; Operator Example Explanation = a=b a is equal to b. <= a<=b a is less than or equal to b. != a!=b a is not equal to b. <> a<>b a is not equal to b. > a>b a is greater than b. >= a>=b a is greater than or equal to b. < a<b a is less than b. Comparison operators are typically used in the WHERE clause of a query. Strings To concatenate strings, you can use the CONCAT function. SELECT concat('This', ' is', ' a', ' parenthetical', 'concantenation.') AS Concatenated_String Alternatively, you can use the double pipe || operator. SELECT 'This' || ' is' || ' a' || ' double pipe' || ' concantenation.' AS Concatenated_String Boolean Boolean operators return the result of a Boolean operation between one or more expressions. Operator Example Explanation AND x AND y True if both x and y are true NOT NOT x True if x is false X OR Y x OR y True if either x or y is true Date and time Interval Use the interval operator to add or subtract a period of time to/from a DATE , TIME , or TIMESTAMP . Syntax { +|- } INTERVAL '<quantity> [ <date_unit> ] [ ...]' Component Description <quantity> An integer. Multiple <quantities> and <date_units> can be used in the same INTERVAL command if they are separated by spaces. <date_unit> A date measurement including any of the following: millennium , century , decade , year , month , week , day , hour , minute , second , millisecond , microsecond or their plural forms. If unspecified, <date_unit> defaults to second . Usage example <date_column> + INTERVAL '1 year 2 months 3 days' <date_column> - INTERVAL '2 weeks' <date_column> - INTERVAL '1 year 3 hours 20 minutes' Cast operator Values can be converted from one data type to another by using the CAST function or the :: operator. Syntax -- CAST function CAST(<value> AS <type>) -- :: operator <value>::<type> Component Description <value> The value to convert or an expression that results in a value to convert. Can be a column name, \u200b \u200ba function applied to a column or another function, or a literal value. <type> The target data type (case-insensitive). Usage example SELECT '2021-12-31'::DATE; SELECT 8.5::FLOAT; SELECT col_a::BIGINT; Subquery Subqueries are queries contained within other queries. They are typically used to return entries based on the existence or absence of a condition, as part of a WHERE clause. This section describes subquery operators supported in Firebolt. Operator Explanation EXISTS The EXISTS operator is used to check for the existence of any record in a subquery. It returns TRUE if the subquery returns one or more records. NOT EXISTS The NOT EXISTS operator returns TRUE if the underlying subquery returns no record. IN The IN operator is used to check whether a value matches any value in a list. NOT IN Retrieve all entries from the value list that don't match the required value. Example: Using the EXISTS operator, find all suppliers with products equal to the price of 22 SELECT supplier_name FROM suppliers WHERE EXISTS (SELECT product_name FROM products WHERE products.supplier_id = suppliers.supplier_id AND price < 22); Example: Using the IN operator, return all the customers from Mannheim or London SELECT customer_name FROM customers WHERE customer_address in ('Mannheim','London'); Example: Using correlated subquery to retrieve all the products that cost more than the avg(price) SELECT product_id, product_name, list_price FROM products p WHERE list_price > ( SELECT AVG( list_price ) FROM products WHERE category_id = p.category_id ); Example: Using scalar boolean subquery to retrieve rows based on true/false condition SELECT * FROM products WHERE (SELECT CASE WHEN min(list_price) > 100 THEN true ELSE false END FROM products);","title":"Operators"},{"location":"sql-reference/commands/operators/#operators","text":"This section describes the operators supported in Firebolt.","title":"Operators"},{"location":"sql-reference/commands/operators/#arithmetic","text":"","title":"Arithmetic"},{"location":"sql-reference/commands/operators/#notice","text":"Precision means that representation of a number is guaranteed to be accurate up to X number digits. In Firebolt, calculations are 6 digits accurate for FLOAT numbers and 15 for DOUBLE PRECISION . This means that calculations have a precision of 6 or 15 respectively and numbers that are truncated. For example: \\ If a number is stored as 1.234567 it is automatically truncated to 1.23456 for FLOAT . When performing arithmetic, the number of leading digits in the output is the product of the leading digits in both inputs. This means that if either or both of the input numbers are larger than 6, then those numbers are first truncated and then the arithmetic is performed. Operator Operator description Example Result + addition SELECT 2 + 3; 5 - subtraction SELECT 2 - 3; -1 * multiplication SELECT 2 * 3; 6 / division (integer division truncates the result) SELECT 4 / 2; 2 % modulo (remainder) SELECT 5 % 4; 1 ^ exponentiation SELECT 2.0 ^ 3.0; 8","title":"Notice"},{"location":"sql-reference/commands/operators/#comparison","text":"Comparison operators are usually implemented with WHERE clauses. SELECT * FROM Table_Name WHERE Price >= Number; Operator Example Explanation = a=b a is equal to b. <= a<=b a is less than or equal to b. != a!=b a is not equal to b. <> a<>b a is not equal to b. > a>b a is greater than b. >= a>=b a is greater than or equal to b. < a<b a is less than b. Comparison operators are typically used in the WHERE clause of a query.","title":"Comparison"},{"location":"sql-reference/commands/operators/#strings","text":"To concatenate strings, you can use the CONCAT function. SELECT concat('This', ' is', ' a', ' parenthetical', 'concantenation.') AS Concatenated_String Alternatively, you can use the double pipe || operator. SELECT 'This' || ' is' || ' a' || ' double pipe' || ' concantenation.' AS Concatenated_String","title":"Strings"},{"location":"sql-reference/commands/operators/#boolean","text":"Boolean operators return the result of a Boolean operation between one or more expressions. Operator Example Explanation AND x AND y True if both x and y are true NOT NOT x True if x is false X OR Y x OR y True if either x or y is true","title":"Boolean"},{"location":"sql-reference/commands/operators/#date-and-time","text":"","title":"Date and time"},{"location":"sql-reference/commands/operators/#interval","text":"Use the interval operator to add or subtract a period of time to/from a DATE , TIME , or TIMESTAMP . Syntax { +|- } INTERVAL '<quantity> [ <date_unit> ] [ ...]' Component Description <quantity> An integer. Multiple <quantities> and <date_units> can be used in the same INTERVAL command if they are separated by spaces. <date_unit> A date measurement including any of the following: millennium , century , decade , year , month , week , day , hour , minute , second , millisecond , microsecond or their plural forms. If unspecified, <date_unit> defaults to second . Usage example <date_column> + INTERVAL '1 year 2 months 3 days' <date_column> - INTERVAL '2 weeks' <date_column> - INTERVAL '1 year 3 hours 20 minutes'","title":"Interval"},{"location":"sql-reference/commands/operators/#cast-operator","text":"Values can be converted from one data type to another by using the CAST function or the :: operator. Syntax -- CAST function CAST(<value> AS <type>) -- :: operator <value>::<type> Component Description <value> The value to convert or an expression that results in a value to convert. Can be a column name, \u200b \u200ba function applied to a column or another function, or a literal value. <type> The target data type (case-insensitive). Usage example SELECT '2021-12-31'::DATE; SELECT 8.5::FLOAT; SELECT col_a::BIGINT;","title":"Cast operator"},{"location":"sql-reference/commands/operators/#subquery","text":"Subqueries are queries contained within other queries. They are typically used to return entries based on the existence or absence of a condition, as part of a WHERE clause. This section describes subquery operators supported in Firebolt. Operator Explanation EXISTS The EXISTS operator is used to check for the existence of any record in a subquery. It returns TRUE if the subquery returns one or more records. NOT EXISTS The NOT EXISTS operator returns TRUE if the underlying subquery returns no record. IN The IN operator is used to check whether a value matches any value in a list. NOT IN Retrieve all entries from the value list that don't match the required value. Example: Using the EXISTS operator, find all suppliers with products equal to the price of 22 SELECT supplier_name FROM suppliers WHERE EXISTS (SELECT product_name FROM products WHERE products.supplier_id = suppliers.supplier_id AND price < 22); Example: Using the IN operator, return all the customers from Mannheim or London SELECT customer_name FROM customers WHERE customer_address in ('Mannheim','London'); Example: Using correlated subquery to retrieve all the products that cost more than the avg(price) SELECT product_id, product_name, list_price FROM products p WHERE list_price > ( SELECT AVG( list_price ) FROM products WHERE category_id = p.category_id ); Example: Using scalar boolean subquery to retrieve rows based on true/false condition SELECT * FROM products WHERE (SELECT CASE WHEN min(list_price) > 100 THEN true ELSE false END FROM products);","title":"Subquery"},{"location":"sql-reference/commands/query-syntax/","text":"Query syntax Firebolt supports performing standard SELECT statements with the following syntax: [ WITH <with_query> [, ...n] ] SELECT [ DISTINCT ] {<select_expr> [, ...]} [ FROM <from_item> [, ...] ] [ WHERE <condition> ] [ GROUP BY <grouping_element> [, ...] ] [ HAVING <condition> [, ...] ] [ UNION <select_expr> [ ...n] [ ORDER BY <expression> [ ASC | DESC ] [ NULLS FIRST | NULLS LAST] [, ...] ] [ LIMIT <count> ] [ OFFSET <start> ] Notice Multiple queries should be separated by a semicolon (;). WITH The WITH clause is used for sub-query refactoring so that you can specify subqueries and then reference them as part of the main query. This simplifies the hierarchy of the main query, enabling you to avoid using multiple nested sub-queries. In order to reference the data from the WITH clause, a name must be specified for it. This name is then treated as a temporary relation table during query execution. Each subquery can comprise a SELECT , TABLE , VALUES , INSERT , UPDATE , or DELETE statement. The WITH clause is the only clause that precedes the main query. The primary query and the queries included in the WITH clause are all executed at the same time; WITH queries are evaluated only once every time the main query is executed, even if the clause is referred to by the main query more than once. When using the WITH clause as part of a DML command, it must include a RETURNING clause as well, without which the main query cannot reference the WITH clause. Syntax WITH <subquery_table_name> [ <column_name> [, ...n] ] AS <subquery> Component Description <subquery_table_name> A unique name for a temp table <column_name> An optional list of one or more column names. Columns should be separated by commas. <subquery> Any query statement Example The following example retrieves all customers from the \"EMEA\" region, having the results of the WITH query in the temporary table emea_customrs . The results of the main query then list the customer_name and contact_details for those customers, sorted by name. WITH emea_customrs AS ( SELECT * FROM customers WHERE region = 'EMEA' ) SELECT customer_name, contact_details FROM emea_customrs ORDER BY customer_name FROM Use the FROM clause to list the tables and any relevant join information and functions necessary for running the query. Syntax FROM <from_item> [, ...n] Component Description <from_item> Indicates the table or tables from which the data is to be retrieved. Example In the following example, the query retrieves all entries from the customers table for which the region value is \"EMEA\". SELECT * FROM customers WHERE region = 'EMEA' JOIN A JOIN operation combines rows from two data sources (tables/views) and creates a new combined row that can be used in the query. Syntax FROM <from_item> [ NATURAL ] <join_type> <from_item> [ ON <join_condition> ] JOIN types: [ INNER ] JOIN LEFT [ OUTER ] JOIN RIGHT [ OUTER ] JOIN FULL [ OUTER ] JOIN CROSS JOIN Example SELECT * FROM customers c JOIN orders o ON c.cust_key = o.cust_key; UNNEST Should be used when a table contains an array typed column in order to create a new table that contains a column with each array element of the array column, alongside duplicating the values of the other columns per each array element. An UNNEST operation is equivalent to performing a JOIN with an array. The purpose is similar to the ARRAY_UNNEST function, but the UNNEST clause functionality is much broader. Syntax FROM <from_item> UNNEST <expr> Component Description Valid values and syntax <from_item> The table containing the array column that you want to use to create a new table <expr> Indicates the array or array column to unnest from. Can be either an array literal or an array typed column. Any valid array literal or column name Example The example is based on the following table: CREATE FACT TABLE table_with_arrays ( product TEXT, cost ARRAY(INT) ) PRIMARY INDEX name; Assume the table was populated and contains the following values: product cost apple [2,5] Orange [3,6,7] The following query with UNNEST : SELECT product, COST FROM table_with_arrays UNNEST COST JOIN orders o ON c.cust_key = o.cust_key; Returns the following result: product cost apple 2 apple 5 Orange 3 Orange 6 Orange 7 WHERE Use the WHERE clause to define conditions for the query in order to filter the query results. When included, the WHERE clause always follows the FROM clause as part of a command such as SELECT . Syntax WHERE <condition> Component Description Valid values and syntax <condition> Indicates the conditions of the query. Any valid boolean expression. Example In the following example, the query retrieves all entries from the customers table for which the region value is \"EMEA\". SELECT * FROM customers WHERE region = 'EMEA' The following query retrieves users who registered after August 30, 2020 from the users' table: SELECT user_id, city, country FROM users WHERE registration_date >= TO_DATE('2020-08-30'); The following query retrieves users who registered after August 30 2020 and made a purchase: SELECT user_id, city, SELECT user_id, city, country FROM users WHERE registration_date >= TO_DATE('2020-08-30') AND user_id IN ( SELECT user_id FROM purchases ) GROUP BY The GROUP BY clause indicates by what column or columns the results of the query should be grouped. GROUP BY is usually used in conjunction with aggregations, such as SUM , COUNT , MIN , etc. Grouping elements can include column names, the position of columns as specified in the SELECT expression, or other expressions used in the query. Syntax GROUP BY <grouping_element> [, ...n] Component Description <grouping_element> Indicates the condition by which the results should be grouped. The number of <grouping_elements> must match the number of columns specified in the SELECT statement, not counting aggregations. In the following example, the results that are retrieved are grouped by the product_name and then by the product_id columns. SELECT product_name, product_id, sum(total_sales) FROM purchases GROUP BY product_name, product_id You can get similar results by specifying the column positions. SELECT product_name, product_id, SUM(total_sales) FROM purchases GROUP BY 1, 2 Other expressions can also be used, such as a CASE statement: SELECT product_name, SUM(price), CASE WHEN price > 500 THEN \"High value\" WHEN price < 500 THEN \"Low value\" END AS price_text FROM purchases GROUP BY product_name, price_text; HAVING The HAVING clause is used in conjunction with the GROUP BY clause, and is computed after computing the GROUP BY clause and aggregate functions. HAVING is used to further eliminate groups that don't satisfy the <condition> by filtering the GROUP BY results. Syntax HAVING <condition> [, ...n] Component Description <condition> Indicates the boolean condition by which the results should be filtered. UNION [ALL] The UNION operator combines the results of two or more SELECT statements into a single query. UNION combines with duplicate elimination. UNION ALL combines without duplicate elimination. When including multiple clauses, the same number of columns must be selected by all participating SELECT statements. Data types of all column parameters must be the same. Multiple clauses are processed left to right; use parentheses to define an explicit order for processing. Syntax <select_expr1> UNION [ALL] <select_expr2> [ ...n] Component Description <select_expr1> A SELECT statement. <select_expr2> A second SELECT statement to be combined with the first. ORDER BY The ORDER BY clause sorts a result set by one or more output expressions. ORDER BY is evaluated as the last step after any GROUP BY or HAVING clause. ASC and DESC determine whether results are sorted in ascending or descending order. When the clause contains multiple expressions, the result set is sorted according to the first expression. Then the second expression is applied to rows that have matching values from the first expression, and so on. The default null ordering is NULLS LAST , regardless of ascending or descending sort order. Syntax ORDER BY <expression> [ ASC | DESC ] [ NULLS FIRST | NULLS LAST] [, ...] Component Description <expression> Each expression may specify output columns from SELECT or an ordinal number for an output column by position, starting at one. [ ASC \\| DESC ] Indicates whether the sort should be in ascending or descending order. [ NULLS FIRST \\| NULLS LAST] Indicates whether null values should be included at the beginning or end of the result. The default null ordering is NULLS LAST , regardless of ascending or descending sort order. LIMIT The LIMIT clause restricts the number of rows that are included in the result set. Syntax LIMIT <count> Component Description Valid values and syntax <count> Indicates the number of rows that should be returned An integer LIMIT_DISTINCT The LIMIT_DISTINCT clause selects a defined number of rows to return for each distinct value as specified by the expression. Returned values will not be grouped together unless you use an ORDER BY statement. LIMIT_DISTINCT does not preclude also using a LIMIT clause. Syntax LIMIT_DISTINCT <returned_values> [OFFSET <offset_value>] BY <expr> [, <expr2> [,...n]] Component Description <returned_values> The number of rows to return for each distinct value, as defined by the expr <offset_value> The number of rows that should be skipped from the beginning of the block before returning rows for <returned_values> . If the <offset_value> exceeds the number of rows in the data block, no rows are returned. The <offset_value> is restricted to be greater than 0. <expr> [, <expr2> [,...n]] The expression(s) to define the distinct value that will be limited by LIMIT_DISTINCT . Multiple expressions can be used. Examples To demonstrate the LIMIT_DISTINCT clause, we will create an example table of students called class_test : CREATE DIMENSION TABLE IF NOT EXISTS class_test3 ( First_name TEXT, Last_name TEXT, Teacher TEXT, Grade_level INT ); INSERT INTO class_test3 VALUES ('Sammy','Sardine','Roberts', 12), ('Carol','Catnip','Roberts', 10), ('Thomas','Tinderbox','Roberts', 10), ('Deborah','Donut','Roberts', 9), ('Humphrey','Hoagie','Roberts', 11), ('Frank','Falafel', 'Chamberpot', 11), ('Peter','Patchouli', 'Chamberpot', 11), ('Albert','Applesauce','Chamberpot', 11), ('Gary','Garnish', 'Chamberpot', 12), ('Roseanna','Rotisserie','Chamberpot', 12), ('Jesse','Jelloshot','Chamberpot', 12), ('Mary','Marblecake','Diddlysquat', 9), ('Iris','Icecream','Diddlysquat', 9), ('Larry','Lardbelly','Diddlysquat', 9), ('Charles','Cokebottle','Diddlysquat', 10), ('Brunhilda','Breadknife','Diddlysquat', 10), ('Franco','Frenchdip','Rosebottom', 11), ('Yolinda','Yogurt','Rosebottom', 11), ('Jojo','Jujubee','Rosebottom', 11), ('Shangxiu','Shadytree','Rosebottom', 12), ('Otis','Oatmeal','Rosebottom', 12), ('Wanda','Waterfall','Rosebottom', 10), ('Shawn','Sharpshooter','Rosebottom', 10) The example below uses LIMIT_DISTINCT to get a subset of each teacher's students. For each teacher, only two students are included in the query results. SELECT Teacher, Student_Firstname, Student_Lastname FROM class_test ORDER BY Teacher LIMIT_DISTINCT 2 BY Teacher; Returns: +--------------------------------------------+ | Teacher,Student_Firstname,Student_Lastname | +--------------------------------------------+ | Chamberpot,Peter,Patchouli | | Chamberpot,Roseanna,Rotisserie | | Diddlysquat,Mary,Marblecake | | Diddlysquat,Larry,Lardbelly | | Roberts,Carol,Catnip | | Roberts,Sammy,Sardine | | Rosebottom,Wanda,Waterfall | | Rosebottom,Shawn,Sharpshooter | +--------------------------------------------+ LIMIT_DISTINCT can also include multiple expressions to limit for distinct values. The example below returns a subset of each teacher's students in each grade level. SELECT Teacher, Grade_level, First_name, Last_name FROM class_test3 ORDER BY Teacher LIMIT_DISTINCT 1 BY Teacher, Grade_level; Returns: +----------------------------------------------------------+ | Teacher,Grade_level, Student_Firstname, Student_Lastname | +----------------------------------------------------------+ | Chamberpot,11,Peter,Patchouli | | Chamberpot,12,Gary,Garnish | | Diddlysquat,9,Mary,Marblecake | | Diddlysquat,10,Charles,Cokebottle | | Roberts,12,Sammy,Sardine | | Roberts,11,Humphrey,Hoagie | | Roberts,10,Carol,Catnip | | Roberts,9,Deborah,Donut | | Rosebottom,12,Otis,Oatmeal | | Rosebottom,10,Shawn,Sharpshooter | | Rosebottom,11,Jojo,Jujubee | +----------------------------------------------------------+ OFFSET The OFFSET clause omits a specified number of rows from the beginning of the result set. When used with LIMIT , it specifies the row number after which the limited rows are returned. When used with FETCH , only the number of rows specified by the <fetch_count> are returned after the OFFSET Syntax [ LIMIT <limit_number> ] OFFSET <offset_number> [ FETCH <fetch_count>] Component Description Valid values and syntax <limit_number> Indicates the number of rows to be returned An integer <offset_number> Indicates the start row number An integer <fetch_count> Fetch with count specifies the number of rows to fetch from the offset An integer In the following example, we will retrieve 3 rows, starting from the 2nd row. SELECT product_name, product_id FROM purchases ORDER BY 1 LIMIT 3 OFFSET 1;","title":"Query syntax"},{"location":"sql-reference/commands/query-syntax/#query-syntax","text":"Firebolt supports performing standard SELECT statements with the following syntax: [ WITH <with_query> [, ...n] ] SELECT [ DISTINCT ] {<select_expr> [, ...]} [ FROM <from_item> [, ...] ] [ WHERE <condition> ] [ GROUP BY <grouping_element> [, ...] ] [ HAVING <condition> [, ...] ] [ UNION <select_expr> [ ...n] [ ORDER BY <expression> [ ASC | DESC ] [ NULLS FIRST | NULLS LAST] [, ...] ] [ LIMIT <count> ] [ OFFSET <start> ]","title":"Query syntax"},{"location":"sql-reference/commands/query-syntax/#notice","text":"Multiple queries should be separated by a semicolon (;).","title":"Notice"},{"location":"sql-reference/commands/query-syntax/#with","text":"The WITH clause is used for sub-query refactoring so that you can specify subqueries and then reference them as part of the main query. This simplifies the hierarchy of the main query, enabling you to avoid using multiple nested sub-queries. In order to reference the data from the WITH clause, a name must be specified for it. This name is then treated as a temporary relation table during query execution. Each subquery can comprise a SELECT , TABLE , VALUES , INSERT , UPDATE , or DELETE statement. The WITH clause is the only clause that precedes the main query. The primary query and the queries included in the WITH clause are all executed at the same time; WITH queries are evaluated only once every time the main query is executed, even if the clause is referred to by the main query more than once. When using the WITH clause as part of a DML command, it must include a RETURNING clause as well, without which the main query cannot reference the WITH clause. Syntax WITH <subquery_table_name> [ <column_name> [, ...n] ] AS <subquery> Component Description <subquery_table_name> A unique name for a temp table <column_name> An optional list of one or more column names. Columns should be separated by commas. <subquery> Any query statement Example The following example retrieves all customers from the \"EMEA\" region, having the results of the WITH query in the temporary table emea_customrs . The results of the main query then list the customer_name and contact_details for those customers, sorted by name. WITH emea_customrs AS ( SELECT * FROM customers WHERE region = 'EMEA' ) SELECT customer_name, contact_details FROM emea_customrs ORDER BY customer_name","title":"WITH"},{"location":"sql-reference/commands/query-syntax/#from","text":"Use the FROM clause to list the tables and any relevant join information and functions necessary for running the query. Syntax FROM <from_item> [, ...n] Component Description <from_item> Indicates the table or tables from which the data is to be retrieved. Example In the following example, the query retrieves all entries from the customers table for which the region value is \"EMEA\". SELECT * FROM customers WHERE region = 'EMEA'","title":"FROM"},{"location":"sql-reference/commands/query-syntax/#join","text":"A JOIN operation combines rows from two data sources (tables/views) and creates a new combined row that can be used in the query. Syntax FROM <from_item> [ NATURAL ] <join_type> <from_item> [ ON <join_condition> ] JOIN types: [ INNER ] JOIN LEFT [ OUTER ] JOIN RIGHT [ OUTER ] JOIN FULL [ OUTER ] JOIN CROSS JOIN Example SELECT * FROM customers c JOIN orders o ON c.cust_key = o.cust_key;","title":"JOIN"},{"location":"sql-reference/commands/query-syntax/#unnest","text":"Should be used when a table contains an array typed column in order to create a new table that contains a column with each array element of the array column, alongside duplicating the values of the other columns per each array element. An UNNEST operation is equivalent to performing a JOIN with an array. The purpose is similar to the ARRAY_UNNEST function, but the UNNEST clause functionality is much broader. Syntax FROM <from_item> UNNEST <expr> Component Description Valid values and syntax <from_item> The table containing the array column that you want to use to create a new table <expr> Indicates the array or array column to unnest from. Can be either an array literal or an array typed column. Any valid array literal or column name Example The example is based on the following table: CREATE FACT TABLE table_with_arrays ( product TEXT, cost ARRAY(INT) ) PRIMARY INDEX name; Assume the table was populated and contains the following values: product cost apple [2,5] Orange [3,6,7] The following query with UNNEST : SELECT product, COST FROM table_with_arrays UNNEST COST JOIN orders o ON c.cust_key = o.cust_key; Returns the following result: product cost apple 2 apple 5 Orange 3 Orange 6 Orange 7","title":"UNNEST"},{"location":"sql-reference/commands/query-syntax/#where","text":"Use the WHERE clause to define conditions for the query in order to filter the query results. When included, the WHERE clause always follows the FROM clause as part of a command such as SELECT . Syntax WHERE <condition> Component Description Valid values and syntax <condition> Indicates the conditions of the query. Any valid boolean expression. Example In the following example, the query retrieves all entries from the customers table for which the region value is \"EMEA\". SELECT * FROM customers WHERE region = 'EMEA' The following query retrieves users who registered after August 30, 2020 from the users' table: SELECT user_id, city, country FROM users WHERE registration_date >= TO_DATE('2020-08-30'); The following query retrieves users who registered after August 30 2020 and made a purchase: SELECT user_id, city, SELECT user_id, city, country FROM users WHERE registration_date >= TO_DATE('2020-08-30') AND user_id IN ( SELECT user_id FROM purchases )","title":"WHERE"},{"location":"sql-reference/commands/query-syntax/#group-by","text":"The GROUP BY clause indicates by what column or columns the results of the query should be grouped. GROUP BY is usually used in conjunction with aggregations, such as SUM , COUNT , MIN , etc. Grouping elements can include column names, the position of columns as specified in the SELECT expression, or other expressions used in the query. Syntax GROUP BY <grouping_element> [, ...n] Component Description <grouping_element> Indicates the condition by which the results should be grouped. The number of <grouping_elements> must match the number of columns specified in the SELECT statement, not counting aggregations. In the following example, the results that are retrieved are grouped by the product_name and then by the product_id columns. SELECT product_name, product_id, sum(total_sales) FROM purchases GROUP BY product_name, product_id You can get similar results by specifying the column positions. SELECT product_name, product_id, SUM(total_sales) FROM purchases GROUP BY 1, 2 Other expressions can also be used, such as a CASE statement: SELECT product_name, SUM(price), CASE WHEN price > 500 THEN \"High value\" WHEN price < 500 THEN \"Low value\" END AS price_text FROM purchases GROUP BY product_name, price_text;","title":"GROUP BY"},{"location":"sql-reference/commands/query-syntax/#having","text":"The HAVING clause is used in conjunction with the GROUP BY clause, and is computed after computing the GROUP BY clause and aggregate functions. HAVING is used to further eliminate groups that don't satisfy the <condition> by filtering the GROUP BY results. Syntax HAVING <condition> [, ...n] Component Description <condition> Indicates the boolean condition by which the results should be filtered.","title":"HAVING"},{"location":"sql-reference/commands/query-syntax/#union-all","text":"The UNION operator combines the results of two or more SELECT statements into a single query. UNION combines with duplicate elimination. UNION ALL combines without duplicate elimination. When including multiple clauses, the same number of columns must be selected by all participating SELECT statements. Data types of all column parameters must be the same. Multiple clauses are processed left to right; use parentheses to define an explicit order for processing. Syntax <select_expr1> UNION [ALL] <select_expr2> [ ...n] Component Description <select_expr1> A SELECT statement. <select_expr2> A second SELECT statement to be combined with the first.","title":"UNION [ALL]"},{"location":"sql-reference/commands/query-syntax/#order-by","text":"The ORDER BY clause sorts a result set by one or more output expressions. ORDER BY is evaluated as the last step after any GROUP BY or HAVING clause. ASC and DESC determine whether results are sorted in ascending or descending order. When the clause contains multiple expressions, the result set is sorted according to the first expression. Then the second expression is applied to rows that have matching values from the first expression, and so on. The default null ordering is NULLS LAST , regardless of ascending or descending sort order. Syntax ORDER BY <expression> [ ASC | DESC ] [ NULLS FIRST | NULLS LAST] [, ...] Component Description <expression> Each expression may specify output columns from SELECT or an ordinal number for an output column by position, starting at one. [ ASC \\| DESC ] Indicates whether the sort should be in ascending or descending order. [ NULLS FIRST \\| NULLS LAST] Indicates whether null values should be included at the beginning or end of the result. The default null ordering is NULLS LAST , regardless of ascending or descending sort order.","title":"ORDER BY"},{"location":"sql-reference/commands/query-syntax/#limit","text":"The LIMIT clause restricts the number of rows that are included in the result set. Syntax LIMIT <count> Component Description Valid values and syntax <count> Indicates the number of rows that should be returned An integer","title":"LIMIT"},{"location":"sql-reference/commands/query-syntax/#limit_distinct","text":"The LIMIT_DISTINCT clause selects a defined number of rows to return for each distinct value as specified by the expression. Returned values will not be grouped together unless you use an ORDER BY statement. LIMIT_DISTINCT does not preclude also using a LIMIT clause. Syntax LIMIT_DISTINCT <returned_values> [OFFSET <offset_value>] BY <expr> [, <expr2> [,...n]] Component Description <returned_values> The number of rows to return for each distinct value, as defined by the expr <offset_value> The number of rows that should be skipped from the beginning of the block before returning rows for <returned_values> . If the <offset_value> exceeds the number of rows in the data block, no rows are returned. The <offset_value> is restricted to be greater than 0. <expr> [, <expr2> [,...n]] The expression(s) to define the distinct value that will be limited by LIMIT_DISTINCT . Multiple expressions can be used. Examples To demonstrate the LIMIT_DISTINCT clause, we will create an example table of students called class_test : CREATE DIMENSION TABLE IF NOT EXISTS class_test3 ( First_name TEXT, Last_name TEXT, Teacher TEXT, Grade_level INT ); INSERT INTO class_test3 VALUES ('Sammy','Sardine','Roberts', 12), ('Carol','Catnip','Roberts', 10), ('Thomas','Tinderbox','Roberts', 10), ('Deborah','Donut','Roberts', 9), ('Humphrey','Hoagie','Roberts', 11), ('Frank','Falafel', 'Chamberpot', 11), ('Peter','Patchouli', 'Chamberpot', 11), ('Albert','Applesauce','Chamberpot', 11), ('Gary','Garnish', 'Chamberpot', 12), ('Roseanna','Rotisserie','Chamberpot', 12), ('Jesse','Jelloshot','Chamberpot', 12), ('Mary','Marblecake','Diddlysquat', 9), ('Iris','Icecream','Diddlysquat', 9), ('Larry','Lardbelly','Diddlysquat', 9), ('Charles','Cokebottle','Diddlysquat', 10), ('Brunhilda','Breadknife','Diddlysquat', 10), ('Franco','Frenchdip','Rosebottom', 11), ('Yolinda','Yogurt','Rosebottom', 11), ('Jojo','Jujubee','Rosebottom', 11), ('Shangxiu','Shadytree','Rosebottom', 12), ('Otis','Oatmeal','Rosebottom', 12), ('Wanda','Waterfall','Rosebottom', 10), ('Shawn','Sharpshooter','Rosebottom', 10) The example below uses LIMIT_DISTINCT to get a subset of each teacher's students. For each teacher, only two students are included in the query results. SELECT Teacher, Student_Firstname, Student_Lastname FROM class_test ORDER BY Teacher LIMIT_DISTINCT 2 BY Teacher; Returns: +--------------------------------------------+ | Teacher,Student_Firstname,Student_Lastname | +--------------------------------------------+ | Chamberpot,Peter,Patchouli | | Chamberpot,Roseanna,Rotisserie | | Diddlysquat,Mary,Marblecake | | Diddlysquat,Larry,Lardbelly | | Roberts,Carol,Catnip | | Roberts,Sammy,Sardine | | Rosebottom,Wanda,Waterfall | | Rosebottom,Shawn,Sharpshooter | +--------------------------------------------+ LIMIT_DISTINCT can also include multiple expressions to limit for distinct values. The example below returns a subset of each teacher's students in each grade level. SELECT Teacher, Grade_level, First_name, Last_name FROM class_test3 ORDER BY Teacher LIMIT_DISTINCT 1 BY Teacher, Grade_level; Returns: +----------------------------------------------------------+ | Teacher,Grade_level, Student_Firstname, Student_Lastname | +----------------------------------------------------------+ | Chamberpot,11,Peter,Patchouli | | Chamberpot,12,Gary,Garnish | | Diddlysquat,9,Mary,Marblecake | | Diddlysquat,10,Charles,Cokebottle | | Roberts,12,Sammy,Sardine | | Roberts,11,Humphrey,Hoagie | | Roberts,10,Carol,Catnip | | Roberts,9,Deborah,Donut | | Rosebottom,12,Otis,Oatmeal | | Rosebottom,10,Shawn,Sharpshooter | | Rosebottom,11,Jojo,Jujubee | +----------------------------------------------------------+","title":"LIMIT_DISTINCT"},{"location":"sql-reference/commands/query-syntax/#offset","text":"The OFFSET clause omits a specified number of rows from the beginning of the result set. When used with LIMIT , it specifies the row number after which the limited rows are returned. When used with FETCH , only the number of rows specified by the <fetch_count> are returned after the OFFSET Syntax [ LIMIT <limit_number> ] OFFSET <offset_number> [ FETCH <fetch_count>] Component Description Valid values and syntax <limit_number> Indicates the number of rows to be returned An integer <offset_number> Indicates the start row number An integer <fetch_count> Fetch with count specifies the number of rows to fetch from the offset An integer In the following example, we will retrieve 3 rows, starting from the 2nd row. SELECT product_name, product_id FROM purchases ORDER BY 1 LIMIT 3 OFFSET 1;","title":"OFFSET"},{"location":"sql-reference/functions-reference/","text":"Functions reference","title":"Functions reference"},{"location":"sql-reference/functions-reference/#functions-reference","text":"","title":"Functions reference"},{"location":"sql-reference/functions-reference/aggregation-functions/","text":"Aggregation functions This page describes the aggregation functions supported in Firebolt. ANY Returns the first value encountered in the specified column. The function is indeterminate. It can be executed in any order and might be executed in a different order each time. ANY(<col>) Parameter Description <col> The column from which the value will be returned. The column can be any supported data type. To demonstrate ANY , we'll create a basic example table. CREATE DIMENSION TABLE IF NOT EXISTS example ( First_name TEXT ); INSERT INTO example VALUES ('Sammy'), ('Carol'), ('Thomas'), ('Deborah'); Example SELECT ANY(First_name) FROM example; Returns: Sammy ANY_VALUE Returns one arbitrary value from the specified column. Syntax \u200b\u200bANY_VALUE(<col>)\u200b\u200b Parameter Description <col> The column from which the value is returned. Example The example below uses the same example table used in the ANY documentation above. SELECT ANY_VALUE(First_name) FROM example; Returns : Carol APPROX_PERCENTILE Returns an approximate value for the specified percentile based on the range of numbers returned by the expression. For example, if you run APPROX_PERCENTILE with a specified <percent> of .75 on a column with 2,000 numbers, and the function returned 655 , then this would indicate that 75% of the 2,000 numbers in the column are less than 655. The number returned is not necessarily in the original range of numbers. Syntax APPROX_PERCENTILE(<expr>,<percent>) Parameter Description <expr> A valid expression, such as a column name, that evaluates to numeric values. <percent> A constant real number greater than or equal to 0.0 and less than 1. For example, .999 specifies the 99.9th percentile. To demonstrate APPROX_PERCENTILE , we'll use the example table number_test as created below. This provides a range of numbers between 1 and 100. CREATE DIMENSION TABLE IF NOT EXISTS number_test ( First_name TEXT ); INSERT INTO number_test VALUES (1), (100), (55), (16), (48), (86), (33), (22); The example below shows APPROX_PERCENTILE of 50% of the number range in number_test . SELECT APPROX_PERCENTILE(num, 0.5) FROM number_test; Returns: 40.5 The example below shows an APPROX_PERCENTILE of 25%. SELECT APPROX_PERCENTILE(num, 0.25) FROM number_test; Returns: 20.5 AVG Calculates the average of an expression Syntax \u200b\u200bAVG(<expr>)\u200b\u200b Parameter Description <expr> The expression used to calculate the average. Valid values for the expression include column names or functions that return a column name for columns that contain numeric values. The AVG() aggregation function ignores rows with NULL values; so an AVG from 3 rows containing 1, 2 and NULL values results in 1.5 because the NULL row is not counted. To calculate an average that includes NULL values, use SUM(COLUMN)/COUNT(*) CHECKSUM Calculates a hash value known as a checksum operation on a list of arguments. Performing a checksum operation is useful for warming up table data or to check if the same values exist in two different tables. Syntax CHECKSUM( <expr1> [, <expr2>] [, <expr3>] [, ...n] ) Example The example below calculates a checksum based on all data in the table mytable and returns the numeric hash value for the checksum. SELECT CHECKSUM(*) FROM mytable; Returns : 18112375909223891695 COUNT Counts the number of rows or not NULL values. Syntax COUNT([ DISTINCT ] <expr>) Parameter Description <expr> Valid values for the expression include column names (or * for counting all columns) or functions that return a column name. When DISTINCT is being used, counts only the unique number of rows with no NULL values. Notes COUNT(*) will return a total count of all rows in the table, while COUNT(<column_name>) will return a count of rows with a non-NULL value in that particular column. By default, COUNT(DISTINCT) will return approximate results. If you wish to get an accurate result (with a performance penalty), set the following parameter : SET firebolt_optimization_enable_exact_count_distinct=1; Examples For this example, we'll create a new table number_test as shown below. CREATE DIMENSION TABLE IF NOT EXISTS number_test ( num TEXT ); INSERT INTO number_test VALUES (1), (1), (2), (3), (3), (3), (4), (5); Doing a regular COUNT returns the total number of rows in the column. We inserted 8 rows earlier, so it should return the same number. SELECT COUNT(num) FROM number_test; Returns : 8 A COUNT(DISTINCT) function on the same column returns the number of unique rows. There are five unique numbers that we inserted earlier. SELECT COUNT(DISTINCT num) FROM number_test; Returns : 5 MAX Calculates the maximum value of an expression across all input values. Syntax \u200b\u200bMAX(<expr>) Parameter Description <expr> The expression used to calculate the maximum values. Valid values for the expression include a column name or functions that return a column name. Examples For this example, we'll create a new table prices as shown below. CREATE DIMENSION TABLE IF NOT EXISTS prices ( item TEXT, price INT ); INSERT INTO prices VALUES ('apple', 4), ('banana', 25), ('orange', 11), ('kiwi', 20) When used on the num column, MAX will return the largest value. SELECT MAX(price) FROM prices; Returns: 25 MAX can also work on text columns by returning the text row with the characters that are last in the lexicographic order. SELECT MAX(item) FROM prices; Returns: orange MAX_BY The MAX_BY function returns a value for the <arg> column based on the max value in a separate column, specified by <val> . If there is more than one max value in <val> , then the first will be used. Syntax MAX_BY(<arg>, <val>) Parameter Description <arg> The column from which the value is returned. <val> The column that is search for a maximum value. Example For this example, we will again use the prices table that was created above for the MAX function. The values for that table are below: item price apple 4 banana 25 orange 11 kiwi 20 In this example below, MAX_BY is used to find the item with the largest price. SELECT MAX_BY(item, price) FROM prices; Returns: banana MEDIAN Calculates an approximate median for a given column. Syntax \u200b\u200bMEDIAN(<col>)\u200b\u200b Parameter Description <col> The column used to calculate the median value. This column can consist of numeric data types or DATE and DATETIME. Example For this example, we'll create a new table num_test as shown below: CREATE DIMENSION TABLE IF NOT EXISTS num_test ( num int ); INSERT INTO num_test VALUES (1), (7), (12), (30), (59), (76), (100); MEDIAN returns the approximate middle value between the lower and higher halves of the values. SELECT MEDIAN(num) FROM number_test Returns: 30 MIN Calculates the minimum value of an expression across all input values. Syntax \u200b\u200bMIN(<expr>) Parameter Description <expr> The expression used to calculate the minimum values. Valid values for the expression include a column name or functions that return a column name. Example For this example, we'll create a new table prices as shown below. CREATE DIMENSION TABLE IF NOT EXISTS prices ( item TEXT, price INT ); INSERT INTO prices VALUES ('apple', 4), ('banana', 25), ('orange', 11), ('kiwi', 20) When used on the num column, MIN will return the largest value. SELECT MIN(price) FROM prices; Returns: 4 MIN can also work on text columns by returning the text row with the characters that are first in the lexicographic order. SELECT MIN(item) FROM prices; Returns : apple MIN_BY The MIN_BY function returns the value of arg column at the row in which the val column is minimal. If there is more than one minimal values in val , then the first will be used. Syntax MIN_BY(arg, val) Parameter Description <arg> The column from which the value is returned. <val> The column that is search for a minimum value. Example For this example, we will again use the prices table that was created above for the MIN function. The values for that table are below: item price apple 4 banana 25 orange 11 kiwi 20 In this example below, MIN_BY is used to find the item with the lowest price. SELECT MIN_BY(item, price) FROM prices Returns: apple NEST Takes a column as an argument, and returns an array of the values. See the full description under Semi-structured data functions. STDDEV_SAMP Computes the standard deviation of a sample consisting of a numeric-expression. Syntax STDDEV_SAMP(<expr>)\u200b Parameter Description <expr> Any column with numeric values or an expression that returns a column with numeric values. Example For this example, we'll create a new table num_test as shown below: CREATE DIMENSION TABLE IF NOT EXISTS num_test ( num int ); INSERT INTO num_test VALUES (1), (7), (12), (30), (59), (76), (100); STDDEV_SAMP returns the standard deviation for the values. SELECT STDDEV_SAMP(num) FROM num_test Returns : 38.18251906180054 SUM Calculates the sum of an expression. Syntax \u200b\u200bSUM(<expr>)\u200b\u200b Parameter Description <expr> The expression used to calculate the sum. Valid values for <expr> include column names or expressions that evaluate to numeric values. Example For this example, we'll create a new table num_test as shown below: CREATE DIMENSION TABLE IF NOT EXISTS num_test ( num int ); INSERT INTO num_test VALUES (1), (7), (12), (30), (59), (76), (100); SUM adds together all of the values in the num column. SELECT SUM(num) FROM numb_test Returns : 285","title":"Aggregation functions"},{"location":"sql-reference/functions-reference/aggregation-functions/#aggregation-functions","text":"This page describes the aggregation functions supported in Firebolt.","title":"Aggregation functions"},{"location":"sql-reference/functions-reference/aggregation-functions/#any","text":"Returns the first value encountered in the specified column. The function is indeterminate. It can be executed in any order and might be executed in a different order each time. ANY(<col>) Parameter Description <col> The column from which the value will be returned. The column can be any supported data type. To demonstrate ANY , we'll create a basic example table. CREATE DIMENSION TABLE IF NOT EXISTS example ( First_name TEXT ); INSERT INTO example VALUES ('Sammy'), ('Carol'), ('Thomas'), ('Deborah'); Example SELECT ANY(First_name) FROM example; Returns: Sammy","title":"ANY"},{"location":"sql-reference/functions-reference/aggregation-functions/#any_value","text":"Returns one arbitrary value from the specified column. Syntax \u200b\u200bANY_VALUE(<col>)\u200b\u200b Parameter Description <col> The column from which the value is returned. Example The example below uses the same example table used in the ANY documentation above. SELECT ANY_VALUE(First_name) FROM example; Returns : Carol","title":"ANY_VALUE"},{"location":"sql-reference/functions-reference/aggregation-functions/#approx_percentile","text":"Returns an approximate value for the specified percentile based on the range of numbers returned by the expression. For example, if you run APPROX_PERCENTILE with a specified <percent> of .75 on a column with 2,000 numbers, and the function returned 655 , then this would indicate that 75% of the 2,000 numbers in the column are less than 655. The number returned is not necessarily in the original range of numbers. Syntax APPROX_PERCENTILE(<expr>,<percent>) Parameter Description <expr> A valid expression, such as a column name, that evaluates to numeric values. <percent> A constant real number greater than or equal to 0.0 and less than 1. For example, .999 specifies the 99.9th percentile. To demonstrate APPROX_PERCENTILE , we'll use the example table number_test as created below. This provides a range of numbers between 1 and 100. CREATE DIMENSION TABLE IF NOT EXISTS number_test ( First_name TEXT ); INSERT INTO number_test VALUES (1), (100), (55), (16), (48), (86), (33), (22); The example below shows APPROX_PERCENTILE of 50% of the number range in number_test . SELECT APPROX_PERCENTILE(num, 0.5) FROM number_test; Returns: 40.5 The example below shows an APPROX_PERCENTILE of 25%. SELECT APPROX_PERCENTILE(num, 0.25) FROM number_test; Returns: 20.5","title":"APPROX_PERCENTILE"},{"location":"sql-reference/functions-reference/aggregation-functions/#avg","text":"Calculates the average of an expression Syntax \u200b\u200bAVG(<expr>)\u200b\u200b Parameter Description <expr> The expression used to calculate the average. Valid values for the expression include column names or functions that return a column name for columns that contain numeric values. The AVG() aggregation function ignores rows with NULL values; so an AVG from 3 rows containing 1, 2 and NULL values results in 1.5 because the NULL row is not counted. To calculate an average that includes NULL values, use SUM(COLUMN)/COUNT(*)","title":"AVG"},{"location":"sql-reference/functions-reference/aggregation-functions/#checksum","text":"Calculates a hash value known as a checksum operation on a list of arguments. Performing a checksum operation is useful for warming up table data or to check if the same values exist in two different tables. Syntax CHECKSUM( <expr1> [, <expr2>] [, <expr3>] [, ...n] ) Example The example below calculates a checksum based on all data in the table mytable and returns the numeric hash value for the checksum. SELECT CHECKSUM(*) FROM mytable; Returns : 18112375909223891695","title":"CHECKSUM"},{"location":"sql-reference/functions-reference/aggregation-functions/#count","text":"Counts the number of rows or not NULL values. Syntax COUNT([ DISTINCT ] <expr>) Parameter Description <expr> Valid values for the expression include column names (or * for counting all columns) or functions that return a column name. When DISTINCT is being used, counts only the unique number of rows with no NULL values. Notes COUNT(*) will return a total count of all rows in the table, while COUNT(<column_name>) will return a count of rows with a non-NULL value in that particular column. By default, COUNT(DISTINCT) will return approximate results. If you wish to get an accurate result (with a performance penalty), set the following parameter : SET firebolt_optimization_enable_exact_count_distinct=1; Examples For this example, we'll create a new table number_test as shown below. CREATE DIMENSION TABLE IF NOT EXISTS number_test ( num TEXT ); INSERT INTO number_test VALUES (1), (1), (2), (3), (3), (3), (4), (5); Doing a regular COUNT returns the total number of rows in the column. We inserted 8 rows earlier, so it should return the same number. SELECT COUNT(num) FROM number_test; Returns : 8 A COUNT(DISTINCT) function on the same column returns the number of unique rows. There are five unique numbers that we inserted earlier. SELECT COUNT(DISTINCT num) FROM number_test; Returns : 5","title":"COUNT"},{"location":"sql-reference/functions-reference/aggregation-functions/#max","text":"Calculates the maximum value of an expression across all input values. Syntax \u200b\u200bMAX(<expr>) Parameter Description <expr> The expression used to calculate the maximum values. Valid values for the expression include a column name or functions that return a column name. Examples For this example, we'll create a new table prices as shown below. CREATE DIMENSION TABLE IF NOT EXISTS prices ( item TEXT, price INT ); INSERT INTO prices VALUES ('apple', 4), ('banana', 25), ('orange', 11), ('kiwi', 20) When used on the num column, MAX will return the largest value. SELECT MAX(price) FROM prices; Returns: 25 MAX can also work on text columns by returning the text row with the characters that are last in the lexicographic order. SELECT MAX(item) FROM prices; Returns: orange","title":"MAX"},{"location":"sql-reference/functions-reference/aggregation-functions/#max_by","text":"The MAX_BY function returns a value for the <arg> column based on the max value in a separate column, specified by <val> . If there is more than one max value in <val> , then the first will be used. Syntax MAX_BY(<arg>, <val>) Parameter Description <arg> The column from which the value is returned. <val> The column that is search for a maximum value. Example For this example, we will again use the prices table that was created above for the MAX function. The values for that table are below: item price apple 4 banana 25 orange 11 kiwi 20 In this example below, MAX_BY is used to find the item with the largest price. SELECT MAX_BY(item, price) FROM prices; Returns: banana","title":"MAX_BY"},{"location":"sql-reference/functions-reference/aggregation-functions/#median","text":"Calculates an approximate median for a given column. Syntax \u200b\u200bMEDIAN(<col>)\u200b\u200b Parameter Description <col> The column used to calculate the median value. This column can consist of numeric data types or DATE and DATETIME. Example For this example, we'll create a new table num_test as shown below: CREATE DIMENSION TABLE IF NOT EXISTS num_test ( num int ); INSERT INTO num_test VALUES (1), (7), (12), (30), (59), (76), (100); MEDIAN returns the approximate middle value between the lower and higher halves of the values. SELECT MEDIAN(num) FROM number_test Returns: 30","title":"MEDIAN"},{"location":"sql-reference/functions-reference/aggregation-functions/#min","text":"Calculates the minimum value of an expression across all input values. Syntax \u200b\u200bMIN(<expr>) Parameter Description <expr> The expression used to calculate the minimum values. Valid values for the expression include a column name or functions that return a column name. Example For this example, we'll create a new table prices as shown below. CREATE DIMENSION TABLE IF NOT EXISTS prices ( item TEXT, price INT ); INSERT INTO prices VALUES ('apple', 4), ('banana', 25), ('orange', 11), ('kiwi', 20) When used on the num column, MIN will return the largest value. SELECT MIN(price) FROM prices; Returns: 4 MIN can also work on text columns by returning the text row with the characters that are first in the lexicographic order. SELECT MIN(item) FROM prices; Returns : apple","title":"MIN"},{"location":"sql-reference/functions-reference/aggregation-functions/#min_by","text":"The MIN_BY function returns the value of arg column at the row in which the val column is minimal. If there is more than one minimal values in val , then the first will be used. Syntax MIN_BY(arg, val) Parameter Description <arg> The column from which the value is returned. <val> The column that is search for a minimum value. Example For this example, we will again use the prices table that was created above for the MIN function. The values for that table are below: item price apple 4 banana 25 orange 11 kiwi 20 In this example below, MIN_BY is used to find the item with the lowest price. SELECT MIN_BY(item, price) FROM prices Returns: apple","title":"MIN_BY"},{"location":"sql-reference/functions-reference/aggregation-functions/#nest","text":"Takes a column as an argument, and returns an array of the values. See the full description under Semi-structured data functions.","title":"NEST"},{"location":"sql-reference/functions-reference/aggregation-functions/#stddev_samp","text":"Computes the standard deviation of a sample consisting of a numeric-expression. Syntax STDDEV_SAMP(<expr>)\u200b Parameter Description <expr> Any column with numeric values or an expression that returns a column with numeric values. Example For this example, we'll create a new table num_test as shown below: CREATE DIMENSION TABLE IF NOT EXISTS num_test ( num int ); INSERT INTO num_test VALUES (1), (7), (12), (30), (59), (76), (100); STDDEV_SAMP returns the standard deviation for the values. SELECT STDDEV_SAMP(num) FROM num_test Returns : 38.18251906180054","title":"STDDEV_SAMP"},{"location":"sql-reference/functions-reference/aggregation-functions/#sum","text":"Calculates the sum of an expression. Syntax \u200b\u200bSUM(<expr>)\u200b\u200b Parameter Description <expr> The expression used to calculate the sum. Valid values for <expr> include column names or expressions that evaluate to numeric values. Example For this example, we'll create a new table num_test as shown below: CREATE DIMENSION TABLE IF NOT EXISTS num_test ( num int ); INSERT INTO num_test VALUES (1), (7), (12), (30), (59), (76), (100); SUM adds together all of the values in the num column. SELECT SUM(num) FROM numb_test Returns : 285","title":"SUM"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/","text":"Conditional and miscellaneous functions This page describes the conditional and miscellaneous functions supported in Firebolt. CASE The CASE expression is a conditional expression similar to if-then-else statements.\\ If the result of the condition is true then the value of the CASE expression is the result that follows the condition. \u200b If the result is false any subsequent WHEN clauses (conditions) are searched in the same manner. \u200b If no WHEN condition is true then the value of the case expression is the result specified in the ELSE clause. \u200b If the ELSE clause is omitted and no condition matches, the result is null. Syntax CASE WHEN <condition> THEN <result> [WHEN ...n] [ELSE <result>] END; Parameter Description <condition> An expression that returns a boolean result. \u200b A condition can be defined for each WHEN, and ELSE clause. <result> The result of any condition. Every \u200b THEN \u200b\u200bclause receives a single result. All results in a single \u200b CASE \u200b\u200bfunction must share the same data type. Example This example references a table Movie_test with the following columns and values: Movie Length Citizen Kane 114 Happy Gilmore 82 Silence of the Lambs 110 The Godfather 150 The Jazz Singer 40 Tropic Thunder 90 The following example categorizes each entry by length. If the movie is longer than zero minutes and less than 50 minutes it is categorized as SHORT. When the length is 50-120 minutes, it's categorized as Medium, and when even longer, it's categorized as Long. SELECT Movie, length, CASE WHEN length > 0 AND length <= 50 THEN 'Short' WHEN length > 50 AND length <= 120 THEN 'Medium' WHEN length > 120 THEN 'Long' END duration FROM movie_test ORDER BY Movie; Returns: +----------------------+--------+----------+ | Title | Length | duration | +----------------------+--------+----------+ | Citizen Kane | 114 | Medium | | Happy Gilmore | 82 | Medium | | Silence of the Lambs | 110 | Medium | | The Godfather | 150 | Long | | The Jazz Singer | 40 | Short | | Tropic Thunder | 90 | Medium | +----------------------+--------+----------+ CAST Similar to TRY_CAST , CAST converts data types into other data types based on the specified parameters. If the conversion cannot be performed, CAST returns an error. To return a NULL value instead, use TRY_CAST . Syntax CAST(<value> AS <type>) Parameter Description <value> The value to convert or an expression that results in a value to convert. Can be a column name, \u200b \u200ba function applied to a column or another function, or a literal value. <type> The target data type (case-insensitive). Example SELECT CAST('1' AS INT) as res; Returns: 1 CAST can also be done by using the :: operator CITY_HASH Takes one or more input parameters of any data type and returns a 64-bit non-cryptographic hash value. CITY_HASH uses the CityHash algorithm for string data types, implementation-specific algorithms for other data types, and the CityHash combinator to produce the resulting hash value. Syntax CITY_HASH(<exp>, [, expr2 [,...]]) Parameter Description <exp> An expression that returns any data type that Firebolt supports. Example SELECT CITY_HASH('15', 'apple', '02-25-1918') Returns: 2383463095444788470 COALESCE Checks from left to right for the first non-NULL argument found for each entry parameter pair. For example, for an Employee table (where each employee can have more than one location), check multiple location parameters, find the first non-null pair per employee (the first location with data per employee). Syntax \u200b\u200bCOALESCE(<value> [,...]) Parameter Description <value> The value(s) to coalesce. Can be either: column name, \u200b \u200ba function applied on a column (or on another function), and a literal (constant value). Example SELECT COALESCE(null, 'London','New York') AS res; Returns: London NULL_IF Alias for NULLIF . NULLIF Compares two values. Returns null if the values are equal and returns the first value if they are not equal. Syntax NULLIF(<exp1>, <exp2>) Parameter Description <expr1> , <expr2> Expressions that evaluate to any data type that Firebolt supports. The expressions must evaluate to the same data type or synonyms, or an error occurs. exp1 is the value returned if the expressions do not evaluate to an equal result. Example NULLIF('Firebolt fast','Firebolt fast') Returns : null NULLIF('Firebolt fast','Firebolt Fast') Returns : Firebolt fast TRY_CAST Similar to CAST , TRY_CAST converts data types into other data types based on the specified parameters. If the conversion cannot be performed, TRY_CAST returns a NULL . To return an error message instead, use CAST . Syntax TRY_CAST(<value> AS <type>) Parameter Description <value> The value to convert or an expression that results in a value to convert. Can be a column name, \u200b \u200ba function applied to a column or another function, or a literal value. <type> The target data type (case-insensitive). Example SELECT TRY_CAST('1' AS INT) as res, TRY_CAST('test' AS INT) as res1; Returns : 1,null","title":"Conditional and miscellaneous functions"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#conditional-and-miscellaneous-functions","text":"This page describes the conditional and miscellaneous functions supported in Firebolt.","title":"Conditional and miscellaneous functions"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#case","text":"The CASE expression is a conditional expression similar to if-then-else statements.\\ If the result of the condition is true then the value of the CASE expression is the result that follows the condition. \u200b If the result is false any subsequent WHEN clauses (conditions) are searched in the same manner. \u200b If no WHEN condition is true then the value of the case expression is the result specified in the ELSE clause. \u200b If the ELSE clause is omitted and no condition matches, the result is null. Syntax CASE WHEN <condition> THEN <result> [WHEN ...n] [ELSE <result>] END; Parameter Description <condition> An expression that returns a boolean result. \u200b A condition can be defined for each WHEN, and ELSE clause. <result> The result of any condition. Every \u200b THEN \u200b\u200bclause receives a single result. All results in a single \u200b CASE \u200b\u200bfunction must share the same data type. Example This example references a table Movie_test with the following columns and values: Movie Length Citizen Kane 114 Happy Gilmore 82 Silence of the Lambs 110 The Godfather 150 The Jazz Singer 40 Tropic Thunder 90 The following example categorizes each entry by length. If the movie is longer than zero minutes and less than 50 minutes it is categorized as SHORT. When the length is 50-120 minutes, it's categorized as Medium, and when even longer, it's categorized as Long. SELECT Movie, length, CASE WHEN length > 0 AND length <= 50 THEN 'Short' WHEN length > 50 AND length <= 120 THEN 'Medium' WHEN length > 120 THEN 'Long' END duration FROM movie_test ORDER BY Movie; Returns: +----------------------+--------+----------+ | Title | Length | duration | +----------------------+--------+----------+ | Citizen Kane | 114 | Medium | | Happy Gilmore | 82 | Medium | | Silence of the Lambs | 110 | Medium | | The Godfather | 150 | Long | | The Jazz Singer | 40 | Short | | Tropic Thunder | 90 | Medium | +----------------------+--------+----------+","title":"CASE"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#cast","text":"Similar to TRY_CAST , CAST converts data types into other data types based on the specified parameters. If the conversion cannot be performed, CAST returns an error. To return a NULL value instead, use TRY_CAST . Syntax CAST(<value> AS <type>) Parameter Description <value> The value to convert or an expression that results in a value to convert. Can be a column name, \u200b \u200ba function applied to a column or another function, or a literal value. <type> The target data type (case-insensitive). Example SELECT CAST('1' AS INT) as res; Returns: 1 CAST can also be done by using the :: operator","title":"CAST"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#city_hash","text":"Takes one or more input parameters of any data type and returns a 64-bit non-cryptographic hash value. CITY_HASH uses the CityHash algorithm for string data types, implementation-specific algorithms for other data types, and the CityHash combinator to produce the resulting hash value. Syntax CITY_HASH(<exp>, [, expr2 [,...]]) Parameter Description <exp> An expression that returns any data type that Firebolt supports. Example SELECT CITY_HASH('15', 'apple', '02-25-1918') Returns: 2383463095444788470","title":"CITY_HASH"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#coalesce","text":"Checks from left to right for the first non-NULL argument found for each entry parameter pair. For example, for an Employee table (where each employee can have more than one location), check multiple location parameters, find the first non-null pair per employee (the first location with data per employee). Syntax \u200b\u200bCOALESCE(<value> [,...]) Parameter Description <value> The value(s) to coalesce. Can be either: column name, \u200b \u200ba function applied on a column (or on another function), and a literal (constant value). Example SELECT COALESCE(null, 'London','New York') AS res; Returns: London","title":"COALESCE"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#null_if","text":"Alias for NULLIF .","title":"NULL_IF"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#nullif","text":"Compares two values. Returns null if the values are equal and returns the first value if they are not equal. Syntax NULLIF(<exp1>, <exp2>) Parameter Description <expr1> , <expr2> Expressions that evaluate to any data type that Firebolt supports. The expressions must evaluate to the same data type or synonyms, or an error occurs. exp1 is the value returned if the expressions do not evaluate to an equal result. Example NULLIF('Firebolt fast','Firebolt fast') Returns : null NULLIF('Firebolt fast','Firebolt Fast') Returns : Firebolt fast","title":"NULLIF"},{"location":"sql-reference/functions-reference/conditional-and-miscellaneous-functions/#try_cast","text":"Similar to CAST , TRY_CAST converts data types into other data types based on the specified parameters. If the conversion cannot be performed, TRY_CAST returns a NULL . To return an error message instead, use CAST . Syntax TRY_CAST(<value> AS <type>) Parameter Description <value> The value to convert or an expression that results in a value to convert. Can be a column name, \u200b \u200ba function applied to a column or another function, or a literal value. <type> The target data type (case-insensitive). Example SELECT TRY_CAST('1' AS INT) as res, TRY_CAST('test' AS INT) as res1; Returns : 1,null","title":"TRY_CAST"},{"location":"sql-reference/functions-reference/date-and-time-functions/","text":"Date and time functions This page describes the date and time functions and format expressions supported in Firebolt. CURRENT_DATE Returns the current year, month and day as a DATE value, formatted as YYYY-MM-DD. Syntax \u200b\u200bCURRENT_DATE()\u200b\u200b Example SELECT CURRENT_DATE(); Returns: 2021-11-04 DATE_ADD Calculates a new DATE or TIMESTAMP by adding or subtracting a specified number of time units from an indicated expression. Syntax \u200b\u200bDATE_ADD('<unit>', <interval>, <date_expr>)\u200b\u200b Parameter Description <unit> A unit of time. This can be any of the following: SECOND <interval> The number of times to increase the \u200b <date_expr>\u200b\u200b by the time unit specified by <unit> . This can be a negative number. <date_expr> An expression that evaluates to a DATE or TIMESTAMP value. Examples The example below uses a table date_test with the columns and values below. Category sale_date a 2012-05-01 b 2021-08-30 c 1999-12-31 This example below adds 15 weeks to the sale_date column. SELECT Category, DATE_ADD('WEEK', 15, sale_date) FROM date_test Returns : +---+------------+ | a | 2012-08-14 | | b | 2021-12-13 | | c | 2000-04-14 | +---+------------+ This example below subtracts 6 months from a given start date string. This string representation of a date first needs to be transformed to DATE type using the CAST function. SELECT DATE_ADD('MONTH', -6, CAST ('2021-11-04' AS DATE)); Returns : 2021-05-04 DATE_DIFF Calculates the difference between \u200b\u200b start_date \u200b\u200b and \u200b end_date \u200b\u200b by the indicated \u200bunit\u200b\u200b. Syntax \u200b\u200bDATE_DIFF('<unit>', <start_date>, <end_date>)\u200b\u200b Parameter Description <unit> A unit of time. This can be any of the following: SECOND <start_date> An expression that evaluates to a DATE or TIMESTAMP value. <end_date> An expression that evaluates to a DATE or TIMESTAMP value. Examples The example below uses a table date_test with the columns and values below. Category sale_date sale_datetime a 2012-05-01 2017-06-15 09:34:21 b 2021-08-30 2014-01-15 12:14:46 c 1999-12-31 1999-09-15 11:33:21 SELECT Category, DATE_DIFF('YEAR', sale_date, sale_datetime) AS year_difference FROM date_test Returns : +----------+-----------------+ | Category | year_difference | | a | 5 | | b | -7 | | c | 0 | +----------+-----------------+ This example below finds the number of days difference between two date strings. The strings first need to be transformed to TIMESTAMP type using the CAST function. SELECT DATE_DIFF( 'day', CAST('2020/08/31 10:00:00' AS TIMESTAMP), CAST('2020/08/31 11:00:00' AS TIMESTAMP) ); Returns : 0 DATE_FORMAT Formats a \u200b DATE \u200b\u200bor \u200b DATETIME \u200b\u200baccording to the given format expression. Syntax \u200b\u200bDATE_FORMAT(<date>, '<format>')\u200b\u200b Parameter Description <date> The date to be formatted. <format> The format to be used for the output using the syntax shown. The reference table below lists allowed expressions and provides example output of each expression for a given date and time. Expression for \\ Description Expression output for Tuesday the 2nd of April, 1975 at 12:24:48:13 past midnight %C The year divided by 100 and truncated to integer (00-99) 19 %d Day of the month, zero-padded (01-31) 02 %D Short MM/DD/YY date, equivalent to %m/%d/%y 04/02/75 %e Day of the month, space-padded ( 1-31) 2 %F Short YYYY-MM-DD date, equivalent to %Y-%m-%d 1975-04-02 %H The hour in 24h format (00-23) 00 %I The hour in 12h format (01-12) 12 %j Day of the year (001-366) 112 %m Month as a decimal number (01-12) 04 %M Minute (00-59) 24 %n New-line character (\u2018\u2019) in order to add a new line in the converted format. For example, %Y%n%m returns 1975 04 %p AM or PM designation PM %R 24-hour HH:MM time, equivalent to %H:%M 00:24 %S The second (00-59) 48 %T ISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S 00:24:48 %u ISO 8601 weekday as number with Monday as 1 (1-7) 2 %V ISO 8601 week number (01-53) 17 %w weekday as a decimal number with Sunday as 0 (0-6) 2 %y Year, last two digits (00-99) 75 %Y Year 1975 %% Escape character to use a % sign % Example The examples below use a table date_test with the columns and values below. The following examples use these TIMESTAMP values to demonstrate the various DATE_FORMAT expressions. Category sale_datetime a 2017-06-15 09:34:21 b 2014-01-15 12:14:46 c 1999-09-15 11:33:21 The example below shows output for <format> expressions %C, %d, %D, %e, %F, %H, %I SELECT Category, DATE_FORMAT(sale_datetime, '%C') AS C, DATE_FORMAT(sale_datetime, '%d') AS d, DATE_FORMAT(sale_datetime, '%D') AS D, DATE_FORMAT(sale_datetime, '%e') AS e, DATE_FORMAT(sale_datetime, '%F') AS F, DATE_FORMAT(sale_datetime, '%H') AS H, DATE_FORMAT(sale_datetime, '%I') AS I FROM date_test ORDER BY Category Returns: +----------+---------------------+----+----+----------+----+------------+----+----+ | Category | sale_datetime | C | d | D | e | F | H | I | | a | 2017-06-15 09:34:21 | 20 | 15 | 06/15/17 | 15 | 2017-06-15 | 09 | 09 | | b | 2014-01-15 12:14:46 | 20 | 15 | 01/15/14 | 15 | 2014-01-15 | 12 | 12 | | c | 1999-09-15 11:33:21 | 19 | 15 | 09/15/99 | 15 | 1999-09-15 | 11 | 11 | +----------+---------------------+----+----+----------+----+------------+----+----+ The example below shows output for <format> expressions %j, %m, %M, %p, %R, %S SELECT Category, sale_datetime, DATE_FORMAT(sale_datetime, '%j') AS j, DATE_FORMAT(sale_datetime, '%m') AS m, DATE_FORMAT(sale_datetime, '%M') AS M, DATE_FORMAT(sale_datetime, '%p') AS p, DATE_FORMAT(sale_datetime, '%R') AS R, DATE_FORMAT(sale_datetime, '%S') AS S FROM date_test ORDER BY Category Returns: +----------+---------------------+-----+----+----+----+-------+----+ | Category | sale_datetime | j | m | M | p | R | S | | a | 2017-06-15 09:34:21 | 166 | 06 | 34 | AM | 09:34 | 21 | | b | 2014-01-15 12:14:46 | 015 | 01 | 14 | PM | 12:14 | 46 | | c | 1999-09-15 11:33:21 | 258 | 09 | 33 | AM | 11:33 | 21 | +----------+---------------------+-----+----+----+----+-------+----+ The example below shows output for <format> expressions %T, %u, %V, %w, %y, %Y, %% SELECT Category, sale_datetime, DATE_FORMAT(sale_datetime, '%T') AS T, DATE_FORMAT(sale_datetime, '%u') AS u, DATE_FORMAT(sale_datetime, '%V') AS V, DATE_FORMAT(sale_datetime, '%w') AS w, DATE_FORMAT(sale_datetime, '%y') AS y, DATE_FORMAT(sale_datetime, '%Y') AS Y, DATE_FORMAT(sale_datetime, '%%') AS percent FROM date_test ORDER BY Category Returns: +----------+---------------------+----------+---+----+---+----+------+---------+ | Category | sale_datetime | T | u | V | w | y | Y | percent | | a | 2017-06-15 09:34:21 | 09:34:21 | 4 | 24 | 4 | 17 | 2017 | % | | b | 2014-01-15 12:14:46 | 12:14:46 | 3 | 03 | 3 | 14 | 2014 | % | | c | 1999-09-15 11:33:21 | 11:33:21 | 3 | 37 | 3 | 99 | 1999 | % | +----------+---------------------+----------+---+----+---+----+------+---------+ DATE_TRUNC Truncate a given date to a specified position. Syntax \u200b\u200bDATE_TRUNC('<precision>', <date>)\u200b\u200b Parameter Description <precision> The time unit for the returned value to be expressed. \u200b This can be any of the following: SECOND <date> The date to be truncated. This can be any expression that evaluates to a DATE or TIMESTAMP value. Example The example below uses a table date_test with the columns and values below. Category sale_datetime a 2017-06-15 09:34:21 b 2014-01-15 12:14:46 c 1999-09-15 11:33:21 SELECT Category, sale_datetime, DATE_TRUNC('MINUTE', sale_datetime) AS MINUTE, DATE_TRUNC('HOUR', sale_datetime) AS HOUR, DATE_TRUNC('DAY', sale_datetime) AS DAY FROM date_test ORDER BY Category Returns : +----------+---------------------+---------------------+---------------------+---------------------+ | Category | sale_datetime | MINUTE | HOUR | DAY | | a | 2017-06-15 09:34:21 | 2017-06-15 09:34:00 | 2017-06-15 09:00:00 | 2017-06-15 00:00:00 | | b | 2014-01-15 12:14:46 | 2014-01-15 12:14:00 | 2014-01-15 12:00:00 | 2014-01-15 00:00:00 | | c | 1999-09-15 11:33:21 | 1999-09-15 11:33:00 | 1999-09-15 11:00:00 | 1999-09-15 00:00:00 | +----------+---------------------+---------------------+---------------------+---------------------+ EXTRACT Retrieves subfields such as year or hour from date/time values. Syntax \u200b\u200bEXTRACT(<field> FROM <source>)\u200b\u200b Parameter Description <field> Supported fields: DAY , DOW, MONTH , WEEK , QUARTER , YEAR , HOUR , MINUTE , SECOND , EPOCH <source> A value expression of type timestamp. Example This example below extracts the year from the timestamp. The string date first need to be transformed to TIMESTAMP type using the CAST function. SELECT EXTRACT( YEAR FROM CAST('2020-01-01 10:00:00' AS TIMESTAMP) ) Returns : 2020 FROM_UNIXTIME Convert Unix time ( LONG in epoch seconds) to DATETIME (YYYY-MM-DD HH:mm:ss). Syntax \u200b\u200bFROM_UNIXTIME(<unix_time>)\u200b\u200b Parameter Description <unix_time> The UNIX epoch time that is to be converted. Example SELECT FROM_UNIXTIME(1493971667); Returns : 2017-05-05 08:07:47 NOW Returns the current date and time. Syntax \u200b\u200bNOW()\u200b\u200b Example SELECT NOW() Returns : 2021-11-04 20:42:54 TIMEZONE Returns the current timezone of the request execution Syntax \u200b\u200bTIMEZONE()\u200b\u200b Example SELECT TIMEZONE() Returns : Etc/UTC TO_DAY_OF_WEEK Converts a date or timestamp to a number representing the day of the week (Monday is 1, and Sunday is 7). Syntax \u200b\u200bTO_DAY_OF_WEEK(<date>)\u200b\u200b Parameter Description <date> An expression that evaluates to a DATE or TIMESTAMP Example This example below finds the day of week number for April 22, 1975. The string first needs to be transformed to DATE type using the CAST function. SELECT TO_DAY_OF_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns : 2 TO_DAY_OF_YEAR Converts a date or timestamp to a number containing the number for the day of the year. Syntax \u200b\u200bTO_DAY_OF_YEAR(<date>)\u200b\u200b Parameter Description <date> An expression that evaluates to a DATE or TIMESTAMP Example This example below finds the day of the year number for April 22, 1975. The string first needs to be transformed to DATE type using the CAST function. SELECT TO_DAY_OF_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns : 112 TO_HOUR Converts a date or timestamp to a number containing the hour. Syntax \u200b\u200bTO_HOUR(<timestamp>)\u200b\u200b Parameter Description <timestamp> The timestamp to be converted into the number of the hour. Example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_HOUR(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 12 TO_MINUTE Converts a timestamp (any date format we support) to a number containing the minute. Syntax \u200b\u200bTO_MINUTE(<timestamp>)\u200b\u200b Parameter Description <timestamp> The timestamp to be converted into the number of the minute. Example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_MINUTE(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 20 TO_MONTH Converts a date or timestamp (any date format we support) to a number containing the month. Syntax \u200b\u200bTO_MONTH(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the month. Example For Tuesday, April 22, 1975: SELECT TO_MONTH(CAST('1975/04/22' AS DATE)) as res; Returns: 4 TO_QUARTER Converts a date or timestamp (any date format we support) to a number containing the quarter. Syntax \u200b\u200bTO_QUARTER(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the quarter. Example For Tuesday, April 22, 1975: SELECT TO_QUARTER(CAST('1975/04/22' AS DATE)) as res; Returns: 2 TO_SECOND Converts a timestamp (any date format we support) to a number containing the second. Syntax \u200b\u200bTO_SECOND(<timestamp>)\u200b\u200b Parameter Description <timestamp> The timestamp to be converted into the number of the second. Example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_SECOND(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 5 TO_STRING Converts a date into a STRING. The date is any date data type\u200b\u200b . Syntax TO_STRING(<date>) Parameter Description <date> The date to be converted to a string. Example This que r y returns today's date and the current time. SELECT TO_STRING(NOW()); Returns : \u200b 2022-10-10 22:22:33 TO_WEEK Converts a date or timestamp (any date format we support) to a number containing the week. Syntax \u200b\u200bTO_WEEK(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the week. Example For Tuesday, April 22, 1975: SELECT TO_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns: 16 TO_YEAR Converts a date or timestamp (any date format we support) to a number containing the year. Syntax \u200b\u200bTO_YEAR(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the year. Example For Tuesday, April 22, 1975: SELECT TO_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns: 1975","title":"Date and time functions"},{"location":"sql-reference/functions-reference/date-and-time-functions/#date-and-time-functions","text":"This page describes the date and time functions and format expressions supported in Firebolt.","title":"Date and time functions"},{"location":"sql-reference/functions-reference/date-and-time-functions/#current_date","text":"Returns the current year, month and day as a DATE value, formatted as YYYY-MM-DD. Syntax \u200b\u200bCURRENT_DATE()\u200b\u200b Example SELECT CURRENT_DATE(); Returns: 2021-11-04","title":"CURRENT_DATE"},{"location":"sql-reference/functions-reference/date-and-time-functions/#date_add","text":"Calculates a new DATE or TIMESTAMP by adding or subtracting a specified number of time units from an indicated expression. Syntax \u200b\u200bDATE_ADD('<unit>', <interval>, <date_expr>)\u200b\u200b Parameter Description <unit> A unit of time. This can be any of the following: SECOND <interval> The number of times to increase the \u200b <date_expr>\u200b\u200b by the time unit specified by <unit> . This can be a negative number. <date_expr> An expression that evaluates to a DATE or TIMESTAMP value. Examples The example below uses a table date_test with the columns and values below. Category sale_date a 2012-05-01 b 2021-08-30 c 1999-12-31 This example below adds 15 weeks to the sale_date column. SELECT Category, DATE_ADD('WEEK', 15, sale_date) FROM date_test Returns : +---+------------+ | a | 2012-08-14 | | b | 2021-12-13 | | c | 2000-04-14 | +---+------------+ This example below subtracts 6 months from a given start date string. This string representation of a date first needs to be transformed to DATE type using the CAST function. SELECT DATE_ADD('MONTH', -6, CAST ('2021-11-04' AS DATE)); Returns : 2021-05-04","title":"DATE_ADD"},{"location":"sql-reference/functions-reference/date-and-time-functions/#date_diff","text":"Calculates the difference between \u200b\u200b start_date \u200b\u200b and \u200b end_date \u200b\u200b by the indicated \u200bunit\u200b\u200b. Syntax \u200b\u200bDATE_DIFF('<unit>', <start_date>, <end_date>)\u200b\u200b Parameter Description <unit> A unit of time. This can be any of the following: SECOND <start_date> An expression that evaluates to a DATE or TIMESTAMP value. <end_date> An expression that evaluates to a DATE or TIMESTAMP value. Examples The example below uses a table date_test with the columns and values below. Category sale_date sale_datetime a 2012-05-01 2017-06-15 09:34:21 b 2021-08-30 2014-01-15 12:14:46 c 1999-12-31 1999-09-15 11:33:21 SELECT Category, DATE_DIFF('YEAR', sale_date, sale_datetime) AS year_difference FROM date_test Returns : +----------+-----------------+ | Category | year_difference | | a | 5 | | b | -7 | | c | 0 | +----------+-----------------+ This example below finds the number of days difference between two date strings. The strings first need to be transformed to TIMESTAMP type using the CAST function. SELECT DATE_DIFF( 'day', CAST('2020/08/31 10:00:00' AS TIMESTAMP), CAST('2020/08/31 11:00:00' AS TIMESTAMP) ); Returns : 0","title":"DATE_DIFF"},{"location":"sql-reference/functions-reference/date-and-time-functions/#date_format","text":"Formats a \u200b DATE \u200b\u200bor \u200b DATETIME \u200b\u200baccording to the given format expression. Syntax \u200b\u200bDATE_FORMAT(<date>, '<format>')\u200b\u200b Parameter Description <date> The date to be formatted. <format> The format to be used for the output using the syntax shown. The reference table below lists allowed expressions and provides example output of each expression for a given date and time. Expression for \\ Description Expression output for Tuesday the 2nd of April, 1975 at 12:24:48:13 past midnight %C The year divided by 100 and truncated to integer (00-99) 19 %d Day of the month, zero-padded (01-31) 02 %D Short MM/DD/YY date, equivalent to %m/%d/%y 04/02/75 %e Day of the month, space-padded ( 1-31) 2 %F Short YYYY-MM-DD date, equivalent to %Y-%m-%d 1975-04-02 %H The hour in 24h format (00-23) 00 %I The hour in 12h format (01-12) 12 %j Day of the year (001-366) 112 %m Month as a decimal number (01-12) 04 %M Minute (00-59) 24 %n New-line character (\u2018\u2019) in order to add a new line in the converted format. For example, %Y%n%m returns 1975 04 %p AM or PM designation PM %R 24-hour HH:MM time, equivalent to %H:%M 00:24 %S The second (00-59) 48 %T ISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S 00:24:48 %u ISO 8601 weekday as number with Monday as 1 (1-7) 2 %V ISO 8601 week number (01-53) 17 %w weekday as a decimal number with Sunday as 0 (0-6) 2 %y Year, last two digits (00-99) 75 %Y Year 1975 %% Escape character to use a % sign % Example The examples below use a table date_test with the columns and values below. The following examples use these TIMESTAMP values to demonstrate the various DATE_FORMAT expressions. Category sale_datetime a 2017-06-15 09:34:21 b 2014-01-15 12:14:46 c 1999-09-15 11:33:21 The example below shows output for <format> expressions %C, %d, %D, %e, %F, %H, %I SELECT Category, DATE_FORMAT(sale_datetime, '%C') AS C, DATE_FORMAT(sale_datetime, '%d') AS d, DATE_FORMAT(sale_datetime, '%D') AS D, DATE_FORMAT(sale_datetime, '%e') AS e, DATE_FORMAT(sale_datetime, '%F') AS F, DATE_FORMAT(sale_datetime, '%H') AS H, DATE_FORMAT(sale_datetime, '%I') AS I FROM date_test ORDER BY Category Returns: +----------+---------------------+----+----+----------+----+------------+----+----+ | Category | sale_datetime | C | d | D | e | F | H | I | | a | 2017-06-15 09:34:21 | 20 | 15 | 06/15/17 | 15 | 2017-06-15 | 09 | 09 | | b | 2014-01-15 12:14:46 | 20 | 15 | 01/15/14 | 15 | 2014-01-15 | 12 | 12 | | c | 1999-09-15 11:33:21 | 19 | 15 | 09/15/99 | 15 | 1999-09-15 | 11 | 11 | +----------+---------------------+----+----+----------+----+------------+----+----+ The example below shows output for <format> expressions %j, %m, %M, %p, %R, %S SELECT Category, sale_datetime, DATE_FORMAT(sale_datetime, '%j') AS j, DATE_FORMAT(sale_datetime, '%m') AS m, DATE_FORMAT(sale_datetime, '%M') AS M, DATE_FORMAT(sale_datetime, '%p') AS p, DATE_FORMAT(sale_datetime, '%R') AS R, DATE_FORMAT(sale_datetime, '%S') AS S FROM date_test ORDER BY Category Returns: +----------+---------------------+-----+----+----+----+-------+----+ | Category | sale_datetime | j | m | M | p | R | S | | a | 2017-06-15 09:34:21 | 166 | 06 | 34 | AM | 09:34 | 21 | | b | 2014-01-15 12:14:46 | 015 | 01 | 14 | PM | 12:14 | 46 | | c | 1999-09-15 11:33:21 | 258 | 09 | 33 | AM | 11:33 | 21 | +----------+---------------------+-----+----+----+----+-------+----+ The example below shows output for <format> expressions %T, %u, %V, %w, %y, %Y, %% SELECT Category, sale_datetime, DATE_FORMAT(sale_datetime, '%T') AS T, DATE_FORMAT(sale_datetime, '%u') AS u, DATE_FORMAT(sale_datetime, '%V') AS V, DATE_FORMAT(sale_datetime, '%w') AS w, DATE_FORMAT(sale_datetime, '%y') AS y, DATE_FORMAT(sale_datetime, '%Y') AS Y, DATE_FORMAT(sale_datetime, '%%') AS percent FROM date_test ORDER BY Category Returns: +----------+---------------------+----------+---+----+---+----+------+---------+ | Category | sale_datetime | T | u | V | w | y | Y | percent | | a | 2017-06-15 09:34:21 | 09:34:21 | 4 | 24 | 4 | 17 | 2017 | % | | b | 2014-01-15 12:14:46 | 12:14:46 | 3 | 03 | 3 | 14 | 2014 | % | | c | 1999-09-15 11:33:21 | 11:33:21 | 3 | 37 | 3 | 99 | 1999 | % | +----------+---------------------+----------+---+----+---+----+------+---------+","title":"DATE_FORMAT"},{"location":"sql-reference/functions-reference/date-and-time-functions/#date_trunc","text":"Truncate a given date to a specified position. Syntax \u200b\u200bDATE_TRUNC('<precision>', <date>)\u200b\u200b Parameter Description <precision> The time unit for the returned value to be expressed. \u200b This can be any of the following: SECOND <date> The date to be truncated. This can be any expression that evaluates to a DATE or TIMESTAMP value. Example The example below uses a table date_test with the columns and values below. Category sale_datetime a 2017-06-15 09:34:21 b 2014-01-15 12:14:46 c 1999-09-15 11:33:21 SELECT Category, sale_datetime, DATE_TRUNC('MINUTE', sale_datetime) AS MINUTE, DATE_TRUNC('HOUR', sale_datetime) AS HOUR, DATE_TRUNC('DAY', sale_datetime) AS DAY FROM date_test ORDER BY Category Returns : +----------+---------------------+---------------------+---------------------+---------------------+ | Category | sale_datetime | MINUTE | HOUR | DAY | | a | 2017-06-15 09:34:21 | 2017-06-15 09:34:00 | 2017-06-15 09:00:00 | 2017-06-15 00:00:00 | | b | 2014-01-15 12:14:46 | 2014-01-15 12:14:00 | 2014-01-15 12:00:00 | 2014-01-15 00:00:00 | | c | 1999-09-15 11:33:21 | 1999-09-15 11:33:00 | 1999-09-15 11:00:00 | 1999-09-15 00:00:00 | +----------+---------------------+---------------------+---------------------+---------------------+","title":"DATE_TRUNC"},{"location":"sql-reference/functions-reference/date-and-time-functions/#extract","text":"Retrieves subfields such as year or hour from date/time values. Syntax \u200b\u200bEXTRACT(<field> FROM <source>)\u200b\u200b Parameter Description <field> Supported fields: DAY , DOW, MONTH , WEEK , QUARTER , YEAR , HOUR , MINUTE , SECOND , EPOCH <source> A value expression of type timestamp. Example This example below extracts the year from the timestamp. The string date first need to be transformed to TIMESTAMP type using the CAST function. SELECT EXTRACT( YEAR FROM CAST('2020-01-01 10:00:00' AS TIMESTAMP) ) Returns : 2020","title":"EXTRACT"},{"location":"sql-reference/functions-reference/date-and-time-functions/#from_unixtime","text":"Convert Unix time ( LONG in epoch seconds) to DATETIME (YYYY-MM-DD HH:mm:ss). Syntax \u200b\u200bFROM_UNIXTIME(<unix_time>)\u200b\u200b Parameter Description <unix_time> The UNIX epoch time that is to be converted. Example SELECT FROM_UNIXTIME(1493971667); Returns : 2017-05-05 08:07:47","title":"FROM_UNIXTIME"},{"location":"sql-reference/functions-reference/date-and-time-functions/#now","text":"Returns the current date and time. Syntax \u200b\u200bNOW()\u200b\u200b Example SELECT NOW() Returns : 2021-11-04 20:42:54","title":"NOW"},{"location":"sql-reference/functions-reference/date-and-time-functions/#timezone","text":"Returns the current timezone of the request execution Syntax \u200b\u200bTIMEZONE()\u200b\u200b Example SELECT TIMEZONE() Returns : Etc/UTC","title":"TIMEZONE"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_day_of_week","text":"Converts a date or timestamp to a number representing the day of the week (Monday is 1, and Sunday is 7). Syntax \u200b\u200bTO_DAY_OF_WEEK(<date>)\u200b\u200b Parameter Description <date> An expression that evaluates to a DATE or TIMESTAMP Example This example below finds the day of week number for April 22, 1975. The string first needs to be transformed to DATE type using the CAST function. SELECT TO_DAY_OF_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns : 2","title":"TO_DAY_OF_WEEK"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_day_of_year","text":"Converts a date or timestamp to a number containing the number for the day of the year. Syntax \u200b\u200bTO_DAY_OF_YEAR(<date>)\u200b\u200b Parameter Description <date> An expression that evaluates to a DATE or TIMESTAMP Example This example below finds the day of the year number for April 22, 1975. The string first needs to be transformed to DATE type using the CAST function. SELECT TO_DAY_OF_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns : 112","title":"TO_DAY_OF_YEAR"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_hour","text":"Converts a date or timestamp to a number containing the hour. Syntax \u200b\u200bTO_HOUR(<timestamp>)\u200b\u200b Parameter Description <timestamp> The timestamp to be converted into the number of the hour. Example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_HOUR(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 12","title":"TO_HOUR"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_minute","text":"Converts a timestamp (any date format we support) to a number containing the minute. Syntax \u200b\u200bTO_MINUTE(<timestamp>)\u200b\u200b Parameter Description <timestamp> The timestamp to be converted into the number of the minute. Example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_MINUTE(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 20","title":"TO_MINUTE"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_month","text":"Converts a date or timestamp (any date format we support) to a number containing the month. Syntax \u200b\u200bTO_MONTH(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the month. Example For Tuesday, April 22, 1975: SELECT TO_MONTH(CAST('1975/04/22' AS DATE)) as res; Returns: 4","title":"TO_MONTH"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_quarter","text":"Converts a date or timestamp (any date format we support) to a number containing the quarter. Syntax \u200b\u200bTO_QUARTER(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the quarter. Example For Tuesday, April 22, 1975: SELECT TO_QUARTER(CAST('1975/04/22' AS DATE)) as res; Returns: 2","title":"TO_QUARTER"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_second","text":"Converts a timestamp (any date format we support) to a number containing the second. Syntax \u200b\u200bTO_SECOND(<timestamp>)\u200b\u200b Parameter Description <timestamp> The timestamp to be converted into the number of the second. Example For Tuesday, April 22, 1975 at 12:20:05: SELECT TO_SECOND(CAST('1975/04/22 12:20:05' AS TIMESTAMP)) as res; Returns: 5","title":"TO_SECOND"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_string","text":"Converts a date into a STRING. The date is any date data type\u200b\u200b . Syntax TO_STRING(<date>) Parameter Description <date> The date to be converted to a string. Example This que r y returns today's date and the current time. SELECT TO_STRING(NOW()); Returns : \u200b 2022-10-10 22:22:33","title":"TO_STRING"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_week","text":"Converts a date or timestamp (any date format we support) to a number containing the week. Syntax \u200b\u200bTO_WEEK(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the week. Example For Tuesday, April 22, 1975: SELECT TO_WEEK(CAST('1975/04/22' AS DATE)) as res; Returns: 16","title":"TO_WEEK"},{"location":"sql-reference/functions-reference/date-and-time-functions/#to_year","text":"Converts a date or timestamp (any date format we support) to a number containing the year. Syntax \u200b\u200bTO_YEAR(<date>)\u200b\u200b Parameter Description <date> The date or timestamp to be converted into the number of the year. Example For Tuesday, April 22, 1975: SELECT TO_YEAR(CAST('1975/04/22' AS DATE)) as res; Returns: 1975","title":"TO_YEAR"},{"location":"sql-reference/functions-reference/numeric-functions/","text":"Numeric functions This page describes the numeric functions supported in Firebolt. ABS Calculates the absolute value of a number <val> . Syntax ABS(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ABS(-200.50) Returns: 200.5 ACOS Calculates the arc cosine of a value <val> . ACOS returns NULL if <val> is higher than 1. Syntax ACOS(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ACOS(0.5) Returns: 1.0471975511965979 ASIN Calculates the arc sinus. ASIN returns NULL if <val> is higher than 1. Syntax ASIN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ASIN(1.0) Returns: 1.5707963267948966 ATAN Calculates the arctangent. Syntax ATAN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ATAN(90) Returns : 1.5596856728972892 CBRT Returns the cubic-root of a non-negative numeric expression. Syntax CBRT(<val>); Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT CBRT(8); Returns : 2 CEIL, CEILING Returns the smallest number that is greater than or equal to a specified value <val> . The value is rounded to a decimal range defined by <dec> . Syntax CEIL(<val>[, <dec>]); CEILING(<val>[, <dec>]); Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, CEIL and CEILING return whole numbers. Example SELECT CEIL(2.5549900, 3); Returns : 2.555 COS Calculates the cosine. Syntax COS(<exp>) Parameter Description <exp> Any expression that evaluates to a numeric data type. Example SELECT COS(180); Returns: -0.5984600690578581 COT Calculates the cotangent. Syntax COT(<exp>) Parameter Description <exp> Any expression that evaluates to a numeric data type. Example SELECT COT(180) Returns: 0.7469988144140444 DEGREES Converts a value in radians to degrees. Syntax DEGREES(<exp>) Parameter Description <exp> Any expression that evaluates to a numeric data type. Example SELECT DEGREES(3); Returns : 171.88733853924697 EXP Returns the FLOAT value of the constant e raised to the power of a specified number. Syntax EXP(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT EXP(2) Returns: 7.389056098924109 FLOOR Returns the largest round number that is less than or equal to <val> . The value is rounded to a decimal range defined by <dec> . Syntax FLOOR(<val>[, <dec>]) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, FLOOR returns whole numbers. Example SELECT FLOOR(2.19, 1) Returns: 2.1 LOG Returns the natural logarithm of a numeric expression based on the desired base. Syntax LOG([<base>,] <num>); Parameter Description <base> Optional. The base for the logarithm. The default base is 10. <numeric> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Examples This example below returns the logarithm of 64.0 with base 2. SELECT LOG(2, 64.0) Returns: 6 This example below returns the logarithm of 64.0 with the default base 10. SELECT LOG(64.0) Returns : 1.8061799739838869 MOD Calculates the remainder after dividing two values, <num> / <den>. Syntax MOD(<num>,<den>) Parameter Description <num> The numerator of the division equation. <den> The denominator of the division equation. Example SELECT MOD(45, 7) Returns : 3 PI Calculates \u03c0 as a FLOAT value. Syntax \u200b\u200bPI() \u200b\u200b Example SELECT PI() Returns: 3.141592653589793 POW, POWER Returns a number <val> raised to the specified power <exp> . Syntax POW(<val>, <exp>)\u200b; POWER(<val>, <exp>)\u200b; Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <exp> The exponent value used to raise <val> Example SELECT POW(2, 5) Returns: 32 RADIANS Converts degrees to radians as a FLOAT value. Syntax \u200b\u200bRADIANS(<val>) \u200b\u200b Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT RADIANS(180) Returns: 3.141592653589793 RANDOM Returns a pseudo-random unsigned value greater than 0 and less than 1 of type DOUBLE . Syntax RANDOM() Examples The example below demonstrates using RANDOM without any other numeric functions. This generates a DOUBLE value less than 1. SELECT RANDOM() Returns: 0.8544004706537051 Using RANDOM for range of values To create a random integer number between two values, you can use RANDOM with the FLOOR function as demonstrated below. a is the lesser value and b is the greater value. SELECT FLOOR( RANDOM() * ( b - a + 1)) + a; For example, the formula below generates a random integer between 50 and 100: SELECT FLOOR( RANDOM() * (100 - 50 + 1)) + 50; Returns: 61 ROUND Rounds a value to a specified number of decimal places. Syntax ROUND(<val> [, <dec>]) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, ROUND returns whole numbers. Examples SELECT ROUND(5.4) Returns: 5 SELECT ROUND(5.6930, 1) Returns: 5.7 SIN Calculates the sinus. Syntax SIN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT SIN(90) Returns: 0.8939966636005579 SQRT Returns the square root of a non-negative numeric expression. Syntax SQRT(<val>); Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Returns NULL if a negative value is given. Example SELECT SQRT(64) Returns: 8 TAN Calculates the tangent. Syntax TAN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT TAN(90) Returns: - 1.995200412208242 TRUNC Returns the rounded absolute value of a numeric value. The returned value will always be rounded to less than the original value. Syntax TRUNC(<val>[, <dec>]) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, TRUNC returns whole numbers. Examples SELECT TRUNC(-20.5) Returns: -20 SELECT TRUNC(-99.999999, 3) Returns: -99.999","title":"Numeric functions"},{"location":"sql-reference/functions-reference/numeric-functions/#numeric-functions","text":"This page describes the numeric functions supported in Firebolt.","title":"Numeric functions"},{"location":"sql-reference/functions-reference/numeric-functions/#abs","text":"Calculates the absolute value of a number <val> . Syntax ABS(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ABS(-200.50) Returns: 200.5","title":"ABS"},{"location":"sql-reference/functions-reference/numeric-functions/#acos","text":"Calculates the arc cosine of a value <val> . ACOS returns NULL if <val> is higher than 1. Syntax ACOS(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ACOS(0.5) Returns: 1.0471975511965979","title":"ACOS"},{"location":"sql-reference/functions-reference/numeric-functions/#asin","text":"Calculates the arc sinus. ASIN returns NULL if <val> is higher than 1. Syntax ASIN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ASIN(1.0) Returns: 1.5707963267948966","title":"ASIN"},{"location":"sql-reference/functions-reference/numeric-functions/#atan","text":"Calculates the arctangent. Syntax ATAN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT ATAN(90) Returns : 1.5596856728972892","title":"ATAN"},{"location":"sql-reference/functions-reference/numeric-functions/#cbrt","text":"Returns the cubic-root of a non-negative numeric expression. Syntax CBRT(<val>); Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT CBRT(8); Returns : 2","title":"CBRT"},{"location":"sql-reference/functions-reference/numeric-functions/#ceil-ceiling","text":"Returns the smallest number that is greater than or equal to a specified value <val> . The value is rounded to a decimal range defined by <dec> . Syntax CEIL(<val>[, <dec>]); CEILING(<val>[, <dec>]); Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, CEIL and CEILING return whole numbers. Example SELECT CEIL(2.5549900, 3); Returns : 2.555","title":"CEIL, CEILING"},{"location":"sql-reference/functions-reference/numeric-functions/#cos","text":"Calculates the cosine. Syntax COS(<exp>) Parameter Description <exp> Any expression that evaluates to a numeric data type. Example SELECT COS(180); Returns: -0.5984600690578581","title":"COS"},{"location":"sql-reference/functions-reference/numeric-functions/#cot","text":"Calculates the cotangent. Syntax COT(<exp>) Parameter Description <exp> Any expression that evaluates to a numeric data type. Example SELECT COT(180) Returns: 0.7469988144140444","title":"COT"},{"location":"sql-reference/functions-reference/numeric-functions/#degrees","text":"Converts a value in radians to degrees. Syntax DEGREES(<exp>) Parameter Description <exp> Any expression that evaluates to a numeric data type. Example SELECT DEGREES(3); Returns : 171.88733853924697","title":"DEGREES"},{"location":"sql-reference/functions-reference/numeric-functions/#exp","text":"Returns the FLOAT value of the constant e raised to the power of a specified number. Syntax EXP(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT EXP(2) Returns: 7.389056098924109","title":"EXP"},{"location":"sql-reference/functions-reference/numeric-functions/#floor","text":"Returns the largest round number that is less than or equal to <val> . The value is rounded to a decimal range defined by <dec> . Syntax FLOOR(<val>[, <dec>]) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, FLOOR returns whole numbers. Example SELECT FLOOR(2.19, 1) Returns: 2.1","title":"FLOOR"},{"location":"sql-reference/functions-reference/numeric-functions/#log","text":"Returns the natural logarithm of a numeric expression based on the desired base. Syntax LOG([<base>,] <num>); Parameter Description <base> Optional. The base for the logarithm. The default base is 10. <numeric> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Examples This example below returns the logarithm of 64.0 with base 2. SELECT LOG(2, 64.0) Returns: 6 This example below returns the logarithm of 64.0 with the default base 10. SELECT LOG(64.0) Returns : 1.8061799739838869","title":"LOG"},{"location":"sql-reference/functions-reference/numeric-functions/#mod","text":"Calculates the remainder after dividing two values, <num> / <den>. Syntax MOD(<num>,<den>) Parameter Description <num> The numerator of the division equation. <den> The denominator of the division equation. Example SELECT MOD(45, 7) Returns : 3","title":"MOD"},{"location":"sql-reference/functions-reference/numeric-functions/#pi","text":"Calculates \u03c0 as a FLOAT value. Syntax \u200b\u200bPI() \u200b\u200b Example SELECT PI() Returns: 3.141592653589793","title":"PI"},{"location":"sql-reference/functions-reference/numeric-functions/#pow-power","text":"Returns a number <val> raised to the specified power <exp> . Syntax POW(<val>, <exp>)\u200b; POWER(<val>, <exp>)\u200b; Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <exp> The exponent value used to raise <val> Example SELECT POW(2, 5) Returns: 32","title":"POW, POWER"},{"location":"sql-reference/functions-reference/numeric-functions/#radians","text":"Converts degrees to radians as a FLOAT value. Syntax \u200b\u200bRADIANS(<val>) \u200b\u200b Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT RADIANS(180) Returns: 3.141592653589793","title":"RADIANS"},{"location":"sql-reference/functions-reference/numeric-functions/#random","text":"Returns a pseudo-random unsigned value greater than 0 and less than 1 of type DOUBLE . Syntax RANDOM() Examples The example below demonstrates using RANDOM without any other numeric functions. This generates a DOUBLE value less than 1. SELECT RANDOM() Returns: 0.8544004706537051 Using RANDOM for range of values To create a random integer number between two values, you can use RANDOM with the FLOOR function as demonstrated below. a is the lesser value and b is the greater value. SELECT FLOOR( RANDOM() * ( b - a + 1)) + a; For example, the formula below generates a random integer between 50 and 100: SELECT FLOOR( RANDOM() * (100 - 50 + 1)) + 50; Returns: 61","title":"RANDOM"},{"location":"sql-reference/functions-reference/numeric-functions/#round","text":"Rounds a value to a specified number of decimal places. Syntax ROUND(<val> [, <dec>]) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, ROUND returns whole numbers. Examples SELECT ROUND(5.4) Returns: 5 SELECT ROUND(5.6930, 1) Returns: 5.7","title":"ROUND"},{"location":"sql-reference/functions-reference/numeric-functions/#sin","text":"Calculates the sinus. Syntax SIN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT SIN(90) Returns: 0.8939966636005579","title":"SIN"},{"location":"sql-reference/functions-reference/numeric-functions/#sqrt","text":"Returns the square root of a non-negative numeric expression. Syntax SQRT(<val>); Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Returns NULL if a negative value is given. Example SELECT SQRT(64) Returns: 8","title":"SQRT"},{"location":"sql-reference/functions-reference/numeric-functions/#tan","text":"Calculates the tangent. Syntax TAN(<val>) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. Example SELECT TAN(90) Returns: - 1.995200412208242","title":"TAN"},{"location":"sql-reference/functions-reference/numeric-functions/#trunc","text":"Returns the rounded absolute value of a numeric value. The returned value will always be rounded to less than the original value. Syntax TRUNC(<val>[, <dec>]) Parameter Description <val> Valid values include column names, functions that return a column with numeric values, and constant numeric values. <dec> Optional. An INT constant that defines the decimal range of the returned value. By default, TRUNC returns whole numbers. Examples SELECT TRUNC(-20.5) Returns: -20 SELECT TRUNC(-99.999999, 3) Returns: -99.999","title":"TRUNC"},{"location":"sql-reference/functions-reference/string-functions/","text":"String functions This page describes the string functions supported in Firebolt. BASE64_ENCODE Encodes a string into Base64 notation. Syntax BASE64_ENCODE(<expr>) Parameter Description <expr> Any expression that evaluates to a STRING , TEXT , or VARCHAR data type Example SELECT BASE64_ENCODE('Hello World') Returns: SGVsbG8gV29ybGQ= CONCAT Concatenates the strings listed in the arguments without a separator. Syntax CONCAT( <string>, <string2>[, ...n] ); Alternative syntax <string> || <string2> || [ ...n] Parameter Description <string>, <string2>[, ...n] The strings to be concatenated. Example SELECT concat('Hello ', 'World!') Returns: Hello World! EXTRACT_ALL Extracts fragments within a string that match a specified regex pattern. String fragments that match are returned as an array of string types. Syntax EXTRACT_ALL( <expr>, '<regex_pattern>' ) Parameter Description <expr> Any expression that evaluates to a STRING , TEXT , or VARCHAR data type <regex_pattern> An re2 regular expression used for matching. Example In the example below, EXTRACT_ALL is used to match variants of \"Hello World\". The regular expression pattern 'Hello.[Ww]orld!?' does not match any special characters except for ! . SELECT EXTRACT_ALL ( 'Hello world, ;-+ Hello World!', 'Hello.[Ww]orld!?' ); Returns: [\"Hello world\",\"Hello World!\"] ILIKE Allows matching of strings based on comparison to a pattern. ILIKE is normally used as part of a WHERE clause. Syntax <expr> ILIKE '<pattern>' Parameter Description <expr> Any expression that evaluates to a TEXT , STRING , or VARCHAR data type. <pattern> Specifies the pattern to match and is case-insensitive. SQL wildcards are supported: Use an underscore ( _ ) to match any single character Use a percent sign ( % ) to match any number of any characters, including no characters Example For this example, we will create and load data into a demonstration table match_test : CREATE DIMENSION TABLE match_test ( first_name TEXT, last_name TEXT ); INSERT INTO match_test VALUES ('Sammy', 'Sardine'), ('Franco', 'Fishmonger'), ('Carol', 'Catnip'), ('Thomas', 'Tinderbox'), ('Deborah', 'Donut'), ('Humphrey', 'Hoagie'), ('Frank', 'Falafel'); We can match first names that partially match the string \"Fran\" and any following characters as follows: SELECT * FROM match_test WHERE first_name ILIKE 'Fran%'; Returns: +------------+------------+ | first_name | last_name | +------------+------------+ | Frank | Falafel | | Franco | Fishmonger | +------------+------------+ LENGTH Calculates the string length. Syntax \u200b\u200bLENGTH(<string>)\u200b\u200b Parameter Description <string> The string for which to return the length. Example SELECT LENGTH('abcd') Returns: 4 LOWER Converts the string to a lowercase format. Syntax \u200b\u200bLOWER(<string>)\u200b\u200b Parameter Description <string> The string to be converted. Example SELECT LOWER('ABCD') Returns: abcd LPAD Adds a specified pad string to the end of the string repetitively up until the length of the resulting string is equivalent to an indicated length. The similar function to pad the end of a string is RPAD . Syntax \u200b\u200bLPAD(<str>, <length>[, <pad>])\u200b\u200b Parameter Description <str> The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b <str> can be a literal string or the name of a column. <length> The length of the string as an integer after it has been left-padded. \u200b A negative number returns an empty string. <pad> The string to add to the start of the primary string <str> . If left blank, <pad> defaults to whitespace characters. Example The following statement adds the string \"ABC\" in front of the string Firebolt repetitively until the resulting string is equivalent to 20 characters in length. SELECT LPAD('Firebolt', 20, 'ABC'); Returns : ABCABCABCABCFirebolt LTRIM Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bLTRIM(<target>)\u200b\u200b Parameter Description <target> The string to be trimmed. Example SELECT LTRIM(' Hello, world! ') Returns: Hello, world! MATCH Checks whether the string matches the regular expression <pattern >, which is a RE2 regular expression. \u200b Returns 0 if it doesn\u2019t match, or 1 if it matches. Syntax \u200b\u200bMATCH(<string>, '<pattern>')\u200b\u200b Parameter Description <string> The string used to search for a match. <pattern> The regular expression pattern used to search <string> for a match. Examples The example below generates 0 as a result because it found no match. It is searching a string of numbers for alphabet characters.** ** SELECT MATCH('123','\\\\[a-Z|A-Z]') AS res; Returns: 0 In this second example, the MATCH expression generates a result of 1 because it found a match. It is searching for numeric digits in the string \"123\". SELECT MATCH('123','\\\\d+'); **Returns: ** 1 MATCH_ANY The same as MATCH , but it searches for a match with one or more more regular expression patterns. It returns 0 if none of the regular expressions match and 1 if any of the patterns matches. Synonym for MULTI_MATCH_ANY Syntax \u200b\u200bMATCH_ANY(<string>, <pattern_array>)\u200b\u200b Parameter Description <string> The string to search for a match. <pattern_array> A series of one or more regular expression patterns to search for a match in the <string> . <pattern_array> must be enclosed in brackets. Each pattern must be enclosed in single quotes and separated with commas. For example, the <pattern_array> below consists of two regular expression patterns: ['^\\S Apple', 'pie $'] Example The query below** **searches for any matches within the string \u200b 123 \u200b\u200bwith the patterns \u200b['\\d+','\\[a-Z|A-Z]'] \u200b. \u200b Since at least one is found, it returns: 1 SELECT MATCH_ANY( '123', [ '\\\\d+', '\\\\[a-Z|A-Z]' ] ) AS res; Returns: 1 MD5 Calculates the MD5 hash of string, returning the result as a string in hexadecimal. Syntax \u200b\u200bMD5(<string>)\u200b\u200b Parameter Description <string> The string to hash. For NULL , the function returns 0 . Example SELECT MD5('text') AS res; Returns : 1cb251ec0d568de6a929b520c4aed8d1 MD5_NUMBER_LOWER64 Represent the lower 64 bits of the MD5 hash value of the input string as BIGINT . Syntax \u200b\u200bMD5_NUMBER_LOWER64(<string>)\u200b\u200b Parameter Description <string> The string to calculate the MD5 hash value on and represent as BIGINT. Example SELECT MD5_NUMBER_LOWER64('test') AS res; Returns : 14618207765679027446 MD5_NUMBER_UPPER64 Represent the upper 64 bits of the MD5 hash value of the input string as BIGINT . Syntax \u200b\u200bMD5_NUMBER_UPPER64(<string>)\u200b\u200b Parameter Description <string> The string to calculate the MD5 on and represent as BIGINT Example SELECT MD5_NUMBER_UPPER64('test') AS res; Returns : 688887797400064883 REGEXP_LIKE This check whether a string pattern matches a regular expression string. Returns 0 if it doesn\u2019t match, or 1 if it matches. \u200b This is a \u200b\u200bRE2\u200b\u200b regular expression. Syntax \u200bREGEXP_LIKE(<string>, '<pattern>')\u200b\u200b Parameter Description <string> The string searched for a match using the RE2 pattern. <pattern> The pattern used to search for a match in the <string> . Examples \u200b\u200bSELECT REGEXP_LIKE('123','\\\\[a-z]') AS res;\u200b\u200b Returns : 0 SELECT REGEXP_LIKE('123','\\\\d+') AS res;\u200b Returns : 1 REGEXP_MATCHES Returns an array of all substrings that match a regular expression pattern. If the pattern does not match, returns an empty array. REGEXP_MATCHES(<string>, <pattern>[,'<flag>[...]']) Parameter Description <string> The string from which to extract substrings, based on a regular expression <pattern> An re2 regular expression for matching with the string. <flag> Optional. Flags allow additional controls over characters used in the regular expression matching. If using multiple flags, you can include them in the same single-quote block without any separator character. Firebolt supports the following re2 flags to override default matching behavior. i - Specifies case-insensitive matching. m - Specifies multi-line mode. In this mode, ^ and $ characters in the regex match the beginning and end of line in in addition to the beginning and end of text in . s - Specifies that the . metacharacter in regex matches the newline character in addition to any character in . U - Specifies Ungreedy mode. In this mode, the meaning of the metacharacters and + in regex are swapped with ? and +? , respectively. See the examples using flags below for the difference in how results are returned. REPEAT This function repeats the provided string a requested number of times. Syntax REPEAT(<string>, <repeating_number>) Parameter Description <string> The string to be repeated. <repeating_number> The number of needed repetitions. The minimum valid repeating number is 0 , which results in an empty string. Example SELECT REPEAT('repeat 3 times ' , 3); Returns : repeat 3 times repeat 3 times repeat 3 times REPLACE Replaces all occurrences of the <pattern> substring within the <string> with the <replacement> substring. Syntax REPLACE (<string>, <pattern>, <replacement>)\u200b Parameter Description <string> The original string that will be searched for instances of the <pattern> . <pattern> The substring to be searched and replaced in the string. <replacement> The substring to replace the original substring defined by <pattern> . To remove the <pattern> substring with no replacement, you can use a empty string '' as the replacement value. Examples In the example below, \"hello\" in \"hello world\" is replaced with \"nice\". SELECT REPLACE('hello world','hello','nice') AS res; Returns : nice world In this example below, \"world\" is replaced by an empty string. SELECT REPLACE('hello world',' world','') AS res; Returns : hello In this following example, the substring \"hi\" is not found in the original string, so the string is returned unchanged. SELECT REPLACE('hello world','hi','something') AS res; Returns : hello world REVERSE This function returns a string of the same size as the original string, with the elements in reverse order. Syntax REVERSE(<string>) Parameter Description <string> The string to be reversed. Example SELECT REVERSE('abcd') AS res Returns : 'dcba' RPAD Adds a specified pad string to the end of the string repetitively up until the length of the resulting string is equivalent to an indicated length. The similar function to pad the start of a string is LPAD . Syntax \u200b\u200bRPAD(<str>, <length>[, <pad>])\u200b\u200b Parameter Description <str> The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b <str> can be a literal string or the name of a column. <length> The integer length that the string will be after it has been left-padded. \u200b A negative number returns an empty string. <pad> The string to add to the end of the primary string <str> . If left blank, <pad> defaults to whitespace characters. Example The following statement adds the string \"ABC\" to the end of the string \"Firebolt\" repetitively until the resulting string is equivalent to 20 characters in length. SELECT RPAD('Firebolt', 20, 'ABC'); Returns : FireboltABCABCABCABC RTRIM Removes all consecutive occurrences of common whitespace (ASCII character 32) from the end of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bRTRIM(<target>)\u200b\u200b Parameter Description <target> The string to be trimmed. Example SELECT RTRIM('Hello, world! ') Returns : Hello, world! SPLIT This function splits a given string by a given separator and returns the result in an array of strings. Syntax SPLIT( <delimiter>, <string> ) Parameter Description <delimiter> The separator to split the string by. <string> The string to split. Example SELECT SPLIT('|','this|is|my|test') AS res Returns : [\"this\",\"is\",\"my\",\"test\"] SPLIT_PART Divides a string based on a specified delimiter into an array of substrings. \u200b The string in the specified index is returned, with 1 being the first index. If the string separator is empty, the string is divided into an array of single characters. Syntax \u200b\u200bSPLIT_PART(<string>, <delimiter>, <index>)\u200b\u200b Please note that the order of the arguments is different than the SPLIT function. Parameter Description <string> An expression evaluating to a string to be split. <delimiter> Any character or substring within <string> . If <delimiter> is an empty string '' , the <string> will be divided into single characters. <index> The index from which to return the substring. Examples SELECT SPLIT_PART('hello#world','#',1) AS res; Returns : hello SELECT SPLIT_PART('this|is|my|test', '|', 4 ) AS res; Returns : test SELECT SPLIT_PART('hello world', '', 7 ) AS res; Returns : w STRPOS Returns the position (in bytes) of the substring found in the string, starting from 1. The returned value is for the first matching value, and not for any subsequent valid matches. Syntax \u200b\u200bSTRPOS(<string>, <substring>)\u200b\u200b Parameter Description <string> The string in which to search for . <substring> The substring to search for. Examples SELECT STRPOS('hello world','hello') AS res Returns : 1 SELECT STRPOS('hello world','world') AS res Returns : 7 SUBSTR Returns a substring starting at the character indicated by the <offset> index and including the number of characters defined by the <length> . Character indexing starts from index 1. The <offset> and <length> arguments must be constants. Syntax SUBSTR(<string>, <offset> [, <length>]) Parameter Description <string> The string to be offset. <offset> The starting position for the substring. 1 is the first character. <length> Optional. The number of characters to be returned by the SUBSTR function. If left blank, length by default returns all of the string not specified by the offset parameter. Example In the example below, the string is offset by 1 and so the SUBSTR command begins at the first letter, \"h\". The <length> of 5 indicates the resulting string should be only five characters long. SELECT SUBSTR('hello world', 1, 5); Returns : hello In this next example, there is no <length> provided. This means all characters are included after the <offset> index, which is 7. SELECT SUBSTR('hello world', 7); Returns : world TO_DATE Converts a string to DATE type. Syntax \u200b\u200bTO_DATE(<string>)\u200b\u200b Parameter Description <string> The string to convert to a date. The string format should be: \u2018YYYY-MM-DD\u2019 Example SELECT TO_DATE('2020-05-31') AS res; Returns : 2020-05-31 TO_DOUBLE Converts a string to a numeric DOUBLE data type. Syntax TO_DOUBLE(<exp>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_DOUBLE('100') Returns : 100 TO_FLOAT Converts a string to a numeric FLOAT data type. Syntax TO_FLOAT(<expr>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_FLOAT('10.5') Returns: 10.5 TO_INT Converts a string to a numeric INT data type. Syntax TO_INT(<exp>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_INT('10') Returns : 10 TO_LONG Converts a string to a numeric LONG data type. Syntax TO_LONG(<exp>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_LONG('1234567890') Returns: 1234567890 TO_TIMESTAMP Converts a string to timestamp. Syntax \u200b\u200bTO_TIMESTAMP(<string>)\u200b\u200b Parameter Description <string> The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Example SELECT TO_TIMESTAMP('2020-05-31 10:31:14') AS res; Returns: 2020-05-31 10:31:14 TO_UNIX_TIMESTAMP Converts a string to a UNIX timestamp. Syntax \u200b\u200bTO_UNIX_TIMESTAMP(<string>)\u200b\u200b Parameter Description <string> The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Example SELECT TO_UNIX_TIMESTAMP('2017-11-05 08:07:47'); Returns : 1509869267 TO_UNIXTIME For \u200b DATETIME \u200b\u200barguments: this function converts the value to its internal numeric representation (Unix Timestamp). \u200b For \u200b TEXT \u200b\u200barguments: this function parses \u200b DATETIME \u200b\u200bfrom a string and returns the corresponding Unix timestamp. Syntax \u200b\u200bTO_UNIXTIME(<string>)\u200b\u200b Parameter Description <string> The string to be converted. Example SELECT TO_UNIXTIME('2017-11-05 08:07:47') AS TO_UNIXTIME; Returns: 1509869267 TRIM Removes all specified characters from the start, end, or both sides of a string. By default removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. Syntax \u200b\u200bTRIM( [LEADING | TRAILING | BOTH] <trim_character> FROM <target_string>)\u200b\u200b Parameter Description [ LEADING \\| TRAILING \\| BOTH ] Specifies which part or parts of the <target_string> to remove the defined <trim_character> . If unspecified, this defaults to BOTH . LEADING - trims from the beginning of the specified string TRAILING - trims from the end of the specified string. BOTH - trims from the beginning and the end of the specified string. <trim_character> The characters to be removed. <target_string> The string to be trimmed. Example In the example below, no part of the string is specified for TRIM , so it defaults to BOTH . SELECT TRIM('$' FROM '$Hello world$') AS res; Returns : Hello world This next example trims only from the start of the string because the LEADING parameter is specified. SELECT TRIM( LEADING '$' FROM '$Hello world$') AS res; Returns : Hello world$ UPPER Converts the string to uppercase format. Syntax \u200b\u200bUPPER(<string>)\u200b\u200b Parameter Description <string> The string to be converted to all uppercase characters. Example SELECT UPPER('hello world') Returns: HELLO WORLD","title":"String functions"},{"location":"sql-reference/functions-reference/string-functions/#string-functions","text":"This page describes the string functions supported in Firebolt.","title":"String functions"},{"location":"sql-reference/functions-reference/string-functions/#base64_encode","text":"Encodes a string into Base64 notation. Syntax BASE64_ENCODE(<expr>) Parameter Description <expr> Any expression that evaluates to a STRING , TEXT , or VARCHAR data type Example SELECT BASE64_ENCODE('Hello World') Returns: SGVsbG8gV29ybGQ=","title":"BASE64_ENCODE"},{"location":"sql-reference/functions-reference/string-functions/#concat","text":"Concatenates the strings listed in the arguments without a separator. Syntax CONCAT( <string>, <string2>[, ...n] ); Alternative syntax <string> || <string2> || [ ...n] Parameter Description <string>, <string2>[, ...n] The strings to be concatenated. Example SELECT concat('Hello ', 'World!') Returns: Hello World!","title":"CONCAT"},{"location":"sql-reference/functions-reference/string-functions/#extract_all","text":"Extracts fragments within a string that match a specified regex pattern. String fragments that match are returned as an array of string types. Syntax EXTRACT_ALL( <expr>, '<regex_pattern>' ) Parameter Description <expr> Any expression that evaluates to a STRING , TEXT , or VARCHAR data type <regex_pattern> An re2 regular expression used for matching. Example In the example below, EXTRACT_ALL is used to match variants of \"Hello World\". The regular expression pattern 'Hello.[Ww]orld!?' does not match any special characters except for ! . SELECT EXTRACT_ALL ( 'Hello world, ;-+ Hello World!', 'Hello.[Ww]orld!?' ); Returns: [\"Hello world\",\"Hello World!\"]","title":"EXTRACT_ALL"},{"location":"sql-reference/functions-reference/string-functions/#ilike","text":"Allows matching of strings based on comparison to a pattern. ILIKE is normally used as part of a WHERE clause. Syntax <expr> ILIKE '<pattern>' Parameter Description <expr> Any expression that evaluates to a TEXT , STRING , or VARCHAR data type. <pattern> Specifies the pattern to match and is case-insensitive. SQL wildcards are supported: Use an underscore ( _ ) to match any single character Use a percent sign ( % ) to match any number of any characters, including no characters Example For this example, we will create and load data into a demonstration table match_test : CREATE DIMENSION TABLE match_test ( first_name TEXT, last_name TEXT ); INSERT INTO match_test VALUES ('Sammy', 'Sardine'), ('Franco', 'Fishmonger'), ('Carol', 'Catnip'), ('Thomas', 'Tinderbox'), ('Deborah', 'Donut'), ('Humphrey', 'Hoagie'), ('Frank', 'Falafel'); We can match first names that partially match the string \"Fran\" and any following characters as follows: SELECT * FROM match_test WHERE first_name ILIKE 'Fran%'; Returns: +------------+------------+ | first_name | last_name | +------------+------------+ | Frank | Falafel | | Franco | Fishmonger | +------------+------------+","title":"ILIKE"},{"location":"sql-reference/functions-reference/string-functions/#length","text":"Calculates the string length. Syntax \u200b\u200bLENGTH(<string>)\u200b\u200b Parameter Description <string> The string for which to return the length. Example SELECT LENGTH('abcd') Returns: 4","title":"LENGTH"},{"location":"sql-reference/functions-reference/string-functions/#lower","text":"Converts the string to a lowercase format. Syntax \u200b\u200bLOWER(<string>)\u200b\u200b Parameter Description <string> The string to be converted. Example SELECT LOWER('ABCD') Returns: abcd","title":"LOWER"},{"location":"sql-reference/functions-reference/string-functions/#lpad","text":"Adds a specified pad string to the end of the string repetitively up until the length of the resulting string is equivalent to an indicated length. The similar function to pad the end of a string is RPAD . Syntax \u200b\u200bLPAD(<str>, <length>[, <pad>])\u200b\u200b Parameter Description <str> The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b <str> can be a literal string or the name of a column. <length> The length of the string as an integer after it has been left-padded. \u200b A negative number returns an empty string. <pad> The string to add to the start of the primary string <str> . If left blank, <pad> defaults to whitespace characters. Example The following statement adds the string \"ABC\" in front of the string Firebolt repetitively until the resulting string is equivalent to 20 characters in length. SELECT LPAD('Firebolt', 20, 'ABC'); Returns : ABCABCABCABCFirebolt","title":"LPAD"},{"location":"sql-reference/functions-reference/string-functions/#ltrim","text":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bLTRIM(<target>)\u200b\u200b Parameter Description <target> The string to be trimmed. Example SELECT LTRIM(' Hello, world! ') Returns: Hello, world!","title":"LTRIM"},{"location":"sql-reference/functions-reference/string-functions/#match","text":"Checks whether the string matches the regular expression <pattern >, which is a RE2 regular expression. \u200b Returns 0 if it doesn\u2019t match, or 1 if it matches. Syntax \u200b\u200bMATCH(<string>, '<pattern>')\u200b\u200b Parameter Description <string> The string used to search for a match. <pattern> The regular expression pattern used to search <string> for a match. Examples The example below generates 0 as a result because it found no match. It is searching a string of numbers for alphabet characters.** ** SELECT MATCH('123','\\\\[a-Z|A-Z]') AS res; Returns: 0 In this second example, the MATCH expression generates a result of 1 because it found a match. It is searching for numeric digits in the string \"123\". SELECT MATCH('123','\\\\d+'); **Returns: ** 1","title":"MATCH"},{"location":"sql-reference/functions-reference/string-functions/#match_any","text":"The same as MATCH , but it searches for a match with one or more more regular expression patterns. It returns 0 if none of the regular expressions match and 1 if any of the patterns matches. Synonym for MULTI_MATCH_ANY Syntax \u200b\u200bMATCH_ANY(<string>, <pattern_array>)\u200b\u200b Parameter Description <string> The string to search for a match. <pattern_array> A series of one or more regular expression patterns to search for a match in the <string> . <pattern_array> must be enclosed in brackets. Each pattern must be enclosed in single quotes and separated with commas. For example, the <pattern_array> below consists of two regular expression patterns: ['^\\S Apple', 'pie $'] Example The query below** **searches for any matches within the string \u200b 123 \u200b\u200bwith the patterns \u200b['\\d+','\\[a-Z|A-Z]'] \u200b. \u200b Since at least one is found, it returns: 1 SELECT MATCH_ANY( '123', [ '\\\\d+', '\\\\[a-Z|A-Z]' ] ) AS res; Returns: 1","title":"MATCH_ANY"},{"location":"sql-reference/functions-reference/string-functions/#md5","text":"Calculates the MD5 hash of string, returning the result as a string in hexadecimal. Syntax \u200b\u200bMD5(<string>)\u200b\u200b Parameter Description <string> The string to hash. For NULL , the function returns 0 . Example SELECT MD5('text') AS res; Returns : 1cb251ec0d568de6a929b520c4aed8d1","title":"MD5"},{"location":"sql-reference/functions-reference/string-functions/#md5_number_lower64","text":"Represent the lower 64 bits of the MD5 hash value of the input string as BIGINT . Syntax \u200b\u200bMD5_NUMBER_LOWER64(<string>)\u200b\u200b Parameter Description <string> The string to calculate the MD5 hash value on and represent as BIGINT. Example SELECT MD5_NUMBER_LOWER64('test') AS res; Returns : 14618207765679027446","title":"MD5_NUMBER_LOWER64"},{"location":"sql-reference/functions-reference/string-functions/#md5_number_upper64","text":"Represent the upper 64 bits of the MD5 hash value of the input string as BIGINT . Syntax \u200b\u200bMD5_NUMBER_UPPER64(<string>)\u200b\u200b Parameter Description <string> The string to calculate the MD5 on and represent as BIGINT Example SELECT MD5_NUMBER_UPPER64('test') AS res; Returns : 688887797400064883","title":"MD5_NUMBER_UPPER64"},{"location":"sql-reference/functions-reference/string-functions/#regexp_like","text":"This check whether a string pattern matches a regular expression string. Returns 0 if it doesn\u2019t match, or 1 if it matches. \u200b This is a \u200b\u200bRE2\u200b\u200b regular expression. Syntax \u200bREGEXP_LIKE(<string>, '<pattern>')\u200b\u200b Parameter Description <string> The string searched for a match using the RE2 pattern. <pattern> The pattern used to search for a match in the <string> . Examples \u200b\u200bSELECT REGEXP_LIKE('123','\\\\[a-z]') AS res;\u200b\u200b Returns : 0 SELECT REGEXP_LIKE('123','\\\\d+') AS res;\u200b Returns : 1","title":"REGEXP_LIKE"},{"location":"sql-reference/functions-reference/string-functions/#regexp_matches","text":"Returns an array of all substrings that match a regular expression pattern. If the pattern does not match, returns an empty array. REGEXP_MATCHES(<string>, <pattern>[,'<flag>[...]']) Parameter Description <string> The string from which to extract substrings, based on a regular expression <pattern> An re2 regular expression for matching with the string. <flag> Optional. Flags allow additional controls over characters used in the regular expression matching. If using multiple flags, you can include them in the same single-quote block without any separator character. Firebolt supports the following re2 flags to override default matching behavior. i - Specifies case-insensitive matching. m - Specifies multi-line mode. In this mode, ^ and $ characters in the regex match the beginning and end of line in in addition to the beginning and end of text in . s - Specifies that the . metacharacter in regex matches the newline character in addition to any character in . U - Specifies Ungreedy mode. In this mode, the meaning of the metacharacters and + in regex are swapped with ? and +? , respectively. See the examples using flags below for the difference in how results are returned.","title":"REGEXP_MATCHES"},{"location":"sql-reference/functions-reference/string-functions/#repeat","text":"This function repeats the provided string a requested number of times. Syntax REPEAT(<string>, <repeating_number>) Parameter Description <string> The string to be repeated. <repeating_number> The number of needed repetitions. The minimum valid repeating number is 0 , which results in an empty string. Example SELECT REPEAT('repeat 3 times ' , 3); Returns : repeat 3 times repeat 3 times repeat 3 times","title":"REPEAT"},{"location":"sql-reference/functions-reference/string-functions/#replace","text":"Replaces all occurrences of the <pattern> substring within the <string> with the <replacement> substring. Syntax REPLACE (<string>, <pattern>, <replacement>)\u200b Parameter Description <string> The original string that will be searched for instances of the <pattern> . <pattern> The substring to be searched and replaced in the string. <replacement> The substring to replace the original substring defined by <pattern> . To remove the <pattern> substring with no replacement, you can use a empty string '' as the replacement value. Examples In the example below, \"hello\" in \"hello world\" is replaced with \"nice\". SELECT REPLACE('hello world','hello','nice') AS res; Returns : nice world In this example below, \"world\" is replaced by an empty string. SELECT REPLACE('hello world',' world','') AS res; Returns : hello In this following example, the substring \"hi\" is not found in the original string, so the string is returned unchanged. SELECT REPLACE('hello world','hi','something') AS res; Returns : hello world","title":"REPLACE"},{"location":"sql-reference/functions-reference/string-functions/#reverse","text":"This function returns a string of the same size as the original string, with the elements in reverse order. Syntax REVERSE(<string>) Parameter Description <string> The string to be reversed. Example SELECT REVERSE('abcd') AS res Returns : 'dcba'","title":"REVERSE"},{"location":"sql-reference/functions-reference/string-functions/#rpad","text":"Adds a specified pad string to the end of the string repetitively up until the length of the resulting string is equivalent to an indicated length. The similar function to pad the start of a string is LPAD . Syntax \u200b\u200bRPAD(<str>, <length>[, <pad>])\u200b\u200b Parameter Description <str> The original string. If the length of the original string is larger than the length parameter, this function removes the overflowing characters from the string. \u200b <str> can be a literal string or the name of a column. <length> The integer length that the string will be after it has been left-padded. \u200b A negative number returns an empty string. <pad> The string to add to the end of the primary string <str> . If left blank, <pad> defaults to whitespace characters. Example The following statement adds the string \"ABC\" to the end of the string \"Firebolt\" repetitively until the resulting string is equivalent to 20 characters in length. SELECT RPAD('Firebolt', 20, 'ABC'); Returns : FireboltABCABCABCABC","title":"RPAD"},{"location":"sql-reference/functions-reference/string-functions/#rtrim","text":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the end of a string. It doesn\u2019t remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax \u200b\u200bRTRIM(<target>)\u200b\u200b Parameter Description <target> The string to be trimmed. Example SELECT RTRIM('Hello, world! ') Returns : Hello, world!","title":"RTRIM"},{"location":"sql-reference/functions-reference/string-functions/#split","text":"This function splits a given string by a given separator and returns the result in an array of strings. Syntax SPLIT( <delimiter>, <string> ) Parameter Description <delimiter> The separator to split the string by. <string> The string to split. Example SELECT SPLIT('|','this|is|my|test') AS res Returns : [\"this\",\"is\",\"my\",\"test\"]","title":"SPLIT"},{"location":"sql-reference/functions-reference/string-functions/#split_part","text":"Divides a string based on a specified delimiter into an array of substrings. \u200b The string in the specified index is returned, with 1 being the first index. If the string separator is empty, the string is divided into an array of single characters. Syntax \u200b\u200bSPLIT_PART(<string>, <delimiter>, <index>)\u200b\u200b Please note that the order of the arguments is different than the SPLIT function. Parameter Description <string> An expression evaluating to a string to be split. <delimiter> Any character or substring within <string> . If <delimiter> is an empty string '' , the <string> will be divided into single characters. <index> The index from which to return the substring. Examples SELECT SPLIT_PART('hello#world','#',1) AS res; Returns : hello SELECT SPLIT_PART('this|is|my|test', '|', 4 ) AS res; Returns : test SELECT SPLIT_PART('hello world', '', 7 ) AS res; Returns : w","title":"SPLIT_PART"},{"location":"sql-reference/functions-reference/string-functions/#strpos","text":"Returns the position (in bytes) of the substring found in the string, starting from 1. The returned value is for the first matching value, and not for any subsequent valid matches. Syntax \u200b\u200bSTRPOS(<string>, <substring>)\u200b\u200b Parameter Description <string> The string in which to search for . <substring> The substring to search for. Examples SELECT STRPOS('hello world','hello') AS res Returns : 1 SELECT STRPOS('hello world','world') AS res Returns : 7","title":"STRPOS"},{"location":"sql-reference/functions-reference/string-functions/#substr","text":"Returns a substring starting at the character indicated by the <offset> index and including the number of characters defined by the <length> . Character indexing starts from index 1. The <offset> and <length> arguments must be constants. Syntax SUBSTR(<string>, <offset> [, <length>]) Parameter Description <string> The string to be offset. <offset> The starting position for the substring. 1 is the first character. <length> Optional. The number of characters to be returned by the SUBSTR function. If left blank, length by default returns all of the string not specified by the offset parameter. Example In the example below, the string is offset by 1 and so the SUBSTR command begins at the first letter, \"h\". The <length> of 5 indicates the resulting string should be only five characters long. SELECT SUBSTR('hello world', 1, 5); Returns : hello In this next example, there is no <length> provided. This means all characters are included after the <offset> index, which is 7. SELECT SUBSTR('hello world', 7); Returns : world","title":"SUBSTR"},{"location":"sql-reference/functions-reference/string-functions/#to_date","text":"Converts a string to DATE type. Syntax \u200b\u200bTO_DATE(<string>)\u200b\u200b Parameter Description <string> The string to convert to a date. The string format should be: \u2018YYYY-MM-DD\u2019 Example SELECT TO_DATE('2020-05-31') AS res; Returns : 2020-05-31","title":"TO_DATE"},{"location":"sql-reference/functions-reference/string-functions/#to_double","text":"Converts a string to a numeric DOUBLE data type. Syntax TO_DOUBLE(<exp>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_DOUBLE('100') Returns : 100","title":"TO_DOUBLE"},{"location":"sql-reference/functions-reference/string-functions/#to_float","text":"Converts a string to a numeric FLOAT data type. Syntax TO_FLOAT(<expr>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_FLOAT('10.5') Returns: 10.5","title":"TO_FLOAT"},{"location":"sql-reference/functions-reference/string-functions/#to_int","text":"Converts a string to a numeric INT data type. Syntax TO_INT(<exp>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_INT('10') Returns : 10","title":"TO_INT"},{"location":"sql-reference/functions-reference/string-functions/#to_long","text":"Converts a string to a numeric LONG data type. Syntax TO_LONG(<exp>) Parameter Description <expr> Any numeric data types or numeric characters that resolve to a VARCHAR , TEXT , or STRING data type. Example SELECT TO_LONG('1234567890') Returns: 1234567890","title":"TO_LONG"},{"location":"sql-reference/functions-reference/string-functions/#to_timestamp","text":"Converts a string to timestamp. Syntax \u200b\u200bTO_TIMESTAMP(<string>)\u200b\u200b Parameter Description <string> The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Example SELECT TO_TIMESTAMP('2020-05-31 10:31:14') AS res; Returns: 2020-05-31 10:31:14","title":"TO_TIMESTAMP"},{"location":"sql-reference/functions-reference/string-functions/#to_unix_timestamp","text":"Converts a string to a UNIX timestamp. Syntax \u200b\u200bTO_UNIX_TIMESTAMP(<string>)\u200b\u200b Parameter Description <string> The string format should be: \u2018YYYY-MM-DD HH:mm:ss\u2019 Example SELECT TO_UNIX_TIMESTAMP('2017-11-05 08:07:47'); Returns : 1509869267","title":"TO_UNIX_TIMESTAMP"},{"location":"sql-reference/functions-reference/string-functions/#to_unixtime","text":"For \u200b DATETIME \u200b\u200barguments: this function converts the value to its internal numeric representation (Unix Timestamp). \u200b For \u200b TEXT \u200b\u200barguments: this function parses \u200b DATETIME \u200b\u200bfrom a string and returns the corresponding Unix timestamp. Syntax \u200b\u200bTO_UNIXTIME(<string>)\u200b\u200b Parameter Description <string> The string to be converted. Example SELECT TO_UNIXTIME('2017-11-05 08:07:47') AS TO_UNIXTIME; Returns: 1509869267","title":"TO_UNIXTIME"},{"location":"sql-reference/functions-reference/string-functions/#trim","text":"Removes all specified characters from the start, end, or both sides of a string. By default removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. Syntax \u200b\u200bTRIM( [LEADING | TRAILING | BOTH] <trim_character> FROM <target_string>)\u200b\u200b Parameter Description [ LEADING \\| TRAILING \\| BOTH ] Specifies which part or parts of the <target_string> to remove the defined <trim_character> . If unspecified, this defaults to BOTH . LEADING - trims from the beginning of the specified string TRAILING - trims from the end of the specified string. BOTH - trims from the beginning and the end of the specified string. <trim_character> The characters to be removed. <target_string> The string to be trimmed. Example In the example below, no part of the string is specified for TRIM , so it defaults to BOTH . SELECT TRIM('$' FROM '$Hello world$') AS res; Returns : Hello world This next example trims only from the start of the string because the LEADING parameter is specified. SELECT TRIM( LEADING '$' FROM '$Hello world$') AS res; Returns : Hello world$","title":"TRIM"},{"location":"sql-reference/functions-reference/string-functions/#upper","text":"Converts the string to uppercase format. Syntax \u200b\u200bUPPER(<string>)\u200b\u200b Parameter Description <string> The string to be converted to all uppercase characters. Example SELECT UPPER('hello world') Returns: HELLO WORLD","title":"UPPER"},{"location":"sql-reference/functions-reference/window-functions/","text":"Window Functions A window function performs a calculation across a set of table rows. Unlike regular aggregate functions, the use of a window function does not cause rows to become grouped into a single output row but allows the rows to retain their separate identities. Window functions are identified by the OVER() syntax. To narrow down the window dataset into individual groups, use the PARTITION BY clause. In case this clause is not used, a single window is being created. This page describes the window functions supported in Firebolt. AVG Returns the average value within the requested window. Syntax AVG( <val> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the AVG() function. <expr> An expression used for the partition by clause. Example The example below is querying test scores for students in various grade levels. Unlike a regular AVG() aggregation, the window function allows us to see how each student individually compares to the average test score for their grade level. SELECT First_name, Grade_level, Test_score, AVG(Test_score) OVER (PARTITION BY Grade_level) AS average_for_grade FROM class_test Results ' +------------+-------------+------------+-------------------------+ ' | First_name | Grade_level | Test_score | average_for_grade | ' +------------+-------------+------------+-------------------------+ ' | Frank | 9 | 76 | 81.33333333333333 | ' | Humphrey | 9 | 90 | 81.33333333333333 | ' | Iris | 9 | 79 | 81.33333333333333 | ' | Sammy | 9 | 85 | 81.33333333333333 | ' | Peter | 9 | 80 | 81.33333333333333 | ' | Jojo | 9 | 78 | 81.33333333333333 | ' | Brunhilda | 12 | 92 | 89 | ' | Franco | 12 | 94 | 89 | ' | Thomas | 12 | 66 | 89 | ' | Gary | 12 | 100 | 89 | ' | Charles | 12 | 93 | 89 | ' | Jesse | 12 | 89 | 89 | ' | Roseanna | 11 | 94 | 73 | ' | Carol | 11 | 52 | 73 | ' | Wanda | 11 | 73 | 73 | ' | Shangxiu | 11 | 76 | 73 | ' | Larry | 11 | 68 | 73 | ' | Otis | 11 | 75 | 73 | ' | Deborah | 10 | 78 | 68.2 | ' | Yolinda | 10 | 30 | 68.2 | ' | Albert | 10 | 59 | 68.2 | ' | Mary | 10 | 85 | 68.2 | ' | Shawn | 10 | 89 | 68.2 | ' +------------+-------------+------------+-------------------------+ COUNT Count the number of values within the requested window. Syntax COUNT( <val> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the COUNT() function. <expr> An expression used for the PARTITION BY clause Example This example below generates a count of how many students are in each grade level while leaving each student as an independent row. SELECT First_name, Grade_level, COUNT(First_name) OVER (PARTITION BY Grade_level) AS count_of_students FROM class_test Results +------------+-------------+-------------------+ | First_name | Grade_level | count_of_students | +------------+-------------+-------------------+ | Frank | 9 | 6 | | Humphrey | 9 | 6 | | Iris | 9 | 6 | | Sammy | 9 | 6 | | Peter | 9 | 6 | | Jojo | 9 | 6 | | Brunhilda | 12 | 6 | | Franco | 12 | 6 | | Thomas | 12 | 6 | | Gary | 12 | 6 | | Charles | 12 | 6 | | Jesse | 12 | 6 | | Roseanna | 11 | 6 | | Carol | 11 | 6 | | Wanda | 11 | 6 | | Shangxiu | 11 | 6 | | Larry | 11 | 6 | | Otis | 11 | 6 | | Deborah | 10 | 5 | | Yolinda | 10 | 5 | | Albert | 10 | 5 | | Mary | 10 | 5 | | Shawn | 10 | 5 | +------------+-------------+-------------------+ DENSE_RANK Rank the current row within the requested window. Syntax DENSE_RANK() OVER ([PARTITION BY <val>] ORDER BY <exp> [ASC|DESC] ) Parameter Description <val> The expression used for the PARTITION BY clause. \\ The expression used in the ORDER BY clause. This parameter determines what value will be ranked. Example In this example below, students are ranked based on their test scores for their grade level. SELECT First_name, Grade_level, Test_score, DENSE_RANK() OVER (PARTITION BY Grade_level ORDER BY Test_score DESC ) AS Rank_in_class FROM class_test Returns: +------------+-------------+------------+---------------+ | First_name | Grade_level | Test_score | Rank_in_class | +------------+-------------+------------+---------------+ | Frank | 9 | 76 | 6 | | Humphrey | 9 | 90 | 1 | | Iris | 9 | 79 | 4 | | Sammy | 9 | 85 | 2 | | Peter | 9 | 80 | 3 | | Jojo | 9 | 78 | 5 | | Deborah | 10 | 78 | 3 | | Yolinda | 10 | 30 | 5 | | Albert | 10 | 59 | 4 | | Mary | 10 | 85 | 2 | | Shawn | 10 | 89 | 1 | | Roseanna | 11 | 94 | 1 | | Carol | 11 | 52 | 6 | | Wanda | 11 | 73 | 4 | | Shangxiu | 11 | 76 | 2 | | Larry | 11 | 68 | 5 | | Otis | 11 | 75 | 3 | | Brunhilda | 12 | 92 | 4 | | Franco | 12 | 94 | 2 | | Thomas | 12 | 66 | 6 | | Gary | 12 | 100 | 1 | | Charles | 12 | 93 | 3 | | Jesse | 12 | 89 | 5 | +------------+-------------+------------+---------------+ LAG Returns the value of the input expression at the given offset before the current row within the requested window. Syntax LAG ( <exp> [, <offset> [, <default> ]] ) OVER ( [ PARTITION BY <exp> ] ORDER BY <exp> [ { ASC | DESC } ] ) Parameter Description <val> Any valid expression that will be returned based on the LAG <offset>. <expr> The expression used for the PARTITION BY clause. <offset> The number of rows backward from the current row from which to obtain a value. A negative number will act as LEAD() <default> The expression to return when the offset goes out of the bounds of the window. Must be a literal INT . The default is NULL. Example In the example below, the LAG function is being used to find the students in each grade level who are sitting next to each other. In some cases, a student does not have an adjacent classmate, so the LAG function returns NULL . SELECT First_name, Grade_level, LAG(First_name, 1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_left, LAG(First_name, -1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_right FROM class_test Returns: +------------+-------------+-------------+--------------+ | First_name | Grade_level | To_the_left | To_the_right | +------------+-------------+-------------+--------------+ | Frank | 9 | NULL | Humphrey | | Humphrey | 9 | Frank | Iris | | Iris | 9 | Humphrey | Jojo | | Sammy | 9 | Peter | NULL | | Peter | 9 | Jojo | Sammy | | Jojo | 9 | Iris | Peter | | Brunhilda | 12 | NULL | Charles | | Franco | 12 | Charles | Gary | | Thomas | 12 | Jesse | NULL | | Gary | 12 | Franco | Jesse | | Charles | 12 | Brunhilda | Franco | | Jesse | 12 | Gary | Thomas | | Roseanna | 11 | Otis | Shangxiu | | Carol | 11 | NULL | Larry | | Wanda | 11 | Shangxiu | NULL | | Shangxiu | 11 | Roseanna | Wanda | | Larry | 11 | Carol | Otis | | Otis | 11 | Larry | Roseanna | | Deborah | 10 | Albert | Mary | | Yolinda | 10 | Shawn | NULL | | Albert | 10 | NULL | Deborah | | Mary | 10 | Deborah | Shawn | | Shawn | 10 | Mary | Yolinda | +------------+-------------+-------------+--------------+ LEAD Returns values from the row after the current row within the requested window. Syntax LEAD ( <val> [, <offset> [, <default> ] ) OVER ( [ PARTITION BY <exp> ] ORDER BY <exp> [ { ASC | DESC } ] ) Parameter Description <val> Any valid expression that will be returned based on the LEAD <offset>. <expr> The expression used for the PARTITION BY clause. <offset> The number of rows forward from the current row from which to obtain a value. <default> The expression to return when the offset goes out of the bounds of the window. Supports any expression whose type is compatible with expression. The default is NULL. Example In the example below, the LEAD function is being used to find the students in each grade level who are sitting next to each other. In some cases, a student does not have an adjacent classmate, so the LEAD function returns NULL . SELECT First_name, Grade_level, LEAD(First_name, -1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_left, LEAD(First_name, 1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_right FROM class_test; Returns: +------------+-------------+-------------+--------------+ | First_name | Grade_level | To_the_left | To_the_right | +------------+-------------+-------------+--------------+ | Frank | 9 | NULL | Humphrey | | Humphrey | 9 | Frank | Iris | | Iris | 9 | Humphrey | Jojo | | Sammy | 9 | Peter | NULL | | Peter | 9 | Jojo | Sammy | | Jojo | 9 | Iris | Peter | | Brunhilda | 12 | NULL | Charles | | Franco | 12 | Charles | Gary | | Thomas | 12 | Jesse | NULL | | Gary | 12 | Franco | Jesse | | Charles | 12 | Brunhilda | Franco | | Jesse | 12 | Gary | Thomas | | Roseanna | 11 | Otis | Shangxiu | | Carol | 11 | NULL | Larry | | Wanda | 11 | Shangxiu | NULL | | Shangxiu | 11 | Roseanna | Wanda | | Larry | 11 | Carol | Otis | | Otis | 11 | Larry | Roseanna | | Deborah | 10 | Albert | Mary | | Yolinda | 10 | Shawn | NULL | | Albert | 10 | NULL | Deborah | | Mary | 10 | Deborah | Shawn | | Shawn | 10 | Mary | Yolinda | +------------+-------------+-------------+--------------+ MIN Returns the minimum value within the requested window. Syntax MIN( <exp> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the MIN function. <exp> An expression used for the PARTITION BY clause. Example The example below queries test scores for students in various grade levels. Unlike a regular MIN() aggregation, the window function allows us to see how each student individually compares to the lowest test score for their grade level. SELECT First_name, Grade_level, Test_score, MIN(Test_score) OVER (PARTITION BY Grade_level) AS Lowest_score FROM class_test Returns: +------------+-------------+------------+--------------+ | First_name | Grade_level | Test_score | Lowest_score | +------------+-------------+------------+--------------+ | Frank | 9 | 76 | 76 | | Humphrey | 9 | 90 | 76 | | Iris | 9 | 79 | 76 | | Sammy | 9 | 85 | 76 | | Peter | 9 | 80 | 76 | | Jojo | 9 | 78 | 76 | | Brunhilda | 12 | 92 | 66 | | Franco | 12 | 94 | 66 | | Thomas | 12 | 66 | 66 | | Gary | 12 | 100 | 66 | | Charles | 12 | 93 | 66 | | Jesse | 12 | 89 | 66 | | Roseanna | 11 | 94 | 52 | | Carol | 11 | 52 | 52 | | Wanda | 11 | 73 | 52 | | Shangxiu | 11 | 76 | 52 | | Larry | 11 | 68 | 52 | | Otis | 11 | 75 | 52 | | Deborah | 10 | 78 | 30 | | Yolinda | 10 | 30 | 30 | | Albert | 10 | 59 | 30 | | Mary | 10 | 85 | 30 | | Shawn | 10 | 89 | 30 | +------------+-------------+------------+--------------+ MAX Returns the maximum value within the requested window. Syntax MAX( <exp> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the MAX function. <exp> An expression used for the PARTITION BY clause. Example The example below queries test scores for students in various grade levels. Unlike a regular MAX() aggregation, the window function allows us to see how each student individually compares to the highest test score for their grade level. SELECT First_name, Grade_level, Test_score, MAX(Test_score) OVER (PARTITION BY Grade_level) AS Highest_score FROM class_test Returns: +------------+-------------+------------+---------------+ | First_name | Grade_level | Test_score | Highest_score | +------------+-------------+------------+---------------+ | Frank | 9 | 76 | 90 | | Humphrey | 9 | 90 | 90 | | Iris | 9 | 79 | 90 | | Sammy | 9 | 85 | 90 | | Peter | 9 | 80 | 90 | | Jojo | 9 | 78 | 90 | | Brunhilda | 12 | 92 | 100 | | Franco | 12 | 94 | 100 | | Thomas | 12 | 66 | 100 | | Gary | 12 | 100 | 100 | | Charles | 12 | 93 | 100 | | Jesse | 12 | 89 | 100 | | Roseanna | 11 | 94 | 94 | | Carol | 11 | 52 | 94 | | Wanda | 11 | 73 | 94 | | Shangxiu | 11 | 76 | 94 | | Larry | 11 | 68 | 94 | | Otis | 11 | 75 | 94 | | Deborah | 10 | 78 | 89 | | Yolinda | 10 | 30 | 89 | | Albert | 10 | 59 | 89 | | Mary | 10 | 85 | 89 | | Shawn | 10 | 89 | 89 | +------------+-------------+------------+---------------+ RANK Rank the current row within the requested window with gaps. Syntax RANK() OVER ([PARTITION BY <exp>] ORDER BY <exp> [ASC|DESC] ) Parameter Description <val> The expression used for the PARTITION BY clause. <exp> The expression used in the ORDER BY clause. This parameter determines what value will be ranked. Example In this example below, students are ranked based on their test scores for their grade level. SELECT First_name, Grade_level, Test_score, RANK() OVER (PARTITION BY Grade_level ORDER BY Test_score DESC ) AS Rank_in_class FROM class_test Returns: +------------+-------------+------------+---------------+ | First_name | Grade_level | Test_score | Rank_in_class | +------------+-------------+------------+---------------+ | Frank | 9 | 76 | 6 | | Humphrey | 9 | 90 | 1 | | Iris | 9 | 79 | 4 | | Sammy | 9 | 85 | 2 | | Peter | 9 | 80 | 3 | | Jojo | 9 | 78 | 5 | | Brunhilda | 12 | 92 | 4 | | Franco | 12 | 94 | 2 | | Thomas | 12 | 66 | 6 | | Gary | 12 | 100 | 1 | | Charles | 12 | 93 | 3 | | Jesse | 12 | 89 | 5 | | Roseanna | 11 | 94 | 1 | | Carol | 11 | 52 | 6 | | Wanda | 11 | 73 | 4 | | Shangxiu | 11 | 76 | 2 | | Larry | 11 | 68 | 5 | | Otis | 11 | 75 | 3 | | Deborah | 10 | 78 | 3 | | Yolinda | 10 | 30 | 5 | | Albert | 10 | 59 | 4 | | Mary | 10 | 85 | 2 | | Shawn | 10 | 89 | 1 | +------------+-------------+------------+---------------+ ROW_NUMBER Returns a unique row number for each row within the requested window. Syntax ROW_NUMBER() OVER ([PARTITION BY <exp>] ORDER BY <exp> [ASC|DESC] ) Parameter Desccription <val> The expression used for the PARTITION BY clause. <exp> The expression used in the ORDER BY clause. This parameter determines what value will be used for ROW_NUMBER . Example In this example below, students in each grade level are assigned a unique number. SELECT First_name, Grade_level, ROW_NUMBER() OVER (PARTITION BY Grade_level ORDER BY Grade_level ASC ) AS Student_No FROM class_test Returns: +------------+-------------+------------+ | First_name | Grade_level | Student_No | +------------+-------------+------------+ | Frank | 9 | 1 | | Humphrey | 9 | 2 | | Iris | 9 | 3 | | Sammy | 9 | 4 | | Peter | 9 | 5 | | Jojo | 9 | 6 | | Brunhilda | 12 | 1 | | Franco | 12 | 2 | | Thomas | 12 | 3 | | Gary | 12 | 4 | | Charles | 12 | 5 | | Jesse | 12 | 6 | | Roseanna | 11 | 1 | | Carol | 11 | 2 | | Wanda | 11 | 3 | | Shangxiu | 11 | 4 | | Larry | 11 | 5 | | Otis | 11 | 6 | | Deborah | 10 | 1 | | Yolinda | 10 | 2 | | Albert | 10 | 3 | | Mary | 10 | 4 | | Shawn | 10 | 5 | +------------+-------------+------------+ SUM Calculate the sum of the values within the requested window. The SUM function works with numeric values and ignores NULL values. Syntax SUM( <val> ) OVER ( [ PARTITION BY <expr> ] ) Parameter Description <val> The expression used for the SUM function <expr> An expression used for the PARTITION BY clause Example The example below shows how many vaccinated students are in the same grade level for each student. SELECT First_name, SUM(Vaccinated) OVER (PARTITION BY Grade_level ) AS Vaccinated_Students FROM class_test Returns: +------------+---------------------+ | First_name | Vaccinated_Students | +------------+---------------------+ | Frank | 5 | | Humphrey | 5 | | Iris | 5 | | Sammy | 5 | | Peter | 5 | | Jojo | 5 | | Brunhilda | 5 | | Franco | 5 | | Thomas | 5 | | Gary | 5 | | Charles | 5 | | Jesse | 5 | | Roseanna | 4 | | Carol | 4 | | Wanda | 4 | | Shangxiu | 4 | | Larry | 4 | | Otis | 4 | | Deborah | 4 | | Yolinda | 4 | | Albert | 4 | | Mary | 4 | | Shawn | 4 | +------------+---------------------+","title":"Window Functions"},{"location":"sql-reference/functions-reference/window-functions/#window-functions","text":"A window function performs a calculation across a set of table rows. Unlike regular aggregate functions, the use of a window function does not cause rows to become grouped into a single output row but allows the rows to retain their separate identities. Window functions are identified by the OVER() syntax. To narrow down the window dataset into individual groups, use the PARTITION BY clause. In case this clause is not used, a single window is being created. This page describes the window functions supported in Firebolt.","title":"Window Functions"},{"location":"sql-reference/functions-reference/window-functions/#avg","text":"Returns the average value within the requested window. Syntax AVG( <val> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the AVG() function. <expr> An expression used for the partition by clause. Example The example below is querying test scores for students in various grade levels. Unlike a regular AVG() aggregation, the window function allows us to see how each student individually compares to the average test score for their grade level. SELECT First_name, Grade_level, Test_score, AVG(Test_score) OVER (PARTITION BY Grade_level) AS average_for_grade FROM class_test Results ' +------------+-------------+------------+-------------------------+ ' | First_name | Grade_level | Test_score | average_for_grade | ' +------------+-------------+------------+-------------------------+ ' | Frank | 9 | 76 | 81.33333333333333 | ' | Humphrey | 9 | 90 | 81.33333333333333 | ' | Iris | 9 | 79 | 81.33333333333333 | ' | Sammy | 9 | 85 | 81.33333333333333 | ' | Peter | 9 | 80 | 81.33333333333333 | ' | Jojo | 9 | 78 | 81.33333333333333 | ' | Brunhilda | 12 | 92 | 89 | ' | Franco | 12 | 94 | 89 | ' | Thomas | 12 | 66 | 89 | ' | Gary | 12 | 100 | 89 | ' | Charles | 12 | 93 | 89 | ' | Jesse | 12 | 89 | 89 | ' | Roseanna | 11 | 94 | 73 | ' | Carol | 11 | 52 | 73 | ' | Wanda | 11 | 73 | 73 | ' | Shangxiu | 11 | 76 | 73 | ' | Larry | 11 | 68 | 73 | ' | Otis | 11 | 75 | 73 | ' | Deborah | 10 | 78 | 68.2 | ' | Yolinda | 10 | 30 | 68.2 | ' | Albert | 10 | 59 | 68.2 | ' | Mary | 10 | 85 | 68.2 | ' | Shawn | 10 | 89 | 68.2 | ' +------------+-------------+------------+-------------------------+","title":"AVG"},{"location":"sql-reference/functions-reference/window-functions/#count","text":"Count the number of values within the requested window. Syntax COUNT( <val> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the COUNT() function. <expr> An expression used for the PARTITION BY clause Example This example below generates a count of how many students are in each grade level while leaving each student as an independent row. SELECT First_name, Grade_level, COUNT(First_name) OVER (PARTITION BY Grade_level) AS count_of_students FROM class_test Results +------------+-------------+-------------------+ | First_name | Grade_level | count_of_students | +------------+-------------+-------------------+ | Frank | 9 | 6 | | Humphrey | 9 | 6 | | Iris | 9 | 6 | | Sammy | 9 | 6 | | Peter | 9 | 6 | | Jojo | 9 | 6 | | Brunhilda | 12 | 6 | | Franco | 12 | 6 | | Thomas | 12 | 6 | | Gary | 12 | 6 | | Charles | 12 | 6 | | Jesse | 12 | 6 | | Roseanna | 11 | 6 | | Carol | 11 | 6 | | Wanda | 11 | 6 | | Shangxiu | 11 | 6 | | Larry | 11 | 6 | | Otis | 11 | 6 | | Deborah | 10 | 5 | | Yolinda | 10 | 5 | | Albert | 10 | 5 | | Mary | 10 | 5 | | Shawn | 10 | 5 | +------------+-------------+-------------------+","title":"COUNT"},{"location":"sql-reference/functions-reference/window-functions/#dense_rank","text":"Rank the current row within the requested window. Syntax DENSE_RANK() OVER ([PARTITION BY <val>] ORDER BY <exp> [ASC|DESC] ) Parameter Description <val> The expression used for the PARTITION BY clause. \\ The expression used in the ORDER BY clause. This parameter determines what value will be ranked. Example In this example below, students are ranked based on their test scores for their grade level. SELECT First_name, Grade_level, Test_score, DENSE_RANK() OVER (PARTITION BY Grade_level ORDER BY Test_score DESC ) AS Rank_in_class FROM class_test Returns: +------------+-------------+------------+---------------+ | First_name | Grade_level | Test_score | Rank_in_class | +------------+-------------+------------+---------------+ | Frank | 9 | 76 | 6 | | Humphrey | 9 | 90 | 1 | | Iris | 9 | 79 | 4 | | Sammy | 9 | 85 | 2 | | Peter | 9 | 80 | 3 | | Jojo | 9 | 78 | 5 | | Deborah | 10 | 78 | 3 | | Yolinda | 10 | 30 | 5 | | Albert | 10 | 59 | 4 | | Mary | 10 | 85 | 2 | | Shawn | 10 | 89 | 1 | | Roseanna | 11 | 94 | 1 | | Carol | 11 | 52 | 6 | | Wanda | 11 | 73 | 4 | | Shangxiu | 11 | 76 | 2 | | Larry | 11 | 68 | 5 | | Otis | 11 | 75 | 3 | | Brunhilda | 12 | 92 | 4 | | Franco | 12 | 94 | 2 | | Thomas | 12 | 66 | 6 | | Gary | 12 | 100 | 1 | | Charles | 12 | 93 | 3 | | Jesse | 12 | 89 | 5 | +------------+-------------+------------+---------------+","title":"DENSE_RANK"},{"location":"sql-reference/functions-reference/window-functions/#lag","text":"Returns the value of the input expression at the given offset before the current row within the requested window. Syntax LAG ( <exp> [, <offset> [, <default> ]] ) OVER ( [ PARTITION BY <exp> ] ORDER BY <exp> [ { ASC | DESC } ] ) Parameter Description <val> Any valid expression that will be returned based on the LAG <offset>. <expr> The expression used for the PARTITION BY clause. <offset> The number of rows backward from the current row from which to obtain a value. A negative number will act as LEAD() <default> The expression to return when the offset goes out of the bounds of the window. Must be a literal INT . The default is NULL. Example In the example below, the LAG function is being used to find the students in each grade level who are sitting next to each other. In some cases, a student does not have an adjacent classmate, so the LAG function returns NULL . SELECT First_name, Grade_level, LAG(First_name, 1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_left, LAG(First_name, -1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_right FROM class_test Returns: +------------+-------------+-------------+--------------+ | First_name | Grade_level | To_the_left | To_the_right | +------------+-------------+-------------+--------------+ | Frank | 9 | NULL | Humphrey | | Humphrey | 9 | Frank | Iris | | Iris | 9 | Humphrey | Jojo | | Sammy | 9 | Peter | NULL | | Peter | 9 | Jojo | Sammy | | Jojo | 9 | Iris | Peter | | Brunhilda | 12 | NULL | Charles | | Franco | 12 | Charles | Gary | | Thomas | 12 | Jesse | NULL | | Gary | 12 | Franco | Jesse | | Charles | 12 | Brunhilda | Franco | | Jesse | 12 | Gary | Thomas | | Roseanna | 11 | Otis | Shangxiu | | Carol | 11 | NULL | Larry | | Wanda | 11 | Shangxiu | NULL | | Shangxiu | 11 | Roseanna | Wanda | | Larry | 11 | Carol | Otis | | Otis | 11 | Larry | Roseanna | | Deborah | 10 | Albert | Mary | | Yolinda | 10 | Shawn | NULL | | Albert | 10 | NULL | Deborah | | Mary | 10 | Deborah | Shawn | | Shawn | 10 | Mary | Yolinda | +------------+-------------+-------------+--------------+","title":"LAG"},{"location":"sql-reference/functions-reference/window-functions/#lead","text":"Returns values from the row after the current row within the requested window. Syntax LEAD ( <val> [, <offset> [, <default> ] ) OVER ( [ PARTITION BY <exp> ] ORDER BY <exp> [ { ASC | DESC } ] ) Parameter Description <val> Any valid expression that will be returned based on the LEAD <offset>. <expr> The expression used for the PARTITION BY clause. <offset> The number of rows forward from the current row from which to obtain a value. <default> The expression to return when the offset goes out of the bounds of the window. Supports any expression whose type is compatible with expression. The default is NULL. Example In the example below, the LEAD function is being used to find the students in each grade level who are sitting next to each other. In some cases, a student does not have an adjacent classmate, so the LEAD function returns NULL . SELECT First_name, Grade_level, LEAD(First_name, -1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_left, LEAD(First_name, 1) OVER (PARTITION BY Grade_level ORDER BY First_name ) AS To_the_right FROM class_test; Returns: +------------+-------------+-------------+--------------+ | First_name | Grade_level | To_the_left | To_the_right | +------------+-------------+-------------+--------------+ | Frank | 9 | NULL | Humphrey | | Humphrey | 9 | Frank | Iris | | Iris | 9 | Humphrey | Jojo | | Sammy | 9 | Peter | NULL | | Peter | 9 | Jojo | Sammy | | Jojo | 9 | Iris | Peter | | Brunhilda | 12 | NULL | Charles | | Franco | 12 | Charles | Gary | | Thomas | 12 | Jesse | NULL | | Gary | 12 | Franco | Jesse | | Charles | 12 | Brunhilda | Franco | | Jesse | 12 | Gary | Thomas | | Roseanna | 11 | Otis | Shangxiu | | Carol | 11 | NULL | Larry | | Wanda | 11 | Shangxiu | NULL | | Shangxiu | 11 | Roseanna | Wanda | | Larry | 11 | Carol | Otis | | Otis | 11 | Larry | Roseanna | | Deborah | 10 | Albert | Mary | | Yolinda | 10 | Shawn | NULL | | Albert | 10 | NULL | Deborah | | Mary | 10 | Deborah | Shawn | | Shawn | 10 | Mary | Yolinda | +------------+-------------+-------------+--------------+","title":"LEAD"},{"location":"sql-reference/functions-reference/window-functions/#min","text":"Returns the minimum value within the requested window. Syntax MIN( <exp> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the MIN function. <exp> An expression used for the PARTITION BY clause. Example The example below queries test scores for students in various grade levels. Unlike a regular MIN() aggregation, the window function allows us to see how each student individually compares to the lowest test score for their grade level. SELECT First_name, Grade_level, Test_score, MIN(Test_score) OVER (PARTITION BY Grade_level) AS Lowest_score FROM class_test Returns: +------------+-------------+------------+--------------+ | First_name | Grade_level | Test_score | Lowest_score | +------------+-------------+------------+--------------+ | Frank | 9 | 76 | 76 | | Humphrey | 9 | 90 | 76 | | Iris | 9 | 79 | 76 | | Sammy | 9 | 85 | 76 | | Peter | 9 | 80 | 76 | | Jojo | 9 | 78 | 76 | | Brunhilda | 12 | 92 | 66 | | Franco | 12 | 94 | 66 | | Thomas | 12 | 66 | 66 | | Gary | 12 | 100 | 66 | | Charles | 12 | 93 | 66 | | Jesse | 12 | 89 | 66 | | Roseanna | 11 | 94 | 52 | | Carol | 11 | 52 | 52 | | Wanda | 11 | 73 | 52 | | Shangxiu | 11 | 76 | 52 | | Larry | 11 | 68 | 52 | | Otis | 11 | 75 | 52 | | Deborah | 10 | 78 | 30 | | Yolinda | 10 | 30 | 30 | | Albert | 10 | 59 | 30 | | Mary | 10 | 85 | 30 | | Shawn | 10 | 89 | 30 | +------------+-------------+------------+--------------+","title":"MIN"},{"location":"sql-reference/functions-reference/window-functions/#max","text":"Returns the maximum value within the requested window. Syntax MAX( <exp> ) OVER ( [ PARTITION BY <exp> ] ) Parameter Description <val> An expression used for the MAX function. <exp> An expression used for the PARTITION BY clause. Example The example below queries test scores for students in various grade levels. Unlike a regular MAX() aggregation, the window function allows us to see how each student individually compares to the highest test score for their grade level. SELECT First_name, Grade_level, Test_score, MAX(Test_score) OVER (PARTITION BY Grade_level) AS Highest_score FROM class_test Returns: +------------+-------------+------------+---------------+ | First_name | Grade_level | Test_score | Highest_score | +------------+-------------+------------+---------------+ | Frank | 9 | 76 | 90 | | Humphrey | 9 | 90 | 90 | | Iris | 9 | 79 | 90 | | Sammy | 9 | 85 | 90 | | Peter | 9 | 80 | 90 | | Jojo | 9 | 78 | 90 | | Brunhilda | 12 | 92 | 100 | | Franco | 12 | 94 | 100 | | Thomas | 12 | 66 | 100 | | Gary | 12 | 100 | 100 | | Charles | 12 | 93 | 100 | | Jesse | 12 | 89 | 100 | | Roseanna | 11 | 94 | 94 | | Carol | 11 | 52 | 94 | | Wanda | 11 | 73 | 94 | | Shangxiu | 11 | 76 | 94 | | Larry | 11 | 68 | 94 | | Otis | 11 | 75 | 94 | | Deborah | 10 | 78 | 89 | | Yolinda | 10 | 30 | 89 | | Albert | 10 | 59 | 89 | | Mary | 10 | 85 | 89 | | Shawn | 10 | 89 | 89 | +------------+-------------+------------+---------------+","title":"MAX"},{"location":"sql-reference/functions-reference/window-functions/#rank","text":"Rank the current row within the requested window with gaps. Syntax RANK() OVER ([PARTITION BY <exp>] ORDER BY <exp> [ASC|DESC] ) Parameter Description <val> The expression used for the PARTITION BY clause. <exp> The expression used in the ORDER BY clause. This parameter determines what value will be ranked. Example In this example below, students are ranked based on their test scores for their grade level. SELECT First_name, Grade_level, Test_score, RANK() OVER (PARTITION BY Grade_level ORDER BY Test_score DESC ) AS Rank_in_class FROM class_test Returns: +------------+-------------+------------+---------------+ | First_name | Grade_level | Test_score | Rank_in_class | +------------+-------------+------------+---------------+ | Frank | 9 | 76 | 6 | | Humphrey | 9 | 90 | 1 | | Iris | 9 | 79 | 4 | | Sammy | 9 | 85 | 2 | | Peter | 9 | 80 | 3 | | Jojo | 9 | 78 | 5 | | Brunhilda | 12 | 92 | 4 | | Franco | 12 | 94 | 2 | | Thomas | 12 | 66 | 6 | | Gary | 12 | 100 | 1 | | Charles | 12 | 93 | 3 | | Jesse | 12 | 89 | 5 | | Roseanna | 11 | 94 | 1 | | Carol | 11 | 52 | 6 | | Wanda | 11 | 73 | 4 | | Shangxiu | 11 | 76 | 2 | | Larry | 11 | 68 | 5 | | Otis | 11 | 75 | 3 | | Deborah | 10 | 78 | 3 | | Yolinda | 10 | 30 | 5 | | Albert | 10 | 59 | 4 | | Mary | 10 | 85 | 2 | | Shawn | 10 | 89 | 1 | +------------+-------------+------------+---------------+","title":"RANK"},{"location":"sql-reference/functions-reference/window-functions/#row_number","text":"Returns a unique row number for each row within the requested window. Syntax ROW_NUMBER() OVER ([PARTITION BY <exp>] ORDER BY <exp> [ASC|DESC] ) Parameter Desccription <val> The expression used for the PARTITION BY clause. <exp> The expression used in the ORDER BY clause. This parameter determines what value will be used for ROW_NUMBER . Example In this example below, students in each grade level are assigned a unique number. SELECT First_name, Grade_level, ROW_NUMBER() OVER (PARTITION BY Grade_level ORDER BY Grade_level ASC ) AS Student_No FROM class_test Returns: +------------+-------------+------------+ | First_name | Grade_level | Student_No | +------------+-------------+------------+ | Frank | 9 | 1 | | Humphrey | 9 | 2 | | Iris | 9 | 3 | | Sammy | 9 | 4 | | Peter | 9 | 5 | | Jojo | 9 | 6 | | Brunhilda | 12 | 1 | | Franco | 12 | 2 | | Thomas | 12 | 3 | | Gary | 12 | 4 | | Charles | 12 | 5 | | Jesse | 12 | 6 | | Roseanna | 11 | 1 | | Carol | 11 | 2 | | Wanda | 11 | 3 | | Shangxiu | 11 | 4 | | Larry | 11 | 5 | | Otis | 11 | 6 | | Deborah | 10 | 1 | | Yolinda | 10 | 2 | | Albert | 10 | 3 | | Mary | 10 | 4 | | Shawn | 10 | 5 | +------------+-------------+------------+","title":"ROW_NUMBER"},{"location":"sql-reference/functions-reference/window-functions/#sum","text":"Calculate the sum of the values within the requested window. The SUM function works with numeric values and ignores NULL values. Syntax SUM( <val> ) OVER ( [ PARTITION BY <expr> ] ) Parameter Description <val> The expression used for the SUM function <expr> An expression used for the PARTITION BY clause Example The example below shows how many vaccinated students are in the same grade level for each student. SELECT First_name, SUM(Vaccinated) OVER (PARTITION BY Grade_level ) AS Vaccinated_Students FROM class_test Returns: +------------+---------------------+ | First_name | Vaccinated_Students | +------------+---------------------+ | Frank | 5 | | Humphrey | 5 | | Iris | 5 | | Sammy | 5 | | Peter | 5 | | Jojo | 5 | | Brunhilda | 5 | | Franco | 5 | | Thomas | 5 | | Gary | 5 | | Charles | 5 | | Jesse | 5 | | Roseanna | 4 | | Carol | 4 | | Wanda | 4 | | Shangxiu | 4 | | Larry | 4 | | Otis | 4 | | Deborah | 4 | | Yolinda | 4 | | Albert | 4 | | Mary | 4 | | Shawn | 4 | +------------+---------------------+","title":"SUM"},{"location":"sql-reference/functions-reference/semi-structured-functions/","text":"Semi-structured data functions Firebolt supports loading and manipulating semi-structured data such as JSON. This, and similar formats (e.g. Avro, Parquet) can contain compound types such as arrays, maps, and structs. Firebolt uses its native ARRAY type to model and query such semi-structured data. The raw JSON input can be transformed into Firebolt's arrays during ingestion, or stored as a plain TEXT column. In both cases, JSON functions will be used to transform the nested, compound JSON types to Firebolt arrays, while array functions , including Aggregate Array Functions, will be used to manipulate and query the semi-structured data. Array functions - used for the manipulation and querying of array typed columns, such as transformation , filtering , and un-nesting - an operation that converts the array to a regular column. Aggregate array functions - these functions work on array-typed columns, but instead of being applied row by row, they combine the results or all the array belonging to the groups defined by the GROUP BY clause. JSON functions - these function extract and transform raw JSON into Firebolt native types, or JSON sub-objects. They are used either during the ELT process or applied to columns storing JSON objects as plain TEXT","title":"Semi-structured data functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/#semi-structured-data-functions","text":"Firebolt supports loading and manipulating semi-structured data such as JSON. This, and similar formats (e.g. Avro, Parquet) can contain compound types such as arrays, maps, and structs. Firebolt uses its native ARRAY type to model and query such semi-structured data. The raw JSON input can be transformed into Firebolt's arrays during ingestion, or stored as a plain TEXT column. In both cases, JSON functions will be used to transform the nested, compound JSON types to Firebolt arrays, while array functions , including Aggregate Array Functions, will be used to manipulate and query the semi-structured data. Array functions - used for the manipulation and querying of array typed columns, such as transformation , filtering , and un-nesting - an operation that converts the array to a regular column. Aggregate array functions - these functions work on array-typed columns, but instead of being applied row by row, they combine the results or all the array belonging to the groups defined by the GROUP BY clause. JSON functions - these function extract and transform raw JSON into Firebolt native types, or JSON sub-objects. They are used either during the ELT process or applied to columns storing JSON objects as plain TEXT","title":"Semi-structured data functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/aggregate-array-functions/","text":"Aggregate array functions Aggregate semi-structured functions work globally on all the arrays in a given column expression, instead of a row-by-row application. At their simplest form (without a GROUP BY clause) - they will provide the result of globally applying the function on all of the elements of the arrays in the column expression specified as their argument. For example, ARRAY_SUM_GLOBAL will return the sum of all the elements in all the array of the given column. ARRAY_MAX_GLOBAL will return the maximum element among all of the elements in all of the arrays in the given column expression. When combined with a GROUP BY clause, these operations will be performed on all of the arrays in each group. From the remainder of this page we will use the following table T in our examples: Category vals a [1,3,4] b [3,5,6,7] a [30,50,60] ARRAY_MAX_GLOBAL Returns the maximum element from all the array elements in each group. Syntax ARRAY_MAX_GLOBAL(<arr>) AS cnt Parameter Description <arr> The array column over from which the function returns the maximum element Example SELECT Category, ARRAY_MAX_GLOBAL(vals) AS mx FROM T GROUP BY Category; Returns : category mx a 4 b 7 c 60 ARRAY_MIN_GLOBAL Returns the minimal element taken from all the array elements in each group. Syntax ARRAY_MIN_GLOBAL(<arr>) Parameter Description <arr> The array column from which the function will return the minimal element Example SELECT Category, ARRAY_MIN_GLOBAL(vals) AS mn FROM T GROUP BY Category; Returns : category sm a 1 b 3 c 30 ARRAY_SUM_GLOBAL Returns the sum of elements in the array column accumulated over the rows in each group. Syntax ARRAY_SUM_GLOBAL(<arr>) Parameter Description <arr> The array column over which the function will sum the elements Example SELECT Category, ARRAY_SUM_GLOBAL(vals) AS sm FROM T GROUP BY Category; Returns : category sm a 8 b 21 c 140","title":"Aggregate array functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/aggregate-array-functions/#aggregate-array-functions","text":"Aggregate semi-structured functions work globally on all the arrays in a given column expression, instead of a row-by-row application. At their simplest form (without a GROUP BY clause) - they will provide the result of globally applying the function on all of the elements of the arrays in the column expression specified as their argument. For example, ARRAY_SUM_GLOBAL will return the sum of all the elements in all the array of the given column. ARRAY_MAX_GLOBAL will return the maximum element among all of the elements in all of the arrays in the given column expression. When combined with a GROUP BY clause, these operations will be performed on all of the arrays in each group. From the remainder of this page we will use the following table T in our examples: Category vals a [1,3,4] b [3,5,6,7] a [30,50,60]","title":"Aggregate array functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/aggregate-array-functions/#array_max_global","text":"Returns the maximum element from all the array elements in each group. Syntax ARRAY_MAX_GLOBAL(<arr>) AS cnt Parameter Description <arr> The array column over from which the function returns the maximum element Example SELECT Category, ARRAY_MAX_GLOBAL(vals) AS mx FROM T GROUP BY Category; Returns : category mx a 4 b 7 c 60","title":"ARRAY_MAX_GLOBAL"},{"location":"sql-reference/functions-reference/semi-structured-functions/aggregate-array-functions/#array_min_global","text":"Returns the minimal element taken from all the array elements in each group. Syntax ARRAY_MIN_GLOBAL(<arr>) Parameter Description <arr> The array column from which the function will return the minimal element Example SELECT Category, ARRAY_MIN_GLOBAL(vals) AS mn FROM T GROUP BY Category; Returns : category sm a 1 b 3 c 30","title":"ARRAY_MIN_GLOBAL"},{"location":"sql-reference/functions-reference/semi-structured-functions/aggregate-array-functions/#array_sum_global","text":"Returns the sum of elements in the array column accumulated over the rows in each group. Syntax ARRAY_SUM_GLOBAL(<arr>) Parameter Description <arr> The array column over which the function will sum the elements Example SELECT Category, ARRAY_SUM_GLOBAL(vals) AS sm FROM T GROUP BY Category; Returns : category sm a 8 b 21 c 140","title":"ARRAY_SUM_GLOBAL"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/","text":"Array Functions This page describes the functions for working with arrays. ALL_MATCH Returns 1 if all elements of an array match the results of the function provided in the <func> parameter, otherwise returns 0 . Syntax ALL_MATCH(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be matched with the function. The array cannot be empty. Examples SELECT ALL_MATCH(x -> x > 0, [1,2,3,9]) AS res; Returns : 1 SELECT ALL_MATCH(x -> x > 10, [1,2,3,9]) AS res; **Returns: ** 0 ANY_MATCH Returns 1 if at least one of the elements of an array matches the results of the function provided in the <func> parameter. Otherwise returns 0 . Syntax \u200b\u200bANY_MATCH(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be matched with the function. The array cannot be empty Examples SELECT ANY_MATCH(x -> x > 3, [1,2,3,9]) AS res; Returns : 1 SELECT ANY_MATCH(x -> x = 10, [ 1, 2, 3, 9 ]) AS res; Returns : 0 ARRAY_CONCAT Combines one or more arrays that are passed as arguments. Syntax ARRAY_CONCAT(<arr1> [, ...n]) Parameter Description <arr> [, ...n] The arrays to be combined. If only one array is given, an identical array is returned. Example SELECT ARRAY_CONCAT([ 1, 2, 3, 4 ], [ 5, 6, 7, 8 ]) AS res; ARRAY_COUNT Returns the number of elements in the <arr> array that match a specified function <func> . Syntax ARRAY_COUNT(<func>, <arr>) Parameter Description <func> Optional. A Lambda function used to check elements in the array. If <func> is not included, ARRAY_COUNT will return a count of all elements in the array. <arr> An array of elements Examples The example below searches through the array for any elements that are greater than 3. Only one number that matches this criteria is found, so the function returns 1 SELECT ARRAY_COUNT(x -> x > 3, [1,2,3,9]) AS res; Returns : 1 In this example below, there is no <func> criteria provided in the ARRAY_COUNT function. This means the function will count all of the elements in the given array. SELECT ARRAY_COUNT([ 1, 2, 3, 9 ]) AS res; **Returns: ** 4 ARRAY_COUNT_GLOBAL Returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_COUNT_GLOBAL(<arr_col>) Parameter Description <arr_col> The array column over which the function will count the elements Example For this example, we will create a table array_test as shown below. CREATE DIMENSION TABLE array_test( array_1 ARRAY(INT) ); INSERT INTO array_test VALUES ([ 1, 2, 3, 4 ]), ([ 5, 0, 20 ]), ([ 6, 2, 6 ]), ([ 9, 10, 13 ]), ([ 20, 13, 40 ]), ([ 1 ]); We can use ARRAY_COUNT_GLOBAL to learn how many total array elements are in all rows. SELECT ARRAY_COUNT_GLOBAL(array_1) FROM array_test **Returns: ** 17 If you want to count elements based on specific criteria, you can use the ARRAY_COUNT function with a SUM aggregation as demonstrated below. SELECT SUM(ARRAY_COUNT(x -> x > 3, array_1)) FROM array_test **Returns: ** 11 ARRAY_CUMULATIVE_SUM Returns an array of partial sums of elements from the source array (a running sum). If the argument <func> is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_CUMULATIVE_SUM( [<func>,] arr) Parameter Description <func> The function used to convert the array members. <arr> The array used for the sum calculations. Examples SELECT ARRAY_CUMULATIVE_SUM(x -> x + 1, [ 1, 2, 3, 9 ]) AS res; Returns : 2,5,9,19 SELECT ARRAY_CUMULATIVE_SUM([ 1, 2, 3, 9 ]) AS res Returns : 1,3,6,15 ARRAY_DISTINCT Returns an array containing only the unique elements of the given array. In other words, if the given array contains multiple identical members, the returned array will include only a single member of that value. Syntax ARRAY_DISTINCT(<arr>) Parameter Description <arr> The array to be analyzed for unique members. Example SELECT ARRAY_DISTINCT([ 1, 1, 2, 2, 3, 4 ]) AS res; Returns : 1,2,3,4 ARRAY_FILL This function scans through the given array <arr> from the first to the last element and replaces arr[i] with arr[i - 1] if the <func> returns 0 . The first element of the given array is not replaced. The lambda function <func> is mandatory. Syntax ARRAY_FILL(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be evaluated by the function. Examples SELECT ARRAY_FILL(x -> x < 0,[1,2,3,9]) AS res; Returns : 1,1,1,1 SELECT ARRAY_FILL(x -> x > 0,[1,2,3,9]) AS res; Returns : 1,2,3,9 ARRAY_FIRST Returns the first element in the given array for which the given <func> function returns something other than 0 . The <func> argument must be included. Syntax ARRAY_FIRST(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array evaluated by the function. Example SELECT ARRAY_FIRST(x -> x > 2,[1,2,3,9]) AS res; Returns : 3 ARRAY_FIRST_INDEX Returns the index of the first element in the indicated array for which the given <func> function returns something other than 0 . Index counting starts at 1. The <func> argument must be included. Syntax ARRAY_FIRST_INDEX(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array evaluated by the function. Example SELECT ARRAY_FIRST_INDEX(x -> x > 2,[1,2,3,9]) AS res; Returns : 3 ARRAY_INTERSECT Evaluates all arrays that are provided as arguments and returns an array of any elements that are present in all the arrays. The order of the resulting array is the same as in the first array. Syntax ARRAY_INTERSECT(<arr>) Parameter Description <arr> A series of arrays to be analyzed for mutual elements. Example In the example below, the only element that is shared between all three arrays is 3. SELECT ARRAY_INTERSECT([1, 2, 3], [1, 3], [2, 3]) Returns : 3 ARRAY_JOIN Concatenates an array of TEXT elements using an optional delimiter. If no delimiter is provided, an empty string is used instead. Syntax ARRAY_JOIN(<arr>[, <delimiter>]) Parameter Description <arr> An array of TEXT elements. <delimiter> The delimiter used for joining the array elements. If you omit this value, an empty string is being used as a delimiter. Examples In the example below, the three elements are joined with no delimiter. SELECT ARRAY_JOIN(['1', '2', '3']) AS res; Returns : 123 In this example below, we are providing a comma delimiter. SELECT ARRAY_JOIN(['a', 'b', 'c'], ',') AS res; **Returns: ** a,b,c ARRAY_MAX Returns the maximum element in an array <arr> . Syntax ARRAY_MAX(<arr>) Parameter Description <arr> The array or array-type column to be checked Example SELECT ARRAY_MAX([1,2,3,4]) AS res; Returns : 4 ARRAY_MIN Returns the minimum element in <arr> . Syntax ARRAY_MIN(<arr>) Parameter Description <arr> The array or array-type column to be checked Example SELECT ARRAY_MIN([1,2,3,4]) AS res; Returns : 1 ARRAY_REPLACE_BACKWARDS Scans an array <arr> from the last to the first element and replaces each of the elements in that array with arr[i + 1] if the <func> returns 0 . The last element of <arr> is not replaced. The <func> argument must be included. Syntax ARRAY_REPLACE_BACKWARDS(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be evaluated by the function. Example SELECT ARRAY_REPLACE_BACKWARDS(x -> x > 2,[1,2,3,9]) AS res; Returns : 3,3,3,9 ARRAY_REVERSE Returns an array of the same size as the original array, with the elements in reverse order. Syntax ARRAY_REVERSE(<arr>) Parameter Description <arr> The array to be reversed. Example SELECT ARRAY_REVERSE([1, 2, 3, 6]) AS res Returns : 6,3,2,1 ARRAY_SORT Returns the elements of <arr> in ascending order. If the argument <func> is provided, the sorting order is determined by the result of applying <func> on each element of <arr> . Syntax ARRAY_SORT([<func>,] <arr>) Parameter Description <func> An optional function to be used to determine the sort order. <arr> The array to be sorted. Examples SELECT ARRAY_SORT([4,1,3,2]) AS res; Returns : 1,2,3,4 In this example below, the modulus operator is used to calculate the remainder on any odd numbers. Therefore ARRAY_ SORT puts the higher (odd) numbers last in the results. SELECT ARRAY_SORT(x -> x % 2, [4,1,3,2]) AS res; Returns : 4,2,1,3 ARRAY_SUM Returns the sum of elements of <arr> . If the argument <func> is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_SUM([<func>,] <arr>) Parameter Description <func> A Lambda function with an arithmetic function used to modify the array elements. <arr> The array to be used to calculate the function. Examples This example below uses a function to first add 1 to all elements before calculating the sum: SELECT ARRAY_SUM(x -> x + 1,[4,1,3,2]) AS res; Returns : 14 In this example below, no function to change the array elements is given. SELECT ARRAY_SUM([4,1,3,2]) AS res; Returns : 10 ARRAY_UNIQ If one argument is passed, returns the number of different elements in the array. If multiple arguments are passed, returns the number of different tuples of elements at corresponding positions in multiple arrays. Syntax ARRAY_UNIQ(<arr> [, ...n]) Parameter Description <arr> [, ...n] The array or arrays to be analyzed. Example SELECT ARRAY_UNIQ([1, 2, 4, 5]) AS res; Returns : 4 Example: Using multiple arrays When using multiple arrays, ARRAY_UNIQ evaluates all the elements at a specific index as tuples for counting the unique values. For example, two arrays [1,1,1,1] and [1,1,1,2] would be evaluated as individual tuples (1,1), (1,1), (1,1), and (1,2). There are 2 unique tuples, so ARRAY_UNIQ would return a value of 2. SELECT ARRAY_UNIQ ( [1,1,1,1], [1,1,1,2] ) AS res; Returns: 2 In the example below, there are three different strings across all of the elements of the given arrays. However, there are only two unique tuples, ('apple', 'pie') and ('apple', 'jack'). SELECT ARRAY_UNIQ ( ['apple','apple','apple','apple'], ['pie','pie','jack','jack'] ) AS res; Returns: 2 ARRAY_UNNEST This function \"unfolds\" a given array by creating a column result containing the individual members from the array's values. Syntax ARRAY_UNNEST(<arr>) Parameter Description <arr> The array to be unfolded. Example SELECT ARRAY_UNNEST([1,2,3,4]) AS res; Returns : res 1 2 3 4 CONTAINS Returns 1 if a specified argument is present in the array, or 0 otherwise. Syntax CONTAINS(<arr>, <arg>) Parameter Description <arr> The array to be checked for the given element. <arg> The element to be searched for within the array. Examples SELECT CONTAINS([1, 2, 3], 3) AS res; Returns : 1 CONTAINS returns a 0 result when single character or substring matches only part of a longer string. SELECT CONTAINS(['a', 'b', 'cookie'], 'c') AS res; **Returns: ** 0 ELEMENT_AT Returns the element at a location <index> from the given array. <index> must be any integer type. Indexes in an array begin at position 1 . Syntax ELEMENT_AT(<arr>, <index>) Parameter Description <arr> The array containing the index. <index> The index that is matched by the function. Negative indexes are supported. If used, the function selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. Example SELECT ELEMENT_AT([1,2,3,4],2) AS res; Returns : 2 FILTER Returns an array containing the elements from <arr> for which the given Lambda function <func> returns something other than 0 . The function can receive one or more arrays as its arguments. If more than one array is provided the following conditions should be met: The number of arguments of the Lambda function must be equal to the number of arrays provided. If the condition isn't met - the query will not run and an error will be returned. All the provided arrays should be of the same length. If the condition isn't met a runtime error will occur. When multiple arrays are provided to the function, the function will evaluate the current elements from each array as its parameter. All of the elements at that index position must evaluate to true (or 1) for this index to be included in the results. The elements that are returned are_ taken only from the first array provided._ Syntax FILTER(<func>, <arr> [, ...n] ) Parameter Description <func> A Lambda function used to check elements in the array. <arr> [, ...n] One or more arrays that will be evaluated by the function. Only the first array that is included will be filtered in the results. Examples In the example below, there is only one array and function. Only one element matches the function criteria, and it is returned. SELECT FILTER(x -> x = 'a' ,['a','b','c','d']) Returns : 'a' In this example below, there are two arrays and two separate functions for evaluation. The y function searches the second array for all elements that are greater than 2. The elements in these positions are returned from the first array. SELECT FILTER(x, y -> y > 2,['a','b','c','d'],[1,2,3,9]) AS res; Returns : ['c', 'd'] FLATTEN Converts an array of arrays into a flat array. That is, for every element that is an array, this function extracts its elements into the new array. The resulting flattened array contains all the elements from all source arrays. The function: Applies to any depth of nested arrays. Does not change arrays that are already flat. Syntax FLATTEN(<arr_of_arrs>) Parameter Description <arr_of_arrs> The array of arrays to be flattened. Example SELECT flatten([[[1,2]], [[2,3], [3,4]]]) Returns : (1, 2, 2, 3, 3, 4) INDEX_OF Returns the index position of the first occurrence of the element in the array (or 0 if not found). Syntax INDEX_OF(<arr>, <x>) Parameter Description <arr> The array to be analyzed. <x> The element from the array that is to be matched. Example SELECT INDEX_OF([1, 3, 5, 7], 5) AS res; Returns : 3 LENGTH Returns the length (number of elements) of the given array. Syntax LENGTH(<arr>) Parameter Description <arr> The array to be checked for length. Example SELECT LENGTH([1, 2, 3, 4]) AS res; Returns : 4 NEST Takes a column as an argument, and returns an array of the values. In case the type of the column is nullable, the NULL values will be ignored. Syntax \u200b\u200bNEST(<col>)\u200b\u200b Parameter Description <col> The name of the column to be converted to an array. Example Assume we have the following prices table: item price apple 4 banana NULL orange 11 kiwi 20 Running the following query: SELECT NEST(price) as arr from prices; Returns : [4,11,20] REDUCE Applies an aggregate function on the elements of the array and returns its result. The name of the aggregation function is passed as a string in single quotes - for example: 'max' , 'sum' . When using parametric aggregate functions, the parameter is indicated after the function name in parentheses ' <func_name>(<parameter>) '. Syntax REDUCE(<agg_function>, <arr>) Parameter Description <agg_function> The name of an aggregate function which should be a constant string <arr> Any number of array type columns as the parameters of the aggregation function. Example SELECT REDUCE('max', [1, 2, 3, 6]) AS res; Returns : 6 SLICE Returns a slice of the array based on the indicated offset and length. Syntax SLICE(<arr>, <offset>[, <length>]) Parameter Description <arr> The array of data to be sliced. Array elements set to NULL are handled as normal values. The numbering of the array items begins with 1 . <offset> Indicates starting point of the array slice. A positive value indicates an offset on the left, and a negative value is an indent on the right. <length> The length of the required slice. If you omit this value, the function returns the slice from the <offset> to the end of the array. Example SELECT SLICE([1, 2, NULL, 4, 5], 2, 3) AS res Returns : 2, null, 4 TRANSFORM Returns an array by applying <func> on each element of <arr> . The Lambda function <func> is mandatory. Syntax TRANSFORM(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be transformed by the function. Example SELECT TRANSFORM(x -> x * 2,[1,2,3,9]) AS res; Returns : 2,4,6,18","title":"Array Functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array-functions","text":"This page describes the functions for working with arrays.","title":"Array Functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#all_match","text":"Returns 1 if all elements of an array match the results of the function provided in the <func> parameter, otherwise returns 0 . Syntax ALL_MATCH(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be matched with the function. The array cannot be empty. Examples SELECT ALL_MATCH(x -> x > 0, [1,2,3,9]) AS res; Returns : 1 SELECT ALL_MATCH(x -> x > 10, [1,2,3,9]) AS res; **Returns: ** 0","title":"ALL_MATCH"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#any_match","text":"Returns 1 if at least one of the elements of an array matches the results of the function provided in the <func> parameter. Otherwise returns 0 . Syntax \u200b\u200bANY_MATCH(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be matched with the function. The array cannot be empty Examples SELECT ANY_MATCH(x -> x > 3, [1,2,3,9]) AS res; Returns : 1 SELECT ANY_MATCH(x -> x = 10, [ 1, 2, 3, 9 ]) AS res; Returns : 0","title":"ANY_MATCH"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_concat","text":"Combines one or more arrays that are passed as arguments. Syntax ARRAY_CONCAT(<arr1> [, ...n]) Parameter Description <arr> [, ...n] The arrays to be combined. If only one array is given, an identical array is returned. Example SELECT ARRAY_CONCAT([ 1, 2, 3, 4 ], [ 5, 6, 7, 8 ]) AS res;","title":"ARRAY_CONCAT"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_count","text":"Returns the number of elements in the <arr> array that match a specified function <func> . Syntax ARRAY_COUNT(<func>, <arr>) Parameter Description <func> Optional. A Lambda function used to check elements in the array. If <func> is not included, ARRAY_COUNT will return a count of all elements in the array. <arr> An array of elements Examples The example below searches through the array for any elements that are greater than 3. Only one number that matches this criteria is found, so the function returns 1 SELECT ARRAY_COUNT(x -> x > 3, [1,2,3,9]) AS res; Returns : 1 In this example below, there is no <func> criteria provided in the ARRAY_COUNT function. This means the function will count all of the elements in the given array. SELECT ARRAY_COUNT([ 1, 2, 3, 9 ]) AS res; **Returns: ** 4","title":"ARRAY_COUNT"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_count_global","text":"Returns the number of elements in the array typed column accumulated over all rows. As such it is an aggregation function. Syntax ARRAY_COUNT_GLOBAL(<arr_col>) Parameter Description <arr_col> The array column over which the function will count the elements Example For this example, we will create a table array_test as shown below. CREATE DIMENSION TABLE array_test( array_1 ARRAY(INT) ); INSERT INTO array_test VALUES ([ 1, 2, 3, 4 ]), ([ 5, 0, 20 ]), ([ 6, 2, 6 ]), ([ 9, 10, 13 ]), ([ 20, 13, 40 ]), ([ 1 ]); We can use ARRAY_COUNT_GLOBAL to learn how many total array elements are in all rows. SELECT ARRAY_COUNT_GLOBAL(array_1) FROM array_test **Returns: ** 17 If you want to count elements based on specific criteria, you can use the ARRAY_COUNT function with a SUM aggregation as demonstrated below. SELECT SUM(ARRAY_COUNT(x -> x > 3, array_1)) FROM array_test **Returns: ** 11","title":"ARRAY_COUNT_GLOBAL"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_cumulative_sum","text":"Returns an array of partial sums of elements from the source array (a running sum). If the argument <func> is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_CUMULATIVE_SUM( [<func>,] arr) Parameter Description <func> The function used to convert the array members. <arr> The array used for the sum calculations. Examples SELECT ARRAY_CUMULATIVE_SUM(x -> x + 1, [ 1, 2, 3, 9 ]) AS res; Returns : 2,5,9,19 SELECT ARRAY_CUMULATIVE_SUM([ 1, 2, 3, 9 ]) AS res Returns : 1,3,6,15","title":"ARRAY_CUMULATIVE_SUM"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_distinct","text":"Returns an array containing only the unique elements of the given array. In other words, if the given array contains multiple identical members, the returned array will include only a single member of that value. Syntax ARRAY_DISTINCT(<arr>) Parameter Description <arr> The array to be analyzed for unique members. Example SELECT ARRAY_DISTINCT([ 1, 1, 2, 2, 3, 4 ]) AS res; Returns : 1,2,3,4","title":"ARRAY_DISTINCT"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_fill","text":"This function scans through the given array <arr> from the first to the last element and replaces arr[i] with arr[i - 1] if the <func> returns 0 . The first element of the given array is not replaced. The lambda function <func> is mandatory. Syntax ARRAY_FILL(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be evaluated by the function. Examples SELECT ARRAY_FILL(x -> x < 0,[1,2,3,9]) AS res; Returns : 1,1,1,1 SELECT ARRAY_FILL(x -> x > 0,[1,2,3,9]) AS res; Returns : 1,2,3,9","title":"ARRAY_FILL"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_first","text":"Returns the first element in the given array for which the given <func> function returns something other than 0 . The <func> argument must be included. Syntax ARRAY_FIRST(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array evaluated by the function. Example SELECT ARRAY_FIRST(x -> x > 2,[1,2,3,9]) AS res; Returns : 3","title":"ARRAY_FIRST"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_first_index","text":"Returns the index of the first element in the indicated array for which the given <func> function returns something other than 0 . Index counting starts at 1. The <func> argument must be included. Syntax ARRAY_FIRST_INDEX(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array evaluated by the function. Example SELECT ARRAY_FIRST_INDEX(x -> x > 2,[1,2,3,9]) AS res; Returns : 3","title":"ARRAY_FIRST_INDEX"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_intersect","text":"Evaluates all arrays that are provided as arguments and returns an array of any elements that are present in all the arrays. The order of the resulting array is the same as in the first array. Syntax ARRAY_INTERSECT(<arr>) Parameter Description <arr> A series of arrays to be analyzed for mutual elements. Example In the example below, the only element that is shared between all three arrays is 3. SELECT ARRAY_INTERSECT([1, 2, 3], [1, 3], [2, 3]) Returns : 3","title":"ARRAY_INTERSECT"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_join","text":"Concatenates an array of TEXT elements using an optional delimiter. If no delimiter is provided, an empty string is used instead. Syntax ARRAY_JOIN(<arr>[, <delimiter>]) Parameter Description <arr> An array of TEXT elements. <delimiter> The delimiter used for joining the array elements. If you omit this value, an empty string is being used as a delimiter. Examples In the example below, the three elements are joined with no delimiter. SELECT ARRAY_JOIN(['1', '2', '3']) AS res; Returns : 123 In this example below, we are providing a comma delimiter. SELECT ARRAY_JOIN(['a', 'b', 'c'], ',') AS res; **Returns: ** a,b,c","title":"ARRAY_JOIN"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_max","text":"Returns the maximum element in an array <arr> . Syntax ARRAY_MAX(<arr>) Parameter Description <arr> The array or array-type column to be checked Example SELECT ARRAY_MAX([1,2,3,4]) AS res; Returns : 4","title":"ARRAY_MAX"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_min","text":"Returns the minimum element in <arr> . Syntax ARRAY_MIN(<arr>) Parameter Description <arr> The array or array-type column to be checked Example SELECT ARRAY_MIN([1,2,3,4]) AS res; Returns : 1","title":"ARRAY_MIN"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_replace_backwards","text":"Scans an array <arr> from the last to the first element and replaces each of the elements in that array with arr[i + 1] if the <func> returns 0 . The last element of <arr> is not replaced. The <func> argument must be included. Syntax ARRAY_REPLACE_BACKWARDS(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be evaluated by the function. Example SELECT ARRAY_REPLACE_BACKWARDS(x -> x > 2,[1,2,3,9]) AS res; Returns : 3,3,3,9","title":"ARRAY_REPLACE_BACKWARDS"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_reverse","text":"Returns an array of the same size as the original array, with the elements in reverse order. Syntax ARRAY_REVERSE(<arr>) Parameter Description <arr> The array to be reversed. Example SELECT ARRAY_REVERSE([1, 2, 3, 6]) AS res Returns : 6,3,2,1","title":"ARRAY_REVERSE"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_sort","text":"Returns the elements of <arr> in ascending order. If the argument <func> is provided, the sorting order is determined by the result of applying <func> on each element of <arr> . Syntax ARRAY_SORT([<func>,] <arr>) Parameter Description <func> An optional function to be used to determine the sort order. <arr> The array to be sorted. Examples SELECT ARRAY_SORT([4,1,3,2]) AS res; Returns : 1,2,3,4 In this example below, the modulus operator is used to calculate the remainder on any odd numbers. Therefore ARRAY_ SORT puts the higher (odd) numbers last in the results. SELECT ARRAY_SORT(x -> x % 2, [4,1,3,2]) AS res; Returns : 4,2,1,3","title":"ARRAY_SORT"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_sum","text":"Returns the sum of elements of <arr> . If the argument <func> is provided, the values of the array elements are converted by this function before summing. Syntax ARRAY_SUM([<func>,] <arr>) Parameter Description <func> A Lambda function with an arithmetic function used to modify the array elements. <arr> The array to be used to calculate the function. Examples This example below uses a function to first add 1 to all elements before calculating the sum: SELECT ARRAY_SUM(x -> x + 1,[4,1,3,2]) AS res; Returns : 14 In this example below, no function to change the array elements is given. SELECT ARRAY_SUM([4,1,3,2]) AS res; Returns : 10","title":"ARRAY_SUM"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_uniq","text":"If one argument is passed, returns the number of different elements in the array. If multiple arguments are passed, returns the number of different tuples of elements at corresponding positions in multiple arrays. Syntax ARRAY_UNIQ(<arr> [, ...n]) Parameter Description <arr> [, ...n] The array or arrays to be analyzed. Example SELECT ARRAY_UNIQ([1, 2, 4, 5]) AS res; Returns : 4 Example: Using multiple arrays When using multiple arrays, ARRAY_UNIQ evaluates all the elements at a specific index as tuples for counting the unique values. For example, two arrays [1,1,1,1] and [1,1,1,2] would be evaluated as individual tuples (1,1), (1,1), (1,1), and (1,2). There are 2 unique tuples, so ARRAY_UNIQ would return a value of 2. SELECT ARRAY_UNIQ ( [1,1,1,1], [1,1,1,2] ) AS res; Returns: 2 In the example below, there are three different strings across all of the elements of the given arrays. However, there are only two unique tuples, ('apple', 'pie') and ('apple', 'jack'). SELECT ARRAY_UNIQ ( ['apple','apple','apple','apple'], ['pie','pie','jack','jack'] ) AS res; Returns: 2","title":"ARRAY_UNIQ"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#array_unnest","text":"This function \"unfolds\" a given array by creating a column result containing the individual members from the array's values. Syntax ARRAY_UNNEST(<arr>) Parameter Description <arr> The array to be unfolded. Example SELECT ARRAY_UNNEST([1,2,3,4]) AS res; Returns : res 1 2 3 4","title":"ARRAY_UNNEST"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#contains","text":"Returns 1 if a specified argument is present in the array, or 0 otherwise. Syntax CONTAINS(<arr>, <arg>) Parameter Description <arr> The array to be checked for the given element. <arg> The element to be searched for within the array. Examples SELECT CONTAINS([1, 2, 3], 3) AS res; Returns : 1 CONTAINS returns a 0 result when single character or substring matches only part of a longer string. SELECT CONTAINS(['a', 'b', 'cookie'], 'c') AS res; **Returns: ** 0","title":"CONTAINS"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#element_at","text":"Returns the element at a location <index> from the given array. <index> must be any integer type. Indexes in an array begin at position 1 . Syntax ELEMENT_AT(<arr>, <index>) Parameter Description <arr> The array containing the index. <index> The index that is matched by the function. Negative indexes are supported. If used, the function selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. Example SELECT ELEMENT_AT([1,2,3,4],2) AS res; Returns : 2","title":"ELEMENT_AT"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#filter","text":"Returns an array containing the elements from <arr> for which the given Lambda function <func> returns something other than 0 . The function can receive one or more arrays as its arguments. If more than one array is provided the following conditions should be met: The number of arguments of the Lambda function must be equal to the number of arrays provided. If the condition isn't met - the query will not run and an error will be returned. All the provided arrays should be of the same length. If the condition isn't met a runtime error will occur. When multiple arrays are provided to the function, the function will evaluate the current elements from each array as its parameter. All of the elements at that index position must evaluate to true (or 1) for this index to be included in the results. The elements that are returned are_ taken only from the first array provided._ Syntax FILTER(<func>, <arr> [, ...n] ) Parameter Description <func> A Lambda function used to check elements in the array. <arr> [, ...n] One or more arrays that will be evaluated by the function. Only the first array that is included will be filtered in the results. Examples In the example below, there is only one array and function. Only one element matches the function criteria, and it is returned. SELECT FILTER(x -> x = 'a' ,['a','b','c','d']) Returns : 'a' In this example below, there are two arrays and two separate functions for evaluation. The y function searches the second array for all elements that are greater than 2. The elements in these positions are returned from the first array. SELECT FILTER(x, y -> y > 2,['a','b','c','d'],[1,2,3,9]) AS res; Returns : ['c', 'd']","title":"FILTER"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#flatten","text":"Converts an array of arrays into a flat array. That is, for every element that is an array, this function extracts its elements into the new array. The resulting flattened array contains all the elements from all source arrays. The function: Applies to any depth of nested arrays. Does not change arrays that are already flat. Syntax FLATTEN(<arr_of_arrs>) Parameter Description <arr_of_arrs> The array of arrays to be flattened. Example SELECT flatten([[[1,2]], [[2,3], [3,4]]]) Returns : (1, 2, 2, 3, 3, 4)","title":"FLATTEN"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#index_of","text":"Returns the index position of the first occurrence of the element in the array (or 0 if not found). Syntax INDEX_OF(<arr>, <x>) Parameter Description <arr> The array to be analyzed. <x> The element from the array that is to be matched. Example SELECT INDEX_OF([1, 3, 5, 7], 5) AS res; Returns : 3","title":"INDEX_OF"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#length","text":"Returns the length (number of elements) of the given array. Syntax LENGTH(<arr>) Parameter Description <arr> The array to be checked for length. Example SELECT LENGTH([1, 2, 3, 4]) AS res; Returns : 4","title":"LENGTH"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#nest","text":"Takes a column as an argument, and returns an array of the values. In case the type of the column is nullable, the NULL values will be ignored. Syntax \u200b\u200bNEST(<col>)\u200b\u200b Parameter Description <col> The name of the column to be converted to an array. Example Assume we have the following prices table: item price apple 4 banana NULL orange 11 kiwi 20 Running the following query: SELECT NEST(price) as arr from prices; Returns : [4,11,20]","title":"NEST"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#reduce","text":"Applies an aggregate function on the elements of the array and returns its result. The name of the aggregation function is passed as a string in single quotes - for example: 'max' , 'sum' . When using parametric aggregate functions, the parameter is indicated after the function name in parentheses ' <func_name>(<parameter>) '. Syntax REDUCE(<agg_function>, <arr>) Parameter Description <agg_function> The name of an aggregate function which should be a constant string <arr> Any number of array type columns as the parameters of the aggregation function. Example SELECT REDUCE('max', [1, 2, 3, 6]) AS res; Returns : 6","title":"REDUCE"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#slice","text":"Returns a slice of the array based on the indicated offset and length. Syntax SLICE(<arr>, <offset>[, <length>]) Parameter Description <arr> The array of data to be sliced. Array elements set to NULL are handled as normal values. The numbering of the array items begins with 1 . <offset> Indicates starting point of the array slice. A positive value indicates an offset on the left, and a negative value is an indent on the right. <length> The length of the required slice. If you omit this value, the function returns the slice from the <offset> to the end of the array. Example SELECT SLICE([1, 2, NULL, 4, 5], 2, 3) AS res Returns : 2, null, 4","title":"SLICE"},{"location":"sql-reference/functions-reference/semi-structured-functions/array-functions/#transform","text":"Returns an array by applying <func> on each element of <arr> . The Lambda function <func> is mandatory. Syntax TRANSFORM(<func>, <arr>) Parameter Description <func> A Lambda function used to check elements in the array. <arr> The array to be transformed by the function. Example SELECT TRANSFORM(x -> x * 2,[1,2,3,9]) AS res; Returns : 2,4,6,18","title":"TRANSFORM"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/","text":"JSON Functions This page describes the functions used for JSON manipulation using \u200bFirebolt\u200b. You can use JSON functions to extract values and objects from a JSON document . Function reference conventions This reference uses the following conventions to represent function syntax. JSON pointer parameters This reference uses the placeholder json_pointer_expression to indicate where you should use a JSON pointer. A JSON pointer is a way to access specific elements in a JSON document. For a formal specification, see RFC6901 . A JSON pointer starts with a forward slash ( / ), which denotes the root of the JSON document. This is followed by a sequence of property (key) names or zero-based ordinal numbers separated by slashes. You can specify property names or use ordinal numbers to specify the _n_th property or the _n_th element of an array. The tilde ( ~ ) and forward slash ( / ) characters have special meanings and need to be escaped according to the guidelines below: To specify a literal tilde ( ~ ), use ~0 To specify a literal slash ( / ), use ~1 For example, consider the JSON document below. { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } With this JSON document, the JSON pointer expressions below evaluate to the results shown. Pointer Result Notes / { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } The whole document /key 123 /key~with~tilde 2 Indicates the value associated with the key~with~tilde property name. /key/with/slash 3 Indicates the value associated with the key/with/slash property name. /0 123 Uses an ordinal to indicate the value associated with the key property name. The key property is in the first 0-based position. /value/keywords/2 analytics Indicates the element \"analytics\", which is in the third 0-based position of the array value associated with they keywords property. Supported type parameters Some functions accept a type parameter shown as expected_type . This parameter is given as a literal string corresponding to supported Firebolt SQL data types to specify the expected type indicated by the JSON pointer parameter. The type parameter does not accept all SQL types because the JSON type system has fewer types than SQL. The following values are supported for this argument: INT - used for integers as well as JSON boolean. DOUBLE - used for real numbers. It will also work with integers. For performance reasons, favor using INT when the values in the JSON document are known integers. TEXT - used for strings. ARRAY(<type>) - indicates an array where <type> is one of INT , DOUBLE , or TEXT . The following data types are not supported : DATE , DATETIME , FLOAT (for real numbers, use DOUBLE ). JSON common example Usage examples in this reference are based on the JSON document below, which is referenced using the <json_common_example> placeholder. { \"key\": 123, \"value\": { \"dyid\": 987, \"uid\": \"987654\", \"keywords\" : [\"insanely\",\"fast\",\"analytics\"], \"tagIdToHits\": { \"map\": { \"1737729\": 32, \"1775582\": 35 } }, \"events\":[ { \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }, { \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } } ] } } JSON_EXTRACT Takes an expression containing JSON string, a JSON Pointer, and a type parameter. It returns a typed scalar, or an array pointed by the JSON Pointer. If the key pointed by the JSON pointer is not found, or the type of the value under that key is different from the one specified, the function returns NULL Syntax \u200b\u200bJSON_EXTRACT(<json>, '<json_pointer_expression>', '<expected_type>') Parameter Type Description <json> TEXT The JSON document from which the value is to be extracted. <json_pointer_expression> Literal string A JSON pointer to the location of the value in the JSON document. <expected_type> Literal String A literal string name of the expected return type . Return Value If the key pointed by the JSON path exists and its type conforms with the expected_type parameter, then JSON_EXTRACT returns the value under that key. Otherwise, it returns NULL Examples SELECT JSON_EXTRACT(< json_common_example >, '/value/dyid', 'INT') Returns : 987 SELECT JSON_EXTRACT(<json_common_example>, '/value/no_such_key', 'TEXT') Returns : NULL SELECT JSON_EXTRACT(<json_common_example>, '/value/uid', 'INT') Returns : NULL since the JSON type under that key is a string. SELECT JSON_EXTRACT(<json_common_example>,'/value/keywords', 'ARRAY(TEXT)') Returns : [\"insanely\",\"fast\",\"analytics\"] JSON_EXTRACT_ARRAY_RAW Returns a string representation of a JSON array pointed by the supplied JSON pointer. This function is useful when working with heterogeneously typed arrays and arrays containing JSON objects in which case each object will be further processed by functions such as TRANSFORM . Syntax \u200b\u200bJSON_EXTRACT_ARRAY_RAW(<json>, '<json_pointer_expression>') Parameter Type Description <json> TEXT The JSON document from which the array is to be extracted <json_pointer_expression> Literal string A JSON pointer to the location of the array in the JSON Return value A Firebolt array with elements that are string representations of the scalars or objects contained in the JSON array under the specified key, if such key exists. Otherwise NULL Example SELECT JSON_EXTRACT_ARRAY_RAW(<json_common_example>, '/value/events') Returns : [\"{\\\"EventId\\\":547,\\\"EventProperties\\\":{\\\"UserName\\\":\\\"John Doe\\\",\\\"Successful\\\":true}}\",\"{\\\"EventId\\\":548,\\\"EventProperties\\\":{\\\"ProductID\\\":\\\"xy123\\\",\\\"items\\\":2}}\"] JSON_EXTRACT_KEYS Returns an array of strings containing the keys at the nesting level pointed at by the JSON Pointer. Syntax JSON_EXTRACT_KEYS(<json>, '<json_pointer_expression>') Parameter Type Description <json> TEXT The JSON document used for key extraction <json_pointer_expression> Literal string A JSON pointer to a location where the keys are to be extracted Return value A Firebolt array of strings consisting of the JSON keys, if any such key exists. Otherwise NULL Example SELECT JSON_EXTRACT_KEYS(< json_common_example >, 'value') FROM RAW_JSON Returns : [\"dyid\",\"uid\",\"keywords\",\"tagIdToHits\",\"events\"] JSON_EXTRACT_RAW Returns the scalar or value pointed by the JSON Pointer as a string. Syntax \u200b\u200bJSON_EXTRACT_RAW(<json>, <json_pointer_expression>) Parameter Type Description <json> TEXT The JSON document from which the value is to be extracted <json_pointer_expression> Literal string A JSON pointer to the location of the scalar or value in the JSON Return value A string representation of the scalar or sub-object under the specified key, if such key exists. Otherwise NULL Example SELECT JSON_EXTRACT_RAW(<json_common_example>,'/value/dyid') Returns : 987 JSON_EXTRACT_RAW(<json_common_example>, '/value/tagIdToHits') Returns : {\"map\":{\"1737729\":32,\"1775582\":35}} JSON_EXTRACT_VALUES Returns an array of string representations, each element containing the value pointed by the JSON Pointer. Syntax \u200b\u200bJSON_EXTRACT_VALUES(<json>, '<json_pointer_expression>') Parameter Type Description <json> TEXT The JSON document from which the values are to be extracted <json_pointer_expression> Literal string A JSON pointer to the location where values are to be extracted Return value A Firebolt array of string values based on the location specified by the <json_pointer_expression>. If no such key exists, it returns NULL. Example SELECT JSON_EXTRACT_VALUES(<json_common_example>, 'value') FROM RAW_JSON Returns : [\"987\",\"\\\"987654\\\"\",\"[\\\"insanely\\\",\\\"fast\\\",\\\"analytics\\\"]\",\"{\\\"map\\\":{\\\"1737729\\\":32,\\\"1775582\\\":35}}\",\"[{\\\"EventId\\\":547,\\\"EventProperties\\\":{\\\"UserName\\\":\\\"John Doe\\\",\\\"Successful\\\":true}},{\\\"EventId\\\":548,\\\"EventProperties\\\":{\\\"ProductID\\\":\\\"xy123\\\",\\\"items\\\":2}}]\"]","title":"JSON Functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json-functions","text":"This page describes the functions used for JSON manipulation using \u200bFirebolt\u200b. You can use JSON functions to extract values and objects from a JSON document .","title":"JSON Functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#function-reference-conventions","text":"This reference uses the following conventions to represent function syntax.","title":"Function reference conventions"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json-pointer-parameters","text":"This reference uses the placeholder json_pointer_expression to indicate where you should use a JSON pointer. A JSON pointer is a way to access specific elements in a JSON document. For a formal specification, see RFC6901 . A JSON pointer starts with a forward slash ( / ), which denotes the root of the JSON document. This is followed by a sequence of property (key) names or zero-based ordinal numbers separated by slashes. You can specify property names or use ordinal numbers to specify the _n_th property or the _n_th element of an array. The tilde ( ~ ) and forward slash ( / ) characters have special meanings and need to be escaped according to the guidelines below: To specify a literal tilde ( ~ ), use ~0 To specify a literal slash ( / ), use ~1 For example, consider the JSON document below. { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } With this JSON document, the JSON pointer expressions below evaluate to the results shown. Pointer Result Notes / { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } The whole document /key 123 /key~with~tilde 2 Indicates the value associated with the key~with~tilde property name. /key/with/slash 3 Indicates the value associated with the key/with/slash property name. /0 123 Uses an ordinal to indicate the value associated with the key property name. The key property is in the first 0-based position. /value/keywords/2 analytics Indicates the element \"analytics\", which is in the third 0-based position of the array value associated with they keywords property.","title":"JSON pointer parameters"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#supported-type-parameters","text":"Some functions accept a type parameter shown as expected_type . This parameter is given as a literal string corresponding to supported Firebolt SQL data types to specify the expected type indicated by the JSON pointer parameter. The type parameter does not accept all SQL types because the JSON type system has fewer types than SQL. The following values are supported for this argument: INT - used for integers as well as JSON boolean. DOUBLE - used for real numbers. It will also work with integers. For performance reasons, favor using INT when the values in the JSON document are known integers. TEXT - used for strings. ARRAY(<type>) - indicates an array where <type> is one of INT , DOUBLE , or TEXT . The following data types are not supported : DATE , DATETIME , FLOAT (for real numbers, use DOUBLE ).","title":"Supported type parameters"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json-common-example","text":"Usage examples in this reference are based on the JSON document below, which is referenced using the <json_common_example> placeholder. { \"key\": 123, \"value\": { \"dyid\": 987, \"uid\": \"987654\", \"keywords\" : [\"insanely\",\"fast\",\"analytics\"], \"tagIdToHits\": { \"map\": { \"1737729\": 32, \"1775582\": 35 } }, \"events\":[ { \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }, { \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } } ] } }","title":"JSON common example"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract","text":"Takes an expression containing JSON string, a JSON Pointer, and a type parameter. It returns a typed scalar, or an array pointed by the JSON Pointer. If the key pointed by the JSON pointer is not found, or the type of the value under that key is different from the one specified, the function returns NULL Syntax \u200b\u200bJSON_EXTRACT(<json>, '<json_pointer_expression>', '<expected_type>') Parameter Type Description <json> TEXT The JSON document from which the value is to be extracted. <json_pointer_expression> Literal string A JSON pointer to the location of the value in the JSON document. <expected_type> Literal String A literal string name of the expected return type . Return Value If the key pointed by the JSON path exists and its type conforms with the expected_type parameter, then JSON_EXTRACT returns the value under that key. Otherwise, it returns NULL Examples SELECT JSON_EXTRACT(< json_common_example >, '/value/dyid', 'INT') Returns : 987 SELECT JSON_EXTRACT(<json_common_example>, '/value/no_such_key', 'TEXT') Returns : NULL SELECT JSON_EXTRACT(<json_common_example>, '/value/uid', 'INT') Returns : NULL since the JSON type under that key is a string. SELECT JSON_EXTRACT(<json_common_example>,'/value/keywords', 'ARRAY(TEXT)') Returns : [\"insanely\",\"fast\",\"analytics\"]","title":"JSON_EXTRACT"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_array_raw","text":"Returns a string representation of a JSON array pointed by the supplied JSON pointer. This function is useful when working with heterogeneously typed arrays and arrays containing JSON objects in which case each object will be further processed by functions such as TRANSFORM . Syntax \u200b\u200bJSON_EXTRACT_ARRAY_RAW(<json>, '<json_pointer_expression>') Parameter Type Description <json> TEXT The JSON document from which the array is to be extracted <json_pointer_expression> Literal string A JSON pointer to the location of the array in the JSON Return value A Firebolt array with elements that are string representations of the scalars or objects contained in the JSON array under the specified key, if such key exists. Otherwise NULL Example SELECT JSON_EXTRACT_ARRAY_RAW(<json_common_example>, '/value/events') Returns : [\"{\\\"EventId\\\":547,\\\"EventProperties\\\":{\\\"UserName\\\":\\\"John Doe\\\",\\\"Successful\\\":true}}\",\"{\\\"EventId\\\":548,\\\"EventProperties\\\":{\\\"ProductID\\\":\\\"xy123\\\",\\\"items\\\":2}}\"]","title":"JSON_EXTRACT_ARRAY_RAW"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_keys","text":"Returns an array of strings containing the keys at the nesting level pointed at by the JSON Pointer. Syntax JSON_EXTRACT_KEYS(<json>, '<json_pointer_expression>') Parameter Type Description <json> TEXT The JSON document used for key extraction <json_pointer_expression> Literal string A JSON pointer to a location where the keys are to be extracted Return value A Firebolt array of strings consisting of the JSON keys, if any such key exists. Otherwise NULL Example SELECT JSON_EXTRACT_KEYS(< json_common_example >, 'value') FROM RAW_JSON Returns : [\"dyid\",\"uid\",\"keywords\",\"tagIdToHits\",\"events\"]","title":"JSON_EXTRACT_KEYS"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_raw","text":"Returns the scalar or value pointed by the JSON Pointer as a string. Syntax \u200b\u200bJSON_EXTRACT_RAW(<json>, <json_pointer_expression>) Parameter Type Description <json> TEXT The JSON document from which the value is to be extracted <json_pointer_expression> Literal string A JSON pointer to the location of the scalar or value in the JSON Return value A string representation of the scalar or sub-object under the specified key, if such key exists. Otherwise NULL Example SELECT JSON_EXTRACT_RAW(<json_common_example>,'/value/dyid') Returns : 987 JSON_EXTRACT_RAW(<json_common_example>, '/value/tagIdToHits') Returns : {\"map\":{\"1737729\":32,\"1775582\":35}}","title":"JSON_EXTRACT_RAW"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_values","text":"Returns an array of string representations, each element containing the value pointed by the JSON Pointer. Syntax \u200b\u200bJSON_EXTRACT_VALUES(<json>, '<json_pointer_expression>') Parameter Type Description <json> TEXT The JSON document from which the values are to be extracted <json_pointer_expression> Literal string A JSON pointer to the location where values are to be extracted Return value A Firebolt array of string values based on the location specified by the <json_pointer_expression>. If no such key exists, it returns NULL. Example SELECT JSON_EXTRACT_VALUES(<json_common_example>, 'value') FROM RAW_JSON Returns : [\"987\",\"\\\"987654\\\"\",\"[\\\"insanely\\\",\\\"fast\\\",\\\"analytics\\\"]\",\"{\\\"map\\\":{\\\"1737729\\\":32,\\\"1775582\\\":35}}\",\"[{\\\"EventId\\\":547,\\\"EventProperties\\\":{\\\"UserName\\\":\\\"John Doe\\\",\\\"Successful\\\":true}},{\\\"EventId\\\":548,\\\"EventProperties\\\":{\\\"ProductID\\\":\\\"xy123\\\",\\\"items\\\":2}}]\"]","title":"JSON_EXTRACT_VALUES"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/","text":"JSON Functions This page describes the functions used for JSON manipulation using \u200bFirebolt\u200b. You can use JSON functions to extract values and objects from a JSON document . Function reference conventions This reference uses the following conventions in representing function syntax. JSON pointer syntax A JSON pointer is a way to access specific elements in a JSON document. For a formal specification see RFC6901 . This reference uses the placeholder json_pointer_expression to indicate where a JSON pointer expression is used. A JSON pointer is a string starting with a forward slash ( / ), which denotes the root of the JSON document. This is followed by a sequence of property names (keys) or zero-based ordinal numbers, separated by slashes. You can specify key names or use ordinal numbers to specify the nth property or the nth element of an array. The tilde ( ~ ) and forward slash ( / ) characters have special meanings and need to be escaped according to the guidelines below: To specify a literal tilde ( ~ ), use ~0 To specify a literal slash ( / ), use ~1 For example, consider the JSON document below. { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } With this JSON document, the JSON pointer expressions below evaluate to the results shown. Pointer Result Notes / { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } The whole document /key 123 /key~0with~0tilde 2 Note the use of escaping /key~1with~1slash 3 Note the use of escaping /2 3 Access by position /value/keywords/2 \"fast\" Accessing an array element Supported data types Some functions accept a type parameter shown as expected_type . This parameter is given as a literal string corresponding to supported Firebolt SQL data types to specify the expected type indicated by the JSON pointer parameter. The type parameter does not accept all SQL types because the JSON type system has fewer types than SQL. The following values are supported for this argument: INT - used for integers as well as JSON boolean. DOUBLE - used for real numbers. It will also work with integers. For performance reasons, favor using INT when the values in the JSON document are known integers. TEXT - used for strings. ARRAY of one of the above types The following data types are not supported : DATE , DATETIME , FLOAT (for real numbers, use DOUBLE ). JSON document for examples Usage examples in this reference are based on the JSON document below. { \"key\": 123, \"value\": { \"dyid\": 987, \"uid\": \"987654\", \"keywords\" : [\"insanely\",\"fast\",\"analytics\"], \"tagIdToHits\": { \"map\": { \"1737729\": 32, \"1775582\": 35 } }, \"events\":[ { \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }, { \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } } ] } } JSON_EXTRACT Takes an expression containing JSON string, a JSON Pointer, and a type parameter, and returns a typed scalar, or an array pointed by the JSON Pointer. If the key pointed by the JSON pointer is not found, or the type of the value under that key is different from the one specified, the function returns NULL Syntax \u200b\u200bJSON_EXTRACT(json, json_pointer_expression, expected_type) Parameter Type Description json TEXT The JSON document from which the value is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the value in the JSON document expected_type Literal String A literal string name of the expected return type. See supported types bellow Return Value If the key pointed by the JSON path exists and its type conforms with the expected_type parameter - the value under that key. Otherwise, return NULL Usage example SELECT JSON_EXTRACT(json,'/value/dyid', 'INT') Returns: 987 JSON_EXTRACT(json, '/value/no_such_key', 'TEXT') Returns: NULL JSON_EXTRACT(json, '/value/data/uid', 'INT') Returns: NULL since the JSON type under that key is a string. JSON_EXTRACT(json,'/value/keywords', 'ARRAY(TEXT)') Returns: [\"insanely\",\"fast\",\"analytics\"] JSON_EXTRACT_ARRAY_RAW Returns a string representation of a JSON array pointed by the supplied JSON pointer. This function is useful when working with heterogeneously typed arrays and arrays containing JSON objects in which case each object will be further processed by functions such as TRANSFORM . Syntax \u200b\u200bJSON_EXTRACT_ARRAY_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the array is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the array in the JSON Return value A Firebolt array whose elements are string representations of the scalars or objects contained in the JSON array under the specified key, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_ARRAY_RAW(json, '/value/events') Returns (as an array of TEXT): [ '{ \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }', '{ \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } }' ] Note that the single quotes denote that these are SQL strings, within a Firebolt array. The exact output may vary depending on the selected output format. JSON_EXTRACT_KEYS Returns an array of strings containing the keys under the (sub)-object pointed by the JSON Pointer. Syntax JSON_EXTRACT_KEYS(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from the keys are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose keys are to be extracted Return value A Firebolt array of strings consisting the keys of JSON (sub)-object, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_KEYS(json, 'value') FROM RAW_JSON Returns: [\"dyid\",\"uid\",\"keywords\",\"tagIdToHits\",\"events\"] JSON_EXTRACT_KEYS_AND_VALUES_RAW TO DO: ADD CONTENT JSON_EXTRACT_VALUES Returns an array of string representations, each element containing the value (scalar or sub-object) pointed by the JSON Pointer. Syntax \u200b\u200bJSON_EXTRACT_VALUES(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the values are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose values are to be extracted Return value A Firebolt array of strings consisting the values of the JSON (sub)-object, if such key exists. Otherwise NULL Usage example select JSON_EXTRACT_VALUES(j, 'value') FROM RAW_JSON Returns: [ '\"987\"', '\"987654\"', '[\"insanely\",\"fast\",\"analytics\"]', '{\"map\":{\"1737729\":32,\"1775582\":35}}', '[{\"EventId\":547,\"EventProperties\":{\"UserName\":\"John Doe\",\"Successful\":true}},{\"EventId\":548,\"EventProperties\":{\"ProductID\":\"xy123\",\"items\":2}}]' ] Here, as with JSON_EXTRACT_KEYS , the single quotes are to illustrate that it is a Firebolt array whose elements are SQL strings. The results as a whole is not a JSON string. Each element of the result is. JSON_EXTRACT_RAW Returns the scalar or sub-object pointed by the JSON Pointer as a string. Syntax \u200b\u200bJSON_EXTRACT_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the sub-object is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the sub-object in the JSON Return value A string representation of the scalar or sub-object under the specified key, if such key exists. Otherwise NULL Usage Example SELECT JSON_EXTRACT_RAW(json,'/value/dyid') Returns: \"987\" JSON_EXTRACT_RAW(json, '/value/data/tagIdToHits') Returns (as a TEXT): \"map\": { \"1737729\": 32, \"1775582\": 35 }","title":"JSON Functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json-functions","text":"This page describes the functions used for JSON manipulation using \u200bFirebolt\u200b. You can use JSON functions to extract values and objects from a JSON document .","title":"JSON Functions"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#function-reference-conventions","text":"This reference uses the following conventions in representing function syntax.","title":"Function reference conventions"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json-pointer-syntax","text":"A JSON pointer is a way to access specific elements in a JSON document. For a formal specification see RFC6901 . This reference uses the placeholder json_pointer_expression to indicate where a JSON pointer expression is used. A JSON pointer is a string starting with a forward slash ( / ), which denotes the root of the JSON document. This is followed by a sequence of property names (keys) or zero-based ordinal numbers, separated by slashes. You can specify key names or use ordinal numbers to specify the nth property or the nth element of an array. The tilde ( ~ ) and forward slash ( / ) characters have special meanings and need to be escaped according to the guidelines below: To specify a literal tilde ( ~ ), use ~0 To specify a literal slash ( / ), use ~1 For example, consider the JSON document below. { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } With this JSON document, the JSON pointer expressions below evaluate to the results shown. Pointer Result Notes / { \"key\": 123, \"key~with~tilde\": 2, \"key/with/slash\": 3, \"value\": { \"dyid\": 987, \"keywords\" : [\"insanely\",\"fast\",\"analytics\"] } } The whole document /key 123 /key~0with~0tilde 2 Note the use of escaping /key~1with~1slash 3 Note the use of escaping /2 3 Access by position /value/keywords/2 \"fast\" Accessing an array element","title":"JSON pointer syntax"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#supported-data-types","text":"Some functions accept a type parameter shown as expected_type . This parameter is given as a literal string corresponding to supported Firebolt SQL data types to specify the expected type indicated by the JSON pointer parameter. The type parameter does not accept all SQL types because the JSON type system has fewer types than SQL. The following values are supported for this argument: INT - used for integers as well as JSON boolean. DOUBLE - used for real numbers. It will also work with integers. For performance reasons, favor using INT when the values in the JSON document are known integers. TEXT - used for strings. ARRAY of one of the above types The following data types are not supported : DATE , DATETIME , FLOAT (for real numbers, use DOUBLE ).","title":"Supported data types"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json-document-for-examples","text":"Usage examples in this reference are based on the JSON document below. { \"key\": 123, \"value\": { \"dyid\": 987, \"uid\": \"987654\", \"keywords\" : [\"insanely\",\"fast\",\"analytics\"], \"tagIdToHits\": { \"map\": { \"1737729\": 32, \"1775582\": 35 } }, \"events\":[ { \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }, { \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } } ] } }","title":"JSON document for examples"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract","text":"Takes an expression containing JSON string, a JSON Pointer, and a type parameter, and returns a typed scalar, or an array pointed by the JSON Pointer. If the key pointed by the JSON pointer is not found, or the type of the value under that key is different from the one specified, the function returns NULL Syntax \u200b\u200bJSON_EXTRACT(json, json_pointer_expression, expected_type) Parameter Type Description json TEXT The JSON document from which the value is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the value in the JSON document expected_type Literal String A literal string name of the expected return type. See supported types bellow Return Value If the key pointed by the JSON path exists and its type conforms with the expected_type parameter - the value under that key. Otherwise, return NULL Usage example SELECT JSON_EXTRACT(json,'/value/dyid', 'INT') Returns: 987 JSON_EXTRACT(json, '/value/no_such_key', 'TEXT') Returns: NULL JSON_EXTRACT(json, '/value/data/uid', 'INT') Returns: NULL since the JSON type under that key is a string. JSON_EXTRACT(json,'/value/keywords', 'ARRAY(TEXT)') Returns: [\"insanely\",\"fast\",\"analytics\"]","title":"JSON_EXTRACT"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_array_raw","text":"Returns a string representation of a JSON array pointed by the supplied JSON pointer. This function is useful when working with heterogeneously typed arrays and arrays containing JSON objects in which case each object will be further processed by functions such as TRANSFORM . Syntax \u200b\u200bJSON_EXTRACT_ARRAY_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the array is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the array in the JSON Return value A Firebolt array whose elements are string representations of the scalars or objects contained in the JSON array under the specified key, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_ARRAY_RAW(json, '/value/events') Returns (as an array of TEXT): [ '{ \"EventId\": 547, \"EventProperties\" : { \"UserName\":\"John Doe\", \"Successful\": true } }', '{ \"EventId\": 548, \"EventProperties\" : { \"ProductID\":\"xy123\", \"items\": 2 } }' ] Note that the single quotes denote that these are SQL strings, within a Firebolt array. The exact output may vary depending on the selected output format.","title":"JSON_EXTRACT_ARRAY_RAW"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_keys","text":"Returns an array of strings containing the keys under the (sub)-object pointed by the JSON Pointer. Syntax JSON_EXTRACT_KEYS(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from the keys are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose keys are to be extracted Return value A Firebolt array of strings consisting the keys of JSON (sub)-object, if such key exists. Otherwise NULL Usage example SELECT JSON_EXTRACT_KEYS(json, 'value') FROM RAW_JSON Returns: [\"dyid\",\"uid\",\"keywords\",\"tagIdToHits\",\"events\"]","title":"JSON_EXTRACT_KEYS"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_keys_and_values_raw","text":"","title":"JSON_EXTRACT_KEYS_AND_VALUES_RAW"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#to-do-add-content","text":"","title":"TO DO: ADD CONTENT"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_values","text":"Returns an array of string representations, each element containing the value (scalar or sub-object) pointed by the JSON Pointer. Syntax \u200b\u200bJSON_EXTRACT_VALUES(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the values are to be extracted json_pointer_expression Literal string A JSON pointer to the location of (sub)-object whose values are to be extracted Return value A Firebolt array of strings consisting the values of the JSON (sub)-object, if such key exists. Otherwise NULL Usage example select JSON_EXTRACT_VALUES(j, 'value') FROM RAW_JSON Returns: [ '\"987\"', '\"987654\"', '[\"insanely\",\"fast\",\"analytics\"]', '{\"map\":{\"1737729\":32,\"1775582\":35}}', '[{\"EventId\":547,\"EventProperties\":{\"UserName\":\"John Doe\",\"Successful\":true}},{\"EventId\":548,\"EventProperties\":{\"ProductID\":\"xy123\",\"items\":2}}]' ] Here, as with JSON_EXTRACT_KEYS , the single quotes are to illustrate that it is a Firebolt array whose elements are SQL strings. The results as a whole is not a JSON string. Each element of the result is.","title":"JSON_EXTRACT_VALUES"},{"location":"sql-reference/functions-reference/semi-structured-functions/json-functions/#json_extract_raw","text":"Returns the scalar or sub-object pointed by the JSON Pointer as a string. Syntax \u200b\u200bJSON_EXTRACT_RAW(json, json_pointer_expression) Parameter Type Description json TEXT The JSON document from which the sub-object is to be extracted json_pointer_expression Literal string A JSON pointer to the location of the sub-object in the JSON Return value A string representation of the scalar or sub-object under the specified key, if such key exists. Otherwise NULL Usage Example SELECT JSON_EXTRACT_RAW(json,'/value/dyid') Returns: \"987\" JSON_EXTRACT_RAW(json, '/value/data/tagIdToHits') Returns (as a TEXT): \"map\": { \"1737729\": 32, \"1775582\": 35 }","title":"JSON_EXTRACT_RAW"},{"location":"work-with-our-sql-editor/","text":"Working with the SQL workspace The Firebolt Manager has a SQL workspace that you use to edit, run, and save SQL scripts and view query results. Each database in Firebolt has a SQL workspace. When you open the SQL workspace for a database, you see the saved scripts and state for that particular database. Opening a SQL workspace You can launch the SQL workspace for a database either from the left navigation pane or from the databases list. To start the SQL workspace for the last database you worked with Choose the >_ icon from the left navigation pane. The workspace for the database that you last worked with opens, and the database is selected from the list. 2. To switch to the workspace for a different database, choose it from the Select a database list. To start the SQL workspace from the Databases list From the Databases list, hover over the Database name and then choose the >_ icon. Quick tour The SQL workspace is organized into three panes. The left pane is the workspace navigation pane. Use it to navigate to different databases and to open different scripts in your database. The center pane is the SQL editor. Use it to edit scripts, save them, and run scripts. When you run a script, results are shown in the bottom part of the pane. The right pane is an object viewer that shows you the tables, views, and indexes in your database. You must start an engine before objects are visible. You can select an object to view its attributes. For example, you can select a table from the list of tables to view columns and data types. Editing SQL scripts The SQL editor uses tabs to help you organize your SQL scripts. You can switch tabs to work with different scripts and run them. You can have multiple query statements on the same tab. Each statement must be terminated by a semi-colon ( ; ). Using auto-complete As you enter your code in a script tab, Firebolt suggests keywords and object names. Press the tab key to quickly add the first suggestion in the list to your script, or use arrow keys to select a different item from the list and then press the tab key. Using keyboard shortcuts You can click the keyboard icon to view available keyboard shortcuts, or you can view a full list on our keyboard shortcuts page . You can also select an item from the list to perform that action. Using query templates Query templates are available for common tasks, such as creating fact or dimension tables. Place the cursor in the editor where you want to insert code, choose the * * icon, and then select a query template from the list. Using the CREATE EXTERNAL TABLE template to import data To create an external table, which is the first step for ingesting data into Firebolt, choose the Import Data button from the object pane or choose the download icon and then choose Import data as shown in the example below. Firebolt creates a new tab with a CREATE EXTERNAL TABLE statement. Saving and managing scripts Choose the vertical ellipses on the script tab to select options for saving a script, duplicating a script to a new tab, exporting a script to a file, and closing the script tab. Although scripts are saved automatically in this way, we recommend that you save your script from time to time and before closing your editor. Firebolt uses cookies in your client to save the state of unsaved scripts automatically. A script is lost and can't be recovered if you close a script tab or clear your browser cache before saving. We recommend that you save scripts immediately when you start working. Saved scripts are saved to Firebolt automatically as you work. To save a script Choose the vertical ellipses next to the script name and then select Save script . 2. Type a new name for the script and then press ENTER. The script appears in the Database scripts list. To rename a script Double-click the script name, type a new name, and then press ENTER. When you rename an unsaved script, Firebolt saves it. To export a script and download it as a .sql file Choose the vertical ellipses next to the script name and then choose Export script . Firebolt downloads the file to your browser's default download directory using the file pattern Yourscriptname .sql . To import a script from a file Choose the arrow icon and then choose Import script . 2. Browse your computer for a file and select it, or drag and drop a file to upload it. Firebolt creates a new tab with the contents of the script you imported. Running scripts and working with results At the bottom of each script tab, you can choose Run script to execute SQL statements. SQL statements can only run on running engines. If an engine isn't running, you can select it from the list and then choose the Start button for that engine. For more information about engines, see Working with engines . You can run all statements in a script or select snippets of SQL to run. To run all SQL statements in a script Position the cursor anywhere in the script editor and then choose Run script . All SQL statements must be terminated by a semi-colon ( ; ) or an error occurs. To run a snippet of SQL as a statement Select the SQL code you want to run as a statement and then choose Run script . Behind the scenes, Firebolt automatically appends a semi-colon to the selected SQL code so it can run as a statement. Viewing results After you run a script or query statement, results appear below the script editor, along with statistics about query execution. When running a query, Firebolt provides several statistics to help you understand and monitor the performance. These statistics can be viewed in the SQL workspace, just above the query results table. From left to right, these icons represent the following: Icon Name Description Row count The total number of rows returned by a query. Elapsed Time The total time needed to complete a query. Scanned Bytes The total amount of bytes scanned to complete a query. This includes scanned memory in the engine cache as well as the F3 storage. Rows / Second The number of rows scanned per second. Warm data A percentage showing how much of the total data used to run a query was loaded from the engine\u2019s cache. Warm data can be scanned very quickly. In contrast, \u201ccold data\u201d is slower to access because it\u2019s being loaded from the F3 remote storage. A high percentage indicates data is read from the engine\u2019s cache, which accelerates query response times. A low percentage indicates that query performance could be improved by configuring indexes or by changing the spec of your engine to have a bigger cache. Viewing query statistics with results While viewing query results, you can choose to view a selection of prepared statistics to help you investigate data characteristics and identify anomalies more quickly. To view query statistics, choose the expansion arrow to the left of a result table header row. Each column expands to show statistics on a new row located just below the header. The statistics shown in each cell depend on the column\u2019s data type . Data type Statistics displayed Numeric The statistics cell displays MIN, MAX, and MEDIAN values for that column. Each data field displays a horizontal data bar to indicate where that value lies in the range of values for that column. For a column with all positive numbers, the bar starts on the left at 0 and extends to the right toward 100% of maximum. If numbers are all negative, the data bar starts on the right at 0 and extends left to 100% of minimum. For mixed positive and negative numbers, the bar starts in the center and extends left and right toward minimum and maximum respectively. String The statistics cell displays the most frequent values by percentage. If no single value is more frequent, the cell displays the number of unique values. Date The statistics cell displays the most frequent values by percentage. If no single value is more frequent, the cell displays the number of unique values. Array The statistics cell displays the percentage of empty, not empty, and NULL values. Viewing more result statistics You can view additional statistics and visualizations by selecting the expand icon that appears when you mouse over the bottom right of the statistics row cell. This opens up a side panel with a dashboard of more statistics detailing the selected column. Viewing multi-statement script results When you run a script that has multiple SQL statements with result sets ( SELECT statements), each result is shown on a separate line with statistics about statement execution. The first statement that ran is numbered 1 and at the bottom of the list. To view the results table for a result set, choose the table icon corresponding to the numbered result set you want to view, or choose the expansion arrow next to the Statement and then choose Open table as shown in the example below.","title":"Working with the SQL workspace"},{"location":"work-with-our-sql-editor/#working-with-the-sql-workspace","text":"The Firebolt Manager has a SQL workspace that you use to edit, run, and save SQL scripts and view query results. Each database in Firebolt has a SQL workspace. When you open the SQL workspace for a database, you see the saved scripts and state for that particular database.","title":"Working with the SQL workspace"},{"location":"work-with-our-sql-editor/#opening-a-sql-workspace","text":"You can launch the SQL workspace for a database either from the left navigation pane or from the databases list. To start the SQL workspace for the last database you worked with Choose the >_ icon from the left navigation pane. The workspace for the database that you last worked with opens, and the database is selected from the list. 2. To switch to the workspace for a different database, choose it from the Select a database list. To start the SQL workspace from the Databases list From the Databases list, hover over the Database name and then choose the >_ icon.","title":"Opening a SQL workspace"},{"location":"work-with-our-sql-editor/#quick-tour","text":"The SQL workspace is organized into three panes. The left pane is the workspace navigation pane. Use it to navigate to different databases and to open different scripts in your database. The center pane is the SQL editor. Use it to edit scripts, save them, and run scripts. When you run a script, results are shown in the bottom part of the pane. The right pane is an object viewer that shows you the tables, views, and indexes in your database. You must start an engine before objects are visible. You can select an object to view its attributes. For example, you can select a table from the list of tables to view columns and data types.","title":"Quick tour"},{"location":"work-with-our-sql-editor/#editing-sql-scripts","text":"The SQL editor uses tabs to help you organize your SQL scripts. You can switch tabs to work with different scripts and run them. You can have multiple query statements on the same tab. Each statement must be terminated by a semi-colon ( ; ).","title":"Editing SQL scripts"},{"location":"work-with-our-sql-editor/#using-auto-complete","text":"As you enter your code in a script tab, Firebolt suggests keywords and object names. Press the tab key to quickly add the first suggestion in the list to your script, or use arrow keys to select a different item from the list and then press the tab key.","title":"Using auto-complete"},{"location":"work-with-our-sql-editor/#using-keyboard-shortcuts","text":"You can click the keyboard icon to view available keyboard shortcuts, or you can view a full list on our keyboard shortcuts page . You can also select an item from the list to perform that action.","title":"Using keyboard shortcuts"},{"location":"work-with-our-sql-editor/#using-query-templates","text":"Query templates are available for common tasks, such as creating fact or dimension tables. Place the cursor in the editor where you want to insert code, choose the * * icon, and then select a query template from the list.","title":"Using query templates"},{"location":"work-with-our-sql-editor/#using-the-create-external-table-template-to-import-data","text":"To create an external table, which is the first step for ingesting data into Firebolt, choose the Import Data button from the object pane or choose the download icon and then choose Import data as shown in the example below. Firebolt creates a new tab with a CREATE EXTERNAL TABLE statement.","title":"Using the CREATE EXTERNAL TABLE template to import data"},{"location":"work-with-our-sql-editor/#saving-and-managing-scripts","text":"Choose the vertical ellipses on the script tab to select options for saving a script, duplicating a script to a new tab, exporting a script to a file, and closing the script tab. Although scripts are saved automatically in this way, we recommend that you save your script from time to time and before closing your editor. Firebolt uses cookies in your client to save the state of unsaved scripts automatically. A script is lost and can't be recovered if you close a script tab or clear your browser cache before saving. We recommend that you save scripts immediately when you start working. Saved scripts are saved to Firebolt automatically as you work. To save a script Choose the vertical ellipses next to the script name and then select Save script . 2. Type a new name for the script and then press ENTER. The script appears in the Database scripts list. To rename a script Double-click the script name, type a new name, and then press ENTER. When you rename an unsaved script, Firebolt saves it. To export a script and download it as a .sql file Choose the vertical ellipses next to the script name and then choose Export script . Firebolt downloads the file to your browser's default download directory using the file pattern Yourscriptname .sql . To import a script from a file Choose the arrow icon and then choose Import script . 2. Browse your computer for a file and select it, or drag and drop a file to upload it. Firebolt creates a new tab with the contents of the script you imported.","title":"Saving and managing scripts"},{"location":"work-with-our-sql-editor/#running-scripts-and-working-with-results","text":"At the bottom of each script tab, you can choose Run script to execute SQL statements. SQL statements can only run on running engines. If an engine isn't running, you can select it from the list and then choose the Start button for that engine. For more information about engines, see Working with engines . You can run all statements in a script or select snippets of SQL to run. To run all SQL statements in a script Position the cursor anywhere in the script editor and then choose Run script . All SQL statements must be terminated by a semi-colon ( ; ) or an error occurs. To run a snippet of SQL as a statement Select the SQL code you want to run as a statement and then choose Run script . Behind the scenes, Firebolt automatically appends a semi-colon to the selected SQL code so it can run as a statement.","title":"Running scripts and working with results"},{"location":"work-with-our-sql-editor/#viewing-results","text":"After you run a script or query statement, results appear below the script editor, along with statistics about query execution. When running a query, Firebolt provides several statistics to help you understand and monitor the performance. These statistics can be viewed in the SQL workspace, just above the query results table. From left to right, these icons represent the following: Icon Name Description Row count The total number of rows returned by a query. Elapsed Time The total time needed to complete a query. Scanned Bytes The total amount of bytes scanned to complete a query. This includes scanned memory in the engine cache as well as the F3 storage. Rows / Second The number of rows scanned per second. Warm data A percentage showing how much of the total data used to run a query was loaded from the engine\u2019s cache. Warm data can be scanned very quickly. In contrast, \u201ccold data\u201d is slower to access because it\u2019s being loaded from the F3 remote storage. A high percentage indicates data is read from the engine\u2019s cache, which accelerates query response times. A low percentage indicates that query performance could be improved by configuring indexes or by changing the spec of your engine to have a bigger cache.","title":"Viewing results"},{"location":"work-with-our-sql-editor/#viewing-query-statistics-with-results","text":"While viewing query results, you can choose to view a selection of prepared statistics to help you investigate data characteristics and identify anomalies more quickly. To view query statistics, choose the expansion arrow to the left of a result table header row. Each column expands to show statistics on a new row located just below the header. The statistics shown in each cell depend on the column\u2019s data type . Data type Statistics displayed Numeric The statistics cell displays MIN, MAX, and MEDIAN values for that column. Each data field displays a horizontal data bar to indicate where that value lies in the range of values for that column. For a column with all positive numbers, the bar starts on the left at 0 and extends to the right toward 100% of maximum. If numbers are all negative, the data bar starts on the right at 0 and extends left to 100% of minimum. For mixed positive and negative numbers, the bar starts in the center and extends left and right toward minimum and maximum respectively. String The statistics cell displays the most frequent values by percentage. If no single value is more frequent, the cell displays the number of unique values. Date The statistics cell displays the most frequent values by percentage. If no single value is more frequent, the cell displays the number of unique values. Array The statistics cell displays the percentage of empty, not empty, and NULL values.","title":"Viewing query statistics with results"},{"location":"work-with-our-sql-editor/#viewing-more-result-statistics","text":"You can view additional statistics and visualizations by selecting the expand icon that appears when you mouse over the bottom right of the statistics row cell. This opens up a side panel with a dashboard of more statistics detailing the selected column.","title":"Viewing more result statistics"},{"location":"work-with-our-sql-editor/#viewing-multi-statement-script-results","text":"When you run a script that has multiple SQL statements with result sets ( SELECT statements), each result is shown on a separate line with statistics about statement execution. The first statement that ran is numbered 1 and at the bottom of the list. To view the results table for a result set, choose the table icon corresponding to the numbered result set you want to view, or choose the expansion arrow next to the Statement and then choose Open table as shown in the example below.","title":"Viewing multi-statement script results"},{"location":"work-with-our-sql-editor/keyboard-shortcuts-for-sql-workspace/","text":"Keyboard shortcuts for SQL workspace Firebolt supports the following keyboard shortcuts in the SQL workspace. Function Keyboard shortcut selectall Ctrl-A gotoline Ctrl-L findnext Ctrl-K findprevious Ctrl-Shift-K selectOrFindNext Alt-K selectOrFindPrevious Alt-Shift-K find Ctrl-F selecttostart Ctrl-Shift-Home gotostart Ctrl-Home selectup Shift-Up golineup Up selecttoend Ctrl-Shift-End gotoend Ctrl-End selectdown Shift-Down golinedown Down selectwordleft Ctrl-Shift-Left gotowordleft Ctrl-Left selecttolinestart Alt-Shift-Left gotolinestart Alt-Left | Home selectleft Shift-Left gotoleft Left selectwordright Ctrl-Shift-Right gotowordright Ctrl-Right selecttolineend Alt-Shift-Right gotolineend Alt-Right | End selectright Shift-Right gotoright Right selectpagedown Shift-Pagedown gotopagedown Pagedown selectpageup Shift-Pageup gotopageup Pageup scrollup Ctrl-Up scrolldown Ctrl-Down selectlinestart Shift-Home selectlineend Shift-End removeline Ctrl-D duplicateSelection Ctrl-Shift-D togglecomment Ctrl-/ modifyNumberUp Ctrl-Shift-Up modifyNumberDown Ctrl-Shift-Down replace Ctrl-H undo Ctrl-Z copylinesup Alt-Shift-Up movelinesup Alt-Up copylinesdown Alt-Shift-Down movelinesdown Alt-Down del Delete backspace Shift-Backspace | Backspace cut_or_delete Shift-Delete removetolinestart Alt-Backspace removetolineend Alt-Delete removetolinestarthard Ctrl-Shift-Backspace removetolineendhard Ctrl-Shift-Delete removewordleft Ctrl-Backspace removewordright Ctrl-Delete indent Tab blockindent Ctrl-] touppercase Ctrl-U tolowercase Ctrl-Shift-U expandtoline Ctrl-Shift-L addCursorAbove Ctrl-Alt-Up addCursorBelow Ctrl-Alt-Down addCursorAboveSkipCurrent Ctrl-Alt-Shift-Up addCursorBelowSkipCurrent Ctrl-Alt-Shift-Down selectMoreBefore Ctrl-Alt-Left selectMoreAfter Ctrl-Alt-Right selectNextBefore Ctrl-Alt-Shift-Left selectNextAfter Ctrl-Alt-Shift-Right findAll Ctrl-Alt-K","title":"Keyboard shortcuts for SQL workspace"},{"location":"work-with-our-sql-editor/keyboard-shortcuts-for-sql-workspace/#keyboard-shortcuts-for-sql-workspace","text":"Firebolt supports the following keyboard shortcuts in the SQL workspace. Function Keyboard shortcut selectall Ctrl-A gotoline Ctrl-L findnext Ctrl-K findprevious Ctrl-Shift-K selectOrFindNext Alt-K selectOrFindPrevious Alt-Shift-K find Ctrl-F selecttostart Ctrl-Shift-Home gotostart Ctrl-Home selectup Shift-Up golineup Up selecttoend Ctrl-Shift-End gotoend Ctrl-End selectdown Shift-Down golinedown Down selectwordleft Ctrl-Shift-Left gotowordleft Ctrl-Left selecttolinestart Alt-Shift-Left gotolinestart Alt-Left | Home selectleft Shift-Left gotoleft Left selectwordright Ctrl-Shift-Right gotowordright Ctrl-Right selecttolineend Alt-Shift-Right gotolineend Alt-Right | End selectright Shift-Right gotoright Right selectpagedown Shift-Pagedown gotopagedown Pagedown selectpageup Shift-Pageup gotopageup Pageup scrollup Ctrl-Up scrolldown Ctrl-Down selectlinestart Shift-Home selectlineend Shift-End removeline Ctrl-D duplicateSelection Ctrl-Shift-D togglecomment Ctrl-/ modifyNumberUp Ctrl-Shift-Up modifyNumberDown Ctrl-Shift-Down replace Ctrl-H undo Ctrl-Z copylinesup Alt-Shift-Up movelinesup Alt-Up copylinesdown Alt-Shift-Down movelinesdown Alt-Down del Delete backspace Shift-Backspace | Backspace cut_or_delete Shift-Delete removetolinestart Alt-Backspace removetolineend Alt-Delete removetolinestarthard Ctrl-Shift-Backspace removetolineendhard Ctrl-Shift-Delete removewordleft Ctrl-Backspace removewordright Ctrl-Delete indent Tab blockindent Ctrl-] touppercase Ctrl-U tolowercase Ctrl-Shift-U expandtoline Ctrl-Shift-L addCursorAbove Ctrl-Alt-Up addCursorBelow Ctrl-Alt-Down addCursorAboveSkipCurrent Ctrl-Alt-Shift-Up addCursorBelowSkipCurrent Ctrl-Alt-Shift-Down selectMoreBefore Ctrl-Alt-Left selectMoreAfter Ctrl-Alt-Right selectNextBefore Ctrl-Alt-Shift-Left selectNextAfter Ctrl-Alt-Shift-Right findAll Ctrl-Alt-K","title":"Keyboard shortcuts for SQL workspace"},{"location":"working-with-engines/","text":"Working with engines Firebolt engines are compute clusters that do the work when you run SQL jobs on your data. This section covers engine concepts, followed by guidance and instructions for working with engines using DDL, the REST API, and the Firebolt Manager. Topics Understanding engine fundamentals Working with engines using DDL Working with engines using the REST API Working with engines using the Firebolt Manager Tuning engine performance","title":"Working with engines"},{"location":"working-with-engines/#working-with-engines","text":"Firebolt engines are compute clusters that do the work when you run SQL jobs on your data. This section covers engine concepts, followed by guidance and instructions for working with engines using DDL, the REST API, and the Firebolt Manager.","title":"Working with engines"},{"location":"working-with-engines/#topics","text":"Understanding engine fundamentals Working with engines using DDL Working with engines using the REST API Working with engines using the Firebolt Manager Tuning engine performance","title":"Topics"},{"location":"working-with-engines/engine-best-practices/","text":"Guidelines and best practices for engine configuration The guidelines and best practices in this section provide a foundation to begin configuring engines to achieve the right balance of performance and cost. No two workloads are the same, so we recommend that you experiment with different engine configurations on pre-production data sets to compare cost and performance until you find a configuration that works for your production requirements. Overall, your goal is to match the engine spec and scale to meet the performance demands of your workload with the lowest cost. Sizing general purpose engines for ingestion and ELT Ingestion speeds scale linearly with the number of nodes in an engine. So, as a general rule, match the profile of your engine configuration to the profile of your source data as much as possible. If you are ingesting large data sets, adding nodes to an engine has the biggest impact on performance. Similarly, if you have small, incremental ingestion tasks, you can use very few nodes. In addition, scale out the number of nodes in an engine as the number of files to ingest from your data store increases. In this case, scaling out has a more significant impact on performance than using nodes that individually have more RAM, SSD storage, and CPUs. As the size of files to be ingested increases, however, choose nodes that have more RAM, storage, and CPUs. This helps ensure that each node can accommodate the computational and storage demands of ingesting large files. Storage-optimized instance types are a good starting point for experimenting with general purpose engines for ingestion and ELT. Sizing analytics engines For analytics engines, use the general guidelines below. Aggregation queries benefit from increased CPU - Aggregation queries tend to be computationally intensive. For this reason, choosing compute-optimized instance types, or instances that have a greater number of CPU cores, is likely to improve performance. Join queries benefit from RAM - Join queries tend to be memory intensive, with large data sets cached in RAM during join operations. To contain all join indexes fully in memory, we recommend that you select memory-optimized instance types or instances with more RAM. The goal is to have enough RAM in each node to contain all join indexes fully in memory. This avoids spilling to disk and the time-consuming disk I/O associated with it.","title":"Guidelines and best practices for engine configuration"},{"location":"working-with-engines/engine-best-practices/#guidelines-and-best-practices-for-engine-configuration","text":"The guidelines and best practices in this section provide a foundation to begin configuring engines to achieve the right balance of performance and cost. No two workloads are the same, so we recommend that you experiment with different engine configurations on pre-production data sets to compare cost and performance until you find a configuration that works for your production requirements. Overall, your goal is to match the engine spec and scale to meet the performance demands of your workload with the lowest cost.","title":"Guidelines and best practices for engine configuration"},{"location":"working-with-engines/engine-best-practices/#sizing-general-purpose-engines-for-ingestion-and-elt","text":"Ingestion speeds scale linearly with the number of nodes in an engine. So, as a general rule, match the profile of your engine configuration to the profile of your source data as much as possible. If you are ingesting large data sets, adding nodes to an engine has the biggest impact on performance. Similarly, if you have small, incremental ingestion tasks, you can use very few nodes. In addition, scale out the number of nodes in an engine as the number of files to ingest from your data store increases. In this case, scaling out has a more significant impact on performance than using nodes that individually have more RAM, SSD storage, and CPUs. As the size of files to be ingested increases, however, choose nodes that have more RAM, storage, and CPUs. This helps ensure that each node can accommodate the computational and storage demands of ingesting large files. Storage-optimized instance types are a good starting point for experimenting with general purpose engines for ingestion and ELT.","title":"Sizing general purpose engines for ingestion and ELT"},{"location":"working-with-engines/engine-best-practices/#sizing-analytics-engines","text":"For analytics engines, use the general guidelines below. Aggregation queries benefit from increased CPU - Aggregation queries tend to be computationally intensive. For this reason, choosing compute-optimized instance types, or instances that have a greater number of CPU cores, is likely to improve performance. Join queries benefit from RAM - Join queries tend to be memory intensive, with large data sets cached in RAM during join operations. To contain all join indexes fully in memory, we recommend that you select memory-optimized instance types or instances with more RAM. The goal is to have enough RAM in each node to contain all join indexes fully in memory. This avoids spilling to disk and the time-consuming disk I/O associated with it.","title":"Sizing analytics engines"},{"location":"working-with-engines/tuning-engine-performance/","text":"Tuning engine performance We recommend that you experiment and tune different engine configurations on pre-production data workflows before you deploy to production. Firebolt's decoupled compute and storage architecture makes that easy. You can play with configurations in pre-production with little effort and low cost. Firebolt chose default engine configurations for databases because they work well in most scenarios, providing significant performance gains and cost savings \u201cout of the box.\u201d We recommend that you start with the default ingest and analytics engine configurations to see how they perform. If you want to increase performance or lower cost even further, the guidance in this topic can help you identify tuning opportunities. Understanding performance factors The engine specification (the node type) and scale (the number of nodes) are the fundamental configurations that you use to tune engine performance. Another configuration parameter, the warmup method, affects engine start times. For information about warmup choices, see Warmup method . For ingestion engines, tuning considerations include the source data to be ingested, the complexity of the data structures, and the response time that your application demands. For analytics engines, considerations include the complexity of aggregations and joins that you use in your queries. We explore these performance aspects and how they affect your tuning adjustments in more detail below. Tuning ingestion engines The first consideration for ingestion engines is the size of individual files and the entire file payload. Large files place different demands on an engine than small files. If your files are between 100 MB and 32 GB, the default ingestion engine specification and size is usually an excellent fit. Consider tuning adjustments in any of the following situations: Individual files or your total ingestion payload are outside the 100 MB \u2013 32 GB range. You ingest complex data structures found in Parquet, JSON, and other nested and semi-structured formats. You ingest many files, especially small files. Choose RAM based on file and payload size Firebolt caches files in RAM when processing them for ingestion. You want enough RAM available to each node, and to the entire engine, to accommodate caching. The more completely your engine can cache files during this process, the faster the engine performs. The size of the ingestion payload should never exceed the total amount of RAM available to the engine. This may cause an out of memory condition and cause ingestion to fail. Adjust node RAM using the engine specification, and then use scale to increase overall engine RAM For large files and payloads, choose an engine specification that provides each node with enough RAM for your largest individual file. Aim for RAM that is 1.5x to 2x the size of the largest file. If node RAM can\u2019t cover the size of the entire payload, adjust engine scale to add nodes. When you scale, aim for engine-wide RAM to exceed the size of the entire payload by the same 1.5x to 2x ratio. This same guidance to add nodes applies to a large file that exceeds any available node capacity. If you ingest smaller files, a single node engine is often enough. Increase the engine specification so that node RAM can cover the payload using the 1.5x to 2x guidance from above. With small files, it\u2019s often possible to decrease the engine specification without a significant impact to performance. For improving the ingestion time of small files, it\u2019s especially important to increase node RAM using the engine specification before you add nodes. Adding nodes may actually diminish performance. This is because additional nodes and many files will amplify the inherent overhead of establishing a connection to Amazon S3 and accessing objects. In addition, if you have many files with a single S3 prefix, AWS throttling limits might become a factor. More on that below. CPU can help relieve RAM demands, especially for complex data structures Firebolt holds heavily compressed or complex data structures in RAM longer during ingestion processing than simple structures. Parquet files and nested JSON are examples of more complex file types with high processing demands. For these file types and data structures, aim for the high end of the 2x RAM ratio mentioned above to begin with. You may even want to exceed it. Also, you may find that increasing the engine specification to improve the CPU or number of cores yields performance gains. This is because files are processed quicker, freeing up RAM for subsequent caching. With many files, file structure in S3 can help Access time to Amazon S3 and the limits that AWS imposes on the number of requests allowed per second (called throttling) play a role in engine configuration. You can add nodes to make requests across your data set more quickly, but you want to be mindful that there is a limit of 3.5K requests per second, per S3 prefix (folder). AWS may update these values occasionally. If you need to scale out to improve ingestion performance for a large number of files, save files at the source in different folders. For more information, see Performance Design Patterns for Amazon S3 in Amazon S3 documentation. For example, if you have 10,000 files to ingest, and you specify a URL like s3://samplecorp/sampledata in your external table definition, avoid placing too many files in that folder (or bucket). Instead, divide the files into sub-folders like s3://samplecorp/sampledata/trancheA , s3://samplecorp/sampledata/trancheB , s3://samplecorp/sampledata/trancheC , and so on, so that requests to each folder remain under the S3 throttling limit. Tuning analytics engines After data is ingested into Firebolt, your analytics engines process fact and dimension tables for insight. With analytics engines, the complexity of data structures and the queries that you perform are essential considerations. As with ingestion engines, the default configuration is a good place to begin as you seek to improve performance or lower cost. With simple queries that have limited joins and aggregations, you can often keep a single-node engine and use a less powerful engine specification than the default, and still maintain a performance standard to meet application requirements. Good indexes help engines Indexes are to Firebolt analytics engines what the suspension and transmission are to a car engine. They help deliver the engine\u2019s power efficiently for top performance. Developing excellent indexes can help to mitigate the demands on an engine, improving performance and lowering cost. Tuning indexes is a topic of its own. For more information, see Using Indexes for faster queries . Mix and match engines for different query types You can run as many analytics engines on a database at the same time as you need, using different engines for different queries. Because analytics engines don\u2019t write data, you don\u2019t need to be concerned about issues with locked data or data inconsistency. Queries that perform complex aggregations and multiple joins are likely to place greater demands on an analytics engine, requiring more processing power, RAM, or both. For these engines, consider modifying the engine specification to add RAM or increase the CPU of nodes according to the guidance below. Join queries benefit from increased RAM - Join queries tend to be memory intensive, with large data sets cached in RAM during join operations. Try increasing the engine specification with more RAM per node. Aim for the memory of each node to be enough to contain all the join indexes fully in memory. Next, add nodes for additional RAM when running large and complex joins. Aggregation queries benefit from increased CPU - Aggregation queries tend to be computationally intensive. Using an engine specification with more CPU power per node is likely to improve performance.","title":"Tuning engine performance"},{"location":"working-with-engines/tuning-engine-performance/#tuning-engine-performance","text":"We recommend that you experiment and tune different engine configurations on pre-production data workflows before you deploy to production. Firebolt's decoupled compute and storage architecture makes that easy. You can play with configurations in pre-production with little effort and low cost. Firebolt chose default engine configurations for databases because they work well in most scenarios, providing significant performance gains and cost savings \u201cout of the box.\u201d We recommend that you start with the default ingest and analytics engine configurations to see how they perform. If you want to increase performance or lower cost even further, the guidance in this topic can help you identify tuning opportunities.","title":"Tuning engine performance"},{"location":"working-with-engines/tuning-engine-performance/#understanding-performance-factors","text":"The engine specification (the node type) and scale (the number of nodes) are the fundamental configurations that you use to tune engine performance. Another configuration parameter, the warmup method, affects engine start times. For information about warmup choices, see Warmup method . For ingestion engines, tuning considerations include the source data to be ingested, the complexity of the data structures, and the response time that your application demands. For analytics engines, considerations include the complexity of aggregations and joins that you use in your queries. We explore these performance aspects and how they affect your tuning adjustments in more detail below.","title":"Understanding performance factors"},{"location":"working-with-engines/tuning-engine-performance/#tuning-ingestion-engines","text":"The first consideration for ingestion engines is the size of individual files and the entire file payload. Large files place different demands on an engine than small files. If your files are between 100 MB and 32 GB, the default ingestion engine specification and size is usually an excellent fit. Consider tuning adjustments in any of the following situations: Individual files or your total ingestion payload are outside the 100 MB \u2013 32 GB range. You ingest complex data structures found in Parquet, JSON, and other nested and semi-structured formats. You ingest many files, especially small files.","title":"Tuning ingestion engines"},{"location":"working-with-engines/tuning-engine-performance/#choose-ram-based-on-file-and-payload-size","text":"Firebolt caches files in RAM when processing them for ingestion. You want enough RAM available to each node, and to the entire engine, to accommodate caching. The more completely your engine can cache files during this process, the faster the engine performs. The size of the ingestion payload should never exceed the total amount of RAM available to the engine. This may cause an out of memory condition and cause ingestion to fail.","title":"Choose RAM based on file and payload size"},{"location":"working-with-engines/tuning-engine-performance/#adjust-node-ram-using-the-engine-specification-and-then-use-scale-to-increase-overall-engine-ram","text":"For large files and payloads, choose an engine specification that provides each node with enough RAM for your largest individual file. Aim for RAM that is 1.5x to 2x the size of the largest file. If node RAM can\u2019t cover the size of the entire payload, adjust engine scale to add nodes. When you scale, aim for engine-wide RAM to exceed the size of the entire payload by the same 1.5x to 2x ratio. This same guidance to add nodes applies to a large file that exceeds any available node capacity. If you ingest smaller files, a single node engine is often enough. Increase the engine specification so that node RAM can cover the payload using the 1.5x to 2x guidance from above. With small files, it\u2019s often possible to decrease the engine specification without a significant impact to performance. For improving the ingestion time of small files, it\u2019s especially important to increase node RAM using the engine specification before you add nodes. Adding nodes may actually diminish performance. This is because additional nodes and many files will amplify the inherent overhead of establishing a connection to Amazon S3 and accessing objects. In addition, if you have many files with a single S3 prefix, AWS throttling limits might become a factor. More on that below.","title":"Adjust node RAM using the engine specification, and then use scale to increase overall engine RAM"},{"location":"working-with-engines/tuning-engine-performance/#cpu-can-help-relieve-ram-demands-especially-for-complex-data-structures","text":"Firebolt holds heavily compressed or complex data structures in RAM longer during ingestion processing than simple structures. Parquet files and nested JSON are examples of more complex file types with high processing demands. For these file types and data structures, aim for the high end of the 2x RAM ratio mentioned above to begin with. You may even want to exceed it. Also, you may find that increasing the engine specification to improve the CPU or number of cores yields performance gains. This is because files are processed quicker, freeing up RAM for subsequent caching.","title":"CPU can help relieve RAM demands, especially for complex data structures"},{"location":"working-with-engines/tuning-engine-performance/#with-many-files-file-structure-in-s3-can-help","text":"Access time to Amazon S3 and the limits that AWS imposes on the number of requests allowed per second (called throttling) play a role in engine configuration. You can add nodes to make requests across your data set more quickly, but you want to be mindful that there is a limit of 3.5K requests per second, per S3 prefix (folder). AWS may update these values occasionally. If you need to scale out to improve ingestion performance for a large number of files, save files at the source in different folders. For more information, see Performance Design Patterns for Amazon S3 in Amazon S3 documentation. For example, if you have 10,000 files to ingest, and you specify a URL like s3://samplecorp/sampledata in your external table definition, avoid placing too many files in that folder (or bucket). Instead, divide the files into sub-folders like s3://samplecorp/sampledata/trancheA , s3://samplecorp/sampledata/trancheB , s3://samplecorp/sampledata/trancheC , and so on, so that requests to each folder remain under the S3 throttling limit.","title":"With many files, file structure in S3 can help"},{"location":"working-with-engines/tuning-engine-performance/#tuning-analytics-engines","text":"After data is ingested into Firebolt, your analytics engines process fact and dimension tables for insight. With analytics engines, the complexity of data structures and the queries that you perform are essential considerations. As with ingestion engines, the default configuration is a good place to begin as you seek to improve performance or lower cost. With simple queries that have limited joins and aggregations, you can often keep a single-node engine and use a less powerful engine specification than the default, and still maintain a performance standard to meet application requirements.","title":"Tuning analytics engines"},{"location":"working-with-engines/tuning-engine-performance/#good-indexes-help-engines","text":"Indexes are to Firebolt analytics engines what the suspension and transmission are to a car engine. They help deliver the engine\u2019s power efficiently for top performance. Developing excellent indexes can help to mitigate the demands on an engine, improving performance and lowering cost. Tuning indexes is a topic of its own. For more information, see Using Indexes for faster queries .","title":"Good indexes help engines"},{"location":"working-with-engines/tuning-engine-performance/#mix-and-match-engines-for-different-query-types","text":"You can run as many analytics engines on a database at the same time as you need, using different engines for different queries. Because analytics engines don\u2019t write data, you don\u2019t need to be concerned about issues with locked data or data inconsistency. Queries that perform complex aggregations and multiple joins are likely to place greater demands on an analytics engine, requiring more processing power, RAM, or both. For these engines, consider modifying the engine specification to add RAM or increase the CPU of nodes according to the guidance below. Join queries benefit from increased RAM - Join queries tend to be memory intensive, with large data sets cached in RAM during join operations. Try increasing the engine specification with more RAM per node. Aim for the memory of each node to be enough to contain all the join indexes fully in memory. Next, add nodes for additional RAM when running large and complex joins. Aggregation queries benefit from increased CPU - Aggregation queries tend to be computationally intensive. Using an engine specification with more CPU power per node is likely to improve performance.","title":"Mix and match engines for different query types"},{"location":"working-with-engines/understanding-engine-fundamentals/","text":"Understanding engine fundamentals Firebolt engines are attached to databases. Each engine provides isolated and assured compute and storage capacity that you can use for different workloads and queries. Engines are configurable and efficient. You can quickly set them up with the capabilities that you need, start them only when you need them, and configure them to stop automatically when not in use. Engine costs accrue only while an engine is running, so you can create as many engines as you like to compare cost and performance and to handle different aspects of your workload. How engines, databases, and scripts work together You can attach as many engines as you need to a database. One common database setup is to have two engines attached: one general purpose engine for data ingestion, and one analytics engine to execute queries. For more information, see Engine type below. A running engine is committed to executing SQL scripts, so an engine must be stopped for you to perform any action on it. You can create, edit, attach, start, stop, and delete stopped engines at any time during the life of a database. When you run a SQL script, you select the engine to use. For databases with multiple engines, you can select one as the default engine. You can run a script on one engine and then switch to a different engine to compare performance and cost. You can also write business logic in your applications outside Firebolt to start engines programmatically. Understanding engine properties Before you create or edit an engine, it\u2019s helpful to understand the details of the engine properties listed below. Engine name The engine name is how you identify the engine. DDL operations use it to identify the engine to act on. The engine name must be unique throughout your Firebolt account. Keep this in mind when creating an engine using DDL. When you create an engine using the Firebolt Manager, the database name is prepended automatically to the engine name to help ensure uniqueness. Engine names can be no longer than 63 characters and can contain letters, numbers, and the underscore character. Firebolt replaces underscores with dashes when the engine URL is formulated (see below). Other special characters and spaces are not supported. Region This is the AWS Region in which Firebolt creates the engine. After you create an engine, you can\u2019t change its Region. An engine must be in the same Region as its database. When you create an engine using the Firebolt Manager, Firebolt automatically creates the engine in the same Region as the database. When you create an engine using DDL, make sure that you create it in the same Region as the database you will attach it to. Engine endpoint Each engine has an endpoint that you use when submitting operations to an engine using the Firebolt REST API. For example, you can use a POST command to submit a script to the engine's https URL to run the script. Each engine endpoint uses the engine name, your account name, and the AWS Region ID according to the pattern shown in the https example below. your-engine-name is the name of your engine, with dashes replacing any underscore characters. For example, the engine name YourDatabase_YourEngine is represented as yourdatabase-yourengine in the URL. firebolt-account-name is the name of your Firebolt account. For example, YourAccount is represented as youraccount . region-id is the AWS Region identifier where the engine lives. For example, us-east-1 . https://your-engine-name.firebolt-account-name.region-id.app.firebolt.io The example below shows an endpoint for an engine named maindb_engine1 in the Region us-east-1 within the Firebolt account AnyCompany. https://maindb-engine1.anycompany.us-east-1.app.firebolt.io Engine type Engines can be one of two types : General purpose engines can do everything analytics engines do, but can also write data to Firebolt tables. They are designed for database creation, data ingestion, and extract, load, and transform (ELT) operations. Having more than one general purpose engine writing to a table can cause data inconsistency and errors. We strongly recommend creating only one general purpose engine per database. Analytics engines are read-only and are designed for queries that do not ingest data. They can't write values. You can run as many analytics engines as you need at the same time. Engine spec When you choose an engine spec , you choose the instance type that is the foundation of the engine\u2019s compute capabilities. Each instance has CPU, RAM, and storage characteristics along with a cost per hour (billed per second) for each instance. You can choose to build an engine on one of several instance types. Firebolt has pre-selected these instance types for characteristics that are best suited for Firebolt workloads. Scale Scale determines the number of nodes that the engine uses and can be an integer ranging from 1 to 128. Firebolt monitors the health of nodes on a continuous basis and automatically repairs nodes that report an unhealthy status. To help ensure uninterrupted operation of engines if a node becomes unhealthy, we recommend a scale of two or more. Warmup method This determines the behavior of the engine on startup. You have three options: Minimal \u2013 The engine loads only join indexes at startup. Other indexes and data are loaded from Firebolt when a query that uses them first runs. This results in faster engine start times, but slower first queries. Preload indexes \u2013 Default. The engine loads primary indexes and join indexes at startup, before the first queries run. First queries are faster than they are with minimal warmup, but engines take longer to start. Preload all data \u2013 The engine loads all indexes and data at startup, before the first queries run. This results in the fastest queries with the slowest engine start times. Only use this option if the size of the database (as shown using the SHOW DATABASES SQL statement or in the Firebolt Manager) will not exceed the total amount of SSD storage available on the engine. Auto-stop duration The period of inactivity, in minutes, after which an engine shuts down automatically to save cost. The default is 20 minutes. Using CREATE ENGINE and ALTER ENGINE SQL statements, you can specify auto-stop duration in one-minute increments. For more information, see CREATE ENGINE and ALTER ENGINE . Using the Firebolt Manager, you can set the auto-stop duration to always on, 20 minutes, or 60 minutes. Viewing and understanding engine states You can execute a SHOW ENGINES statement to list all engines in your Firebolt account and view engine status. You can also use the Databases list or the Engines list in the Firebolt Manager. On - the engine is running queries or available to run queries. You cannot edit, delete, or attach a running engine. Starting - the engine was started. It is provisioning resources, warming up, and will be ready to use shortly. Stopped - the engine was stopped. It is not available to run queries. You can edit, delete, or attach engines in this state. Stopping - the engine is shutting down. It is finishing query tasks in process and is not available for new queries. Dropping - the engine is being deleted. It is finishing query tasks in process and is not available for new queries. Repairing - at least one node is out of service. Firebolt is working on replacing nodes. The engine is not available to run queries while in this state. Firebolt engines go into repairing status if there is an infrastructure or software failure. When an engine is repairing, it stops any current query actions.","title":"Understanding engine fundamentals"},{"location":"working-with-engines/understanding-engine-fundamentals/#understanding-engine-fundamentals","text":"Firebolt engines are attached to databases. Each engine provides isolated and assured compute and storage capacity that you can use for different workloads and queries. Engines are configurable and efficient. You can quickly set them up with the capabilities that you need, start them only when you need them, and configure them to stop automatically when not in use. Engine costs accrue only while an engine is running, so you can create as many engines as you like to compare cost and performance and to handle different aspects of your workload.","title":"Understanding engine fundamentals"},{"location":"working-with-engines/understanding-engine-fundamentals/#how-engines-databases-and-scripts-work-together","text":"You can attach as many engines as you need to a database. One common database setup is to have two engines attached: one general purpose engine for data ingestion, and one analytics engine to execute queries. For more information, see Engine type below. A running engine is committed to executing SQL scripts, so an engine must be stopped for you to perform any action on it. You can create, edit, attach, start, stop, and delete stopped engines at any time during the life of a database. When you run a SQL script, you select the engine to use. For databases with multiple engines, you can select one as the default engine. You can run a script on one engine and then switch to a different engine to compare performance and cost. You can also write business logic in your applications outside Firebolt to start engines programmatically.","title":"How engines, databases, and scripts work together"},{"location":"working-with-engines/understanding-engine-fundamentals/#understanding-engine-properties","text":"Before you create or edit an engine, it\u2019s helpful to understand the details of the engine properties listed below.","title":"Understanding engine properties"},{"location":"working-with-engines/understanding-engine-fundamentals/#engine-name","text":"The engine name is how you identify the engine. DDL operations use it to identify the engine to act on. The engine name must be unique throughout your Firebolt account. Keep this in mind when creating an engine using DDL. When you create an engine using the Firebolt Manager, the database name is prepended automatically to the engine name to help ensure uniqueness. Engine names can be no longer than 63 characters and can contain letters, numbers, and the underscore character. Firebolt replaces underscores with dashes when the engine URL is formulated (see below). Other special characters and spaces are not supported.","title":"Engine name"},{"location":"working-with-engines/understanding-engine-fundamentals/#region","text":"This is the AWS Region in which Firebolt creates the engine. After you create an engine, you can\u2019t change its Region. An engine must be in the same Region as its database. When you create an engine using the Firebolt Manager, Firebolt automatically creates the engine in the same Region as the database. When you create an engine using DDL, make sure that you create it in the same Region as the database you will attach it to.","title":"Region"},{"location":"working-with-engines/understanding-engine-fundamentals/#engine-endpoint","text":"Each engine has an endpoint that you use when submitting operations to an engine using the Firebolt REST API. For example, you can use a POST command to submit a script to the engine's https URL to run the script. Each engine endpoint uses the engine name, your account name, and the AWS Region ID according to the pattern shown in the https example below. your-engine-name is the name of your engine, with dashes replacing any underscore characters. For example, the engine name YourDatabase_YourEngine is represented as yourdatabase-yourengine in the URL. firebolt-account-name is the name of your Firebolt account. For example, YourAccount is represented as youraccount . region-id is the AWS Region identifier where the engine lives. For example, us-east-1 . https://your-engine-name.firebolt-account-name.region-id.app.firebolt.io The example below shows an endpoint for an engine named maindb_engine1 in the Region us-east-1 within the Firebolt account AnyCompany. https://maindb-engine1.anycompany.us-east-1.app.firebolt.io","title":"Engine endpoint"},{"location":"working-with-engines/understanding-engine-fundamentals/#engine-type","text":"Engines can be one of two types : General purpose engines can do everything analytics engines do, but can also write data to Firebolt tables. They are designed for database creation, data ingestion, and extract, load, and transform (ELT) operations. Having more than one general purpose engine writing to a table can cause data inconsistency and errors. We strongly recommend creating only one general purpose engine per database. Analytics engines are read-only and are designed for queries that do not ingest data. They can't write values. You can run as many analytics engines as you need at the same time.","title":"Engine type"},{"location":"working-with-engines/understanding-engine-fundamentals/#engine-spec","text":"When you choose an engine spec , you choose the instance type that is the foundation of the engine\u2019s compute capabilities. Each instance has CPU, RAM, and storage characteristics along with a cost per hour (billed per second) for each instance. You can choose to build an engine on one of several instance types. Firebolt has pre-selected these instance types for characteristics that are best suited for Firebolt workloads.","title":"Engine spec"},{"location":"working-with-engines/understanding-engine-fundamentals/#scale","text":"Scale determines the number of nodes that the engine uses and can be an integer ranging from 1 to 128. Firebolt monitors the health of nodes on a continuous basis and automatically repairs nodes that report an unhealthy status. To help ensure uninterrupted operation of engines if a node becomes unhealthy, we recommend a scale of two or more.","title":"Scale"},{"location":"working-with-engines/understanding-engine-fundamentals/#warmup-method","text":"This determines the behavior of the engine on startup. You have three options: Minimal \u2013 The engine loads only join indexes at startup. Other indexes and data are loaded from Firebolt when a query that uses them first runs. This results in faster engine start times, but slower first queries. Preload indexes \u2013 Default. The engine loads primary indexes and join indexes at startup, before the first queries run. First queries are faster than they are with minimal warmup, but engines take longer to start. Preload all data \u2013 The engine loads all indexes and data at startup, before the first queries run. This results in the fastest queries with the slowest engine start times. Only use this option if the size of the database (as shown using the SHOW DATABASES SQL statement or in the Firebolt Manager) will not exceed the total amount of SSD storage available on the engine.","title":"Warmup method"},{"location":"working-with-engines/understanding-engine-fundamentals/#auto-stop-duration","text":"The period of inactivity, in minutes, after which an engine shuts down automatically to save cost. The default is 20 minutes. Using CREATE ENGINE and ALTER ENGINE SQL statements, you can specify auto-stop duration in one-minute increments. For more information, see CREATE ENGINE and ALTER ENGINE . Using the Firebolt Manager, you can set the auto-stop duration to always on, 20 minutes, or 60 minutes.","title":"Auto-stop duration"},{"location":"working-with-engines/understanding-engine-fundamentals/#viewing-and-understanding-engine-states","text":"You can execute a SHOW ENGINES statement to list all engines in your Firebolt account and view engine status. You can also use the Databases list or the Engines list in the Firebolt Manager. On - the engine is running queries or available to run queries. You cannot edit, delete, or attach a running engine. Starting - the engine was started. It is provisioning resources, warming up, and will be ready to use shortly. Stopped - the engine was stopped. It is not available to run queries. You can edit, delete, or attach engines in this state. Stopping - the engine is shutting down. It is finishing query tasks in process and is not available for new queries. Dropping - the engine is being deleted. It is finishing query tasks in process and is not available for new queries. Repairing - at least one node is out of service. Firebolt is working on replacing nodes. The engine is not available to run queries while in this state. Firebolt engines go into repairing status if there is an infrastructure or software failure. When an engine is repairing, it stops any current query actions.","title":"Viewing and understanding engine states"},{"location":"working-with-engines/working-with-engines-using-ddl/","text":"Working with engines using DDL You can execute the operations on engines listed below using SQL statements. List (SHOW) engines Start an engine Stop an engine Create an engine Attach an engine to a database Edit (ALTER) an engine Delete (DROP) an engine To list (SHOW) all engines in your Firebolt account Using a running engine, execute the SHOW ENGINES statement as shown in the example below. SHOW ENGINES; The statement returns a list of engines by name, including the region, engine specification, scale, status, and attached database. To start an engine Using a running engine, execute a START ENGINE statement similar to the example below. START ENGINE MyDatabase_MyFireboltEngine To stop an engine Execute a STOP ENGINE command using a running engine similar to the example below. You can use the same engine that you are stopping. STOP ENGINE MyDatabase_MyFireboltEngine To create an engine Using a running engine, execute a CREATE ENGINE statement similar to one of the examples below. For more information, see CREATE ENGINE . Example - Create engine using default values CREATE ENGINE MyDatabase_MyFireboltEngine; GENERAL_PURPOSE is the default engine type if none is specified. Example - Create an analytics engine, specifying all properties CREATE ENGINE MyDatabase_MyFireboltEngine WITH REGION = 'us-west-2' ENGINE_TYPE = DATA_ANALYTICS SCALE = 32 SPEC = 'c5d.4xlarge' AUTO_STOP = '45' WARMUP = PRELOAD_ALL_DATA; To attach an engine to a database This action is available only using DDL. Using a running engine, execute an ATTACH ENGINE statement similar to the example below. sql ATTACH ENGINE MyDatabase_MyFireboltEngine TO MyDatabase; To edit (ALTER) an engine Using a running engine, execute an ALTER ENGINE statement similar to the example below. ALTER ENGINE MyDatabase_MyFireboltEngine SET SCALE = 12 SPEC = 'c5d.4xlarge' AUTO_STOP = '45' RENAME TO 'MyProductionDatabase_MyFireboltEngine' WARMUP = PRELOAD_ALL_DATA; To delete (DROP) an engine Using a running engine, execute a DROP ENGINE SQL statement similar to the example below. DROP ENGINE MyDatabase_MyFireboltEngine;","title":"Working with engines using DDL"},{"location":"working-with-engines/working-with-engines-using-ddl/#working-with-engines-using-ddl","text":"You can execute the operations on engines listed below using SQL statements. List (SHOW) engines Start an engine Stop an engine Create an engine Attach an engine to a database Edit (ALTER) an engine Delete (DROP) an engine","title":"Working with engines using DDL"},{"location":"working-with-engines/working-with-engines-using-ddl/#to-list-show-all-engines-in-your-firebolt-account","text":"Using a running engine, execute the SHOW ENGINES statement as shown in the example below. SHOW ENGINES; The statement returns a list of engines by name, including the region, engine specification, scale, status, and attached database.","title":"To list (SHOW) all engines in your Firebolt account"},{"location":"working-with-engines/working-with-engines-using-ddl/#to-start-an-engine","text":"Using a running engine, execute a START ENGINE statement similar to the example below. START ENGINE MyDatabase_MyFireboltEngine","title":"To start an engine"},{"location":"working-with-engines/working-with-engines-using-ddl/#to-stop-an-engine","text":"Execute a STOP ENGINE command using a running engine similar to the example below. You can use the same engine that you are stopping. STOP ENGINE MyDatabase_MyFireboltEngine","title":"To stop an engine"},{"location":"working-with-engines/working-with-engines-using-ddl/#to-create-an-engine","text":"Using a running engine, execute a CREATE ENGINE statement similar to one of the examples below. For more information, see CREATE ENGINE .","title":"To create an engine"},{"location":"working-with-engines/working-with-engines-using-ddl/#example-create-engine-using-default-values","text":"CREATE ENGINE MyDatabase_MyFireboltEngine; GENERAL_PURPOSE is the default engine type if none is specified.","title":"Example - Create engine using default values"},{"location":"working-with-engines/working-with-engines-using-ddl/#example-create-an-analytics-engine-specifying-all-properties","text":"CREATE ENGINE MyDatabase_MyFireboltEngine WITH REGION = 'us-west-2' ENGINE_TYPE = DATA_ANALYTICS SCALE = 32 SPEC = 'c5d.4xlarge' AUTO_STOP = '45' WARMUP = PRELOAD_ALL_DATA;","title":"Example - Create an analytics engine, specifying all properties"},{"location":"working-with-engines/working-with-engines-using-ddl/#to-attach-an-engine-to-a-database","text":"This action is available only using DDL. Using a running engine, execute an ATTACH ENGINE statement similar to the example below. sql ATTACH ENGINE MyDatabase_MyFireboltEngine TO MyDatabase;","title":"To attach an engine to a database"},{"location":"working-with-engines/working-with-engines-using-ddl/#to-edit-alter-an-engine","text":"Using a running engine, execute an ALTER ENGINE statement similar to the example below. ALTER ENGINE MyDatabase_MyFireboltEngine SET SCALE = 12 SPEC = 'c5d.4xlarge' AUTO_STOP = '45' RENAME TO 'MyProductionDatabase_MyFireboltEngine' WARMUP = PRELOAD_ALL_DATA;","title":"To edit (ALTER) an engine"},{"location":"working-with-engines/working-with-engines-using-ddl/#to-delete-drop-an-engine","text":"Using a running engine, execute a DROP ENGINE SQL statement similar to the example below. DROP ENGINE MyDatabase_MyFireboltEngine;","title":"To delete (DROP) an engine"},{"location":"working-with-engines/working-with-engines-using-the-firebolt-manager/","text":"Working with engines using the Firebolt Manager You can create, edit, and delete an engine using the Firebolt Manager. Keep in mind that an engine must be stopped for you to perform these tasks. For more information about settings and their meaning, see Understanding engine properties . To create or edit an engine using the Firebolt Manager If you are creating a new database, from the Databases list, choose New database . \u2014or\u2014 If you are adding an engine to an existing database, choose the ellipses on the far right of the database row, and then choose Edit database . To create a new engine, choose Add new engine . \u2014or\u2014 To edit an existing engine, under Database engines , choose the engine to edit. Enter an Engine name . Engine names automatically begin with the name of the database they are attached to. Choose an Engine type . Under Engine spec , choose an EC2 instance type from the list. Total engine stats and the engine list in the left pane update to reflect your selection. Under Engine scale , select a scale factor of 1 to 128. Total engine stats and the engine list in the left pane update to reflect your selection. Select a Warmup method and Auto-stop duration. Choose Update database . You return to the Databases list, and your database appears under Database Engines . To delete an engine using the Firebolt Manager From the Databases list, under Database engines , choose the delete icon for the database you want to delete. Starting and stopping engines You can start and stop engines in the Firebolt Manager using any of the methods below. From the SQL Workspace \u2013 You can choose Run Script . If the selected engine isn\u2019t running, you can choose to start it. You can also select an engine from the list at the bottom of the scripting tab and choose Start or Stop . From the Databases list \u2013 Choose the ellipses to the far right of the database row, choose Edit database , select the engine you want to stop, and then choose Stop . From the Engines list \u2013 Choose the ellipses to the far right of the engine row, and then choose Start or Stop .","title":"Working with engines using the Firebolt Manager"},{"location":"working-with-engines/working-with-engines-using-the-firebolt-manager/#working-with-engines-using-the-firebolt-manager","text":"You can create, edit, and delete an engine using the Firebolt Manager. Keep in mind that an engine must be stopped for you to perform these tasks. For more information about settings and their meaning, see Understanding engine properties .","title":"Working with engines using the Firebolt Manager"},{"location":"working-with-engines/working-with-engines-using-the-firebolt-manager/#to-create-or-edit-an-engine-using-the-firebolt-manager","text":"If you are creating a new database, from the Databases list, choose New database . \u2014or\u2014 If you are adding an engine to an existing database, choose the ellipses on the far right of the database row, and then choose Edit database . To create a new engine, choose Add new engine . \u2014or\u2014 To edit an existing engine, under Database engines , choose the engine to edit. Enter an Engine name . Engine names automatically begin with the name of the database they are attached to. Choose an Engine type . Under Engine spec , choose an EC2 instance type from the list. Total engine stats and the engine list in the left pane update to reflect your selection. Under Engine scale , select a scale factor of 1 to 128. Total engine stats and the engine list in the left pane update to reflect your selection. Select a Warmup method and Auto-stop duration. Choose Update database . You return to the Databases list, and your database appears under Database Engines .","title":"To create or edit an engine using the Firebolt Manager"},{"location":"working-with-engines/working-with-engines-using-the-firebolt-manager/#to-delete-an-engine-using-the-firebolt-manager","text":"From the Databases list, under Database engines , choose the delete icon for the database you want to delete.","title":"To delete an engine using the Firebolt Manager"},{"location":"working-with-engines/working-with-engines-using-the-firebolt-manager/#starting-and-stopping-engines","text":"You can start and stop engines in the Firebolt Manager using any of the methods below. From the SQL Workspace \u2013 You can choose Run Script . If the selected engine isn\u2019t running, you can choose to start it. You can also select an engine from the list at the bottom of the scripting tab and choose Start or Stop . From the Databases list \u2013 Choose the ellipses to the far right of the database row, choose Edit database , select the engine you want to stop, and then choose Stop . From the Engines list \u2013 Choose the ellipses to the far right of the engine row, and then choose Start or Stop .","title":"Starting and stopping engines"},{"location":"working-with-engines/working-with-engines-using-the-rest-api/","text":"Working with engines using the REST API You can use available Firebolt REST API endpoints to perform engine operations. For more information, see Start, stop, and restart engines .","title":"Working with engines using the REST API"},{"location":"working-with-engines/working-with-engines-using-the-rest-api/#working-with-engines-using-the-rest-api","text":"You can use available Firebolt REST API endpoints to perform engine operations. For more information, see Start, stop, and restart engines .","title":"Working with engines using the REST API"},{"location":"working-with-semi-structured-data/","text":"Working with semi-structured data Background Semi-structured data is any data that does not adhere to a strict tabular schema and/or some of its field types is not of standard SQL type. Such data usually has a nested structure and supports complex data types like arrays, maps, and structs (compound types). The prototypical example of such a data format is JSON, but many other serialization formats such as Avro, Parquet, and ORC support similar features. For reference see semi-structured data functions Arrays are the building blocks of how Firebolt represent semi-structured data, among others, they are used to represent: Arrays of varying lengths in the input, unknown at the creation of the table. These arrays can have arbitrary nesting levels, however, the nesting level should be the same for a given column and known at the creation time of the table. Maps - using two coordinated arrays - one for keys, the other for values. This is especially useful for JSON like semi-structured data sources in which each object can have different keys - so a fixed schema cannot handle such data properly In some cases, when the JSON adheres to a fixed schema, that is, each object has a known set of keys, and nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. This page will introduce the correspondence between semi-structured constructs to firebolt arrays. The subsection will elaborate on the following topics: Working with arrays Ingesting semi-structured data Representing semi-structured data in Firebolt Source Data Throughout this page and its subsections, we will use a set of JSON records that can result from a website's logs/web-analytics platform. We will start with a simple example that will become more involved and realistic as we present new concepts. Each record in the source data represents a \"visit\" or \"session\" on the website. The records will usually be stored in an \"Object per line\" file, that is, each line is a JSON object, although the file as a whole is not a valid JSON. This test file will usually be stored in the Data Lake in a compressed form. More about it in the ingestion section . Assume we have the following two records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Here are some important points to pay attention to: For each record, there are mandatory scalar fields: \"id\", \"StartTime\", and \"Duration\". There is an array of arbitrary length (potentially empty) of \"tags\". There is a \"map\" of user agent properties. Those properties can change from record to record and the full set of potential properties is not known at the creation of the table. Representation as a Firebolt table Let's see how we represent the above semi-structured \"schema\" in a Firebolt table. For the mandatory scalar fields, we will have regular columns. For the \"tags\" array we will define a column whose type will be ARRAY(TEXT) . For the user agent properties map, we will define two columns: one for the keys and one for the values - this is a pattern used to represent maps (or dictionaries) in Firebolt. We will encounter it in many examples. Combining the above will result in the following table - note that the headings specify the column name and type id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] The DDL statement creating the above table will be: CREATE [FACT|DIMENSION] TABLE visits ( id INT, StartTime DATETIME, tags ARRAY(TEXT), agent_props_keys ARRAY(TEXT), agent_props_vals ARRAY(TEXT) ) PRIMARY INDEX ... In the next sections, we will see how to query and manipulate the resultant table ( working with arrays ) and how to ingest and transform the semi-structured data into a Firebolt table ( ingesting semi-structured data ).","title":"Working with semi-structured data"},{"location":"working-with-semi-structured-data/#working-with-semi-structured-data","text":"","title":"Working with semi-structured data"},{"location":"working-with-semi-structured-data/#background","text":"Semi-structured data is any data that does not adhere to a strict tabular schema and/or some of its field types is not of standard SQL type. Such data usually has a nested structure and supports complex data types like arrays, maps, and structs (compound types). The prototypical example of such a data format is JSON, but many other serialization formats such as Avro, Parquet, and ORC support similar features. For reference see semi-structured data functions Arrays are the building blocks of how Firebolt represent semi-structured data, among others, they are used to represent: Arrays of varying lengths in the input, unknown at the creation of the table. These arrays can have arbitrary nesting levels, however, the nesting level should be the same for a given column and known at the creation time of the table. Maps - using two coordinated arrays - one for keys, the other for values. This is especially useful for JSON like semi-structured data sources in which each object can have different keys - so a fixed schema cannot handle such data properly In some cases, when the JSON adheres to a fixed schema, that is, each object has a known set of keys, and nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. This page will introduce the correspondence between semi-structured constructs to firebolt arrays. The subsection will elaborate on the following topics: Working with arrays Ingesting semi-structured data","title":"Background"},{"location":"working-with-semi-structured-data/#representing-semi-structured-data-in-firebolt","text":"","title":"Representing semi-structured data in Firebolt"},{"location":"working-with-semi-structured-data/#source-data","text":"Throughout this page and its subsections, we will use a set of JSON records that can result from a website's logs/web-analytics platform. We will start with a simple example that will become more involved and realistic as we present new concepts. Each record in the source data represents a \"visit\" or \"session\" on the website. The records will usually be stored in an \"Object per line\" file, that is, each line is a JSON object, although the file as a whole is not a valid JSON. This test file will usually be stored in the Data Lake in a compressed form. More about it in the ingestion section . Assume we have the following two records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Here are some important points to pay attention to: For each record, there are mandatory scalar fields: \"id\", \"StartTime\", and \"Duration\". There is an array of arbitrary length (potentially empty) of \"tags\". There is a \"map\" of user agent properties. Those properties can change from record to record and the full set of potential properties is not known at the creation of the table.","title":"Source Data"},{"location":"working-with-semi-structured-data/#representation-as-a-firebolt-table","text":"Let's see how we represent the above semi-structured \"schema\" in a Firebolt table. For the mandatory scalar fields, we will have regular columns. For the \"tags\" array we will define a column whose type will be ARRAY(TEXT) . For the user agent properties map, we will define two columns: one for the keys and one for the values - this is a pattern used to represent maps (or dictionaries) in Firebolt. We will encounter it in many examples. Combining the above will result in the following table - note that the headings specify the column name and type id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] The DDL statement creating the above table will be: CREATE [FACT|DIMENSION] TABLE visits ( id INT, StartTime DATETIME, tags ARRAY(TEXT), agent_props_keys ARRAY(TEXT), agent_props_vals ARRAY(TEXT) ) PRIMARY INDEX ... In the next sections, we will see how to query and manipulate the resultant table ( working with arrays ) and how to ingest and transform the semi-structured data into a Firebolt table ( ingesting semi-structured data ).","title":"Representation as a Firebolt table"},{"location":"working-with-semi-structured-data/ingesting-semi-structured-data/","text":"Ingesting semi-structured data There are three major approaches to ingest and handle semi-structured data: Transforming the input using JSON and ARRAY functions to fit the target schema during ingestion. Ingesting the JSON object as raw TEXT rows, and later using JSON and ARRAY functions to query and manipulate them When the input JSON adheres to a fixed schema, that is, each object has a known set of keys, and the nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. Omitted keys can be handled by specifying default values for the respective columns, but keys that are defined at table creation time will be ignored. All options can be combined depending on the use case: the nature of the input data and the queries to be performed. The 3rd approach is not very common with true semi-structured data sources, but usually is the result of an export from table-oriented storage, and therefore will be discussed in a separate section. We will continue with our sour JSON example. Assume that each JSON record is stored as plain text in the column raw_json of a (potentially external) table named source_json As a reminder here are the JSON records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Recall that the target table named \"Visits\" should look as follows: id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] Extracting top-level scalars and arrays For the top-level keys: \"id\", \"Duration\", and \"tags\" the task is straightforward using JSON_EXTRACT function. This function accepts three parameters: Although \"StartTime\" is also a scalar field, since there is no native DATETIME type in JSON type system it will require an additional step An expression containing a JSON string A JSON Pointer specifying the location in the JSON object from where the value will be extracted, A type specifier indicating Firebolt's SQL type that will be returned from the function. This type should correspond to the JSON type found under the key pointed by the JSON Pointer. For a detailed discussion of JSON to SQL type mapping see the Type Parameters section in the JSON Functions reference. Note that our native support for arrays makes the extraction of tags as simple as other scalar types. Putting those concepts in action will result in the following query that will return the expected tabular representation: SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags FROM source_json Result: id duration tags 1 450 [\"summer-sale\",\"sports\"] 2 959 [\"gadgets\",\"audio\"] Since we want to store \"StartTime\" as a DATETIME SQL typed column, which will allow many optimizations, correct ordering, and other benefits, and since JSON type system lacks such type we will have to cast the result of JSON_EXTRACT of this field: SELECT -- ... other fields CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) FROM source_json Extracting sub-object keys and values Now we have to perform a non-trivial transformation to the input data. We need to take the JSON keys of the sub-object user_agent , and their corresponding values and reshape them as two coordinated arrays. The functions JSON_EXTRACT_KEYS and JSON_EXTRACT_VALUES do exactly this. The first will return the keys under the sub-objects pointed by the JSON pointer provided as the first parameter, and the second will return the values of this sub-object as strings . That means that if under a certain key there is an arbitrarily nested sub-object - the whole object will be returned as a single TEXT element in the resulting array. Putting it all together The following statement takes the raw JSON input and transforms it into our target schema. The result is provided as an illustration, since an INSERT INTO ... return only the number of affected rows. INSERT INTO Visits SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) as StartTime, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags, JSON_EXTRACT_KEYS(raw_json,'/user_agent') as agent_props_keys, JSON_EXTRACT_VALUES(raw_json,'/user_agent') as agent_props_vals FROM doc_visits_source Result (if the script whould have been excecuted without the INSERT INTO clause): id StartTime duration tags agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Ingesting semi-structured data"},{"location":"working-with-semi-structured-data/ingesting-semi-structured-data/#ingesting-semi-structured-data","text":"There are three major approaches to ingest and handle semi-structured data: Transforming the input using JSON and ARRAY functions to fit the target schema during ingestion. Ingesting the JSON object as raw TEXT rows, and later using JSON and ARRAY functions to query and manipulate them When the input JSON adheres to a fixed schema, that is, each object has a known set of keys, and the nesting level of at most 2 (not including nesting of arrays which as stated - can be arbitrary) the data can be ingested directly. Omitted keys can be handled by specifying default values for the respective columns, but keys that are defined at table creation time will be ignored. All options can be combined depending on the use case: the nature of the input data and the queries to be performed. The 3rd approach is not very common with true semi-structured data sources, but usually is the result of an export from table-oriented storage, and therefore will be discussed in a separate section. We will continue with our sour JSON example. Assume that each JSON record is stored as plain text in the column raw_json of a (potentially external) table named source_json As a reminder here are the JSON records: // 1st record { \"id\": 1, \"StartTime\": \"2020-01-06 17:00:00\", \"Duration\": 450, \"tags\": [\"summer-sale\",\"sports\"], \"user_agent\":{ \"agent\": \"Mozilla/5.0\", \"platform\": \"Windows NT 6.1\", \"resolution\": \"1024x4069\" } } // 2nd record { \"id\": 2, \"StartTime\": \"2020-01-05 12:00:00\", \"Duration\": 959, \"tags\": [\"gadgets\",\"audio\"], \"user_agent\":{ \"agent\": \"Safari\", \"platform\": \"iOS 14\" } } Recall that the target table named \"Visits\" should look as follows: id INT StartTime DATETIME Duration INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Ingesting semi-structured data"},{"location":"working-with-semi-structured-data/ingesting-semi-structured-data/#extracting-top-level-scalars-and-arrays","text":"For the top-level keys: \"id\", \"Duration\", and \"tags\" the task is straightforward using JSON_EXTRACT function. This function accepts three parameters: Although \"StartTime\" is also a scalar field, since there is no native DATETIME type in JSON type system it will require an additional step An expression containing a JSON string A JSON Pointer specifying the location in the JSON object from where the value will be extracted, A type specifier indicating Firebolt's SQL type that will be returned from the function. This type should correspond to the JSON type found under the key pointed by the JSON Pointer. For a detailed discussion of JSON to SQL type mapping see the Type Parameters section in the JSON Functions reference. Note that our native support for arrays makes the extraction of tags as simple as other scalar types. Putting those concepts in action will result in the following query that will return the expected tabular representation: SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags FROM source_json Result: id duration tags 1 450 [\"summer-sale\",\"sports\"] 2 959 [\"gadgets\",\"audio\"] Since we want to store \"StartTime\" as a DATETIME SQL typed column, which will allow many optimizations, correct ordering, and other benefits, and since JSON type system lacks such type we will have to cast the result of JSON_EXTRACT of this field: SELECT -- ... other fields CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) FROM source_json","title":"Extracting top-level scalars and arrays"},{"location":"working-with-semi-structured-data/ingesting-semi-structured-data/#extracting-sub-object-keys-and-values","text":"Now we have to perform a non-trivial transformation to the input data. We need to take the JSON keys of the sub-object user_agent , and their corresponding values and reshape them as two coordinated arrays. The functions JSON_EXTRACT_KEYS and JSON_EXTRACT_VALUES do exactly this. The first will return the keys under the sub-objects pointed by the JSON pointer provided as the first parameter, and the second will return the values of this sub-object as strings . That means that if under a certain key there is an arbitrarily nested sub-object - the whole object will be returned as a single TEXT element in the resulting array.","title":"Extracting sub-object keys and values"},{"location":"working-with-semi-structured-data/ingesting-semi-structured-data/#putting-it-all-together","text":"The following statement takes the raw JSON input and transforms it into our target schema. The result is provided as an illustration, since an INSERT INTO ... return only the number of affected rows. INSERT INTO Visits SELECT JSON_EXTRACT(raw_json, '/id','INT') as id, CAST(JSON_EXTRACT(raw_json, '/StartTime','TEXT') AS DATETIME) as StartTime, JSON_EXTRACT(raw_json, '/Duration','INT') as duration, JSON_EXTRACT(raw_json, '/tags','ARRAY(TEXT)') as tags, JSON_EXTRACT_KEYS(raw_json,'/user_agent') as agent_props_keys, JSON_EXTRACT_VALUES(raw_json,'/user_agent') as agent_props_vals FROM doc_visits_source Result (if the script whould have been excecuted without the INSERT INTO clause): id StartTime duration tags agent_props_keys agent_props_vals 1 2020-01-06 17:00:00 450 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 2020-01-05 12:00:00 959 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Putting it all together"},{"location":"working-with-semi-structured-data/tutorial-semi-structured-data/","text":"Tutorial - Working with semi-structured data In this tutorial, we will present an end-to-end example demonstrating a typical workflow with semi-structure data. From ingestion and transformation to Firebolt's data model (ELT) to interactive analytical queries","title":"Tutorial - Working with semi-structured data"},{"location":"working-with-semi-structured-data/tutorial-semi-structured-data/#tutorial-working-with-semi-structured-data","text":"In this tutorial, we will present an end-to-end example demonstrating a typical workflow with semi-structure data. From ingestion and transformation to Firebolt's data model (ELT) to interactive analytical queries","title":"Tutorial - Working with semi-structured data"},{"location":"working-with-semi-structured-data/working-with-arrays/","text":"Working with arrays In this section, we introduce the mechanisms which allow querying and manipulating arrays in a Firebolt warehouse. For a full reference see array functions and aggregate array functions . Array types are declared using ARRAY(<type>) where <type> can be any Firebolt supported type, including array, thus, arrays can be arbitrarily nested. The innermost type (the scalar) can be nullable, however, array typed columns cannot be nullable. Array literals are also supported, so for example, the following is a valid SQL: SELECT [1,2,3,4] Throughout this page we will use the table presented in the overview page and assume its name is visits , the relevant columns are presented here for convenience. id INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"] Basic Functionality There are several self-explanatory functions to work with arrays including LENGTH , ARRAY_CONCAT , and FLATTEN . See the respective reference for a full description. Here's a short example: SELECT LENGTH(agent_prop_keys) FROM visits -- returns 3,2 SELECT ARRAY_CONCAT(agent_props_keys, agent_props_vals) FROM visits -- returns [\"agent\", \"platform\", \"resolution\", \"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] SELECT ARRAY_FLATTEN([ [[1,2,3],[4,5]], [[6]] ]) -- returns [1,2,3,4,5,2] Manipulating arrays with Lambda functions Much of the power in Firebolt's array manipulation functionality comes from incorporating Lambda functions. These are expressions passed to functions applied to arrays and in turn, are being applied element by element. The results of the Lambda function interpreted according to the array function in use. The general form of array functions taking a Lambda as an argument is ARRAY_FUNC(<lambda-expression>, arr[, arr1, arr2...]) , where the lambda expression is in the form a1[, a2, a3...] -> <lambda-body> , and arr , arr1 , arr2 ... are expressions evaluating to arrays. The variables a1, a2, ... are called the lambda arguments. The number of the lambda arguments must be equal to the number of arrays passed starting from the second argument of the array function. In addition, the lengths of all the arrays should be equal. We'll start with a simple example using a single array argument. SELECT TRANSFORM(t -> UPPER(t), tags) as up_tags FROM visits Here, the function TRANSFORM will apply the lambda body - that is convert the element to upper-case - on each of the array elements and will result in: up_tags [\"SUMMER_SALE\", \"SPORTS\"] [\"GADGETS\", \"AUDIO\"] A common use case where multiple array arguments are provided is used in the context of two arrays representing a map. The function ARRAY_FIRST returns the first element for which the lambda expression returns a result other than 0. The return value will always be taken from the first array argument provided, however, the lambda expression can compute its result based on all of the lambda arguments corresponding to elements of the array arguments. So if we want to find the value in agent_props_vals corresponding to the key \"platform\" in agent_props_keys the following query: SELECT ARRAY_FIRST(v, k -> k = 'platform', agent_props_vals, agent_props_keys) as platform FROM visits Returns the desired results: platform \"Windows NT 6.1\" \"iOS 14\" UNNEST Sometimes it is desirable to transform the nested array structure to a standard tabular format. This can be used to expose views to BI tools that cannot handle Firebolt array syntax, or the tabular format is more natural to query using standard SQL idioms. UNNEST serves these purposes. UNNEST is part of the FROM clause and it resembles a JOIN sub-clause. Given an array typed column, it unfolds the element of the array and duplicates all other columns found in the SELECT clause per each array element. For example the following query: SELECT id, tags FROM visits UNNEST(tags) Will result in: id tags 1 \"summer-sale\" 1 \"sports\" 2 \"gadgets\" 2 \"audio\"","title":"Working with arrays"},{"location":"working-with-semi-structured-data/working-with-arrays/#working-with-arrays","text":"In this section, we introduce the mechanisms which allow querying and manipulating arrays in a Firebolt warehouse. For a full reference see array functions and aggregate array functions . Array types are declared using ARRAY(<type>) where <type> can be any Firebolt supported type, including array, thus, arrays can be arbitrarily nested. The innermost type (the scalar) can be nullable, however, array typed columns cannot be nullable. Array literals are also supported, so for example, the following is a valid SQL: SELECT [1,2,3,4] Throughout this page we will use the table presented in the overview page and assume its name is visits , the relevant columns are presented here for convenience. id INT tags ARRAY(TEXT) agent_props_keys agent_props_vals 1 [\"summer-sale\",\"sports\"] [\"agent\", \"platform\", \"resolution\"] [\"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] 2 [\"gadgets\",\"audio\"] [\"agent\", \"platform\"] [\"Safari\", \"iOS 14\"]","title":"Working with arrays"},{"location":"working-with-semi-structured-data/working-with-arrays/#basic-functionality","text":"There are several self-explanatory functions to work with arrays including LENGTH , ARRAY_CONCAT , and FLATTEN . See the respective reference for a full description. Here's a short example: SELECT LENGTH(agent_prop_keys) FROM visits -- returns 3,2 SELECT ARRAY_CONCAT(agent_props_keys, agent_props_vals) FROM visits -- returns [\"agent\", \"platform\", \"resolution\", \"Mozilla/5.0\", \"Windows NT 6.1\", \"1024x4069\"] SELECT ARRAY_FLATTEN([ [[1,2,3],[4,5]], [[6]] ]) -- returns [1,2,3,4,5,2]","title":"Basic Functionality"},{"location":"working-with-semi-structured-data/working-with-arrays/#manipulating-arrays-with-lambda-functions","text":"Much of the power in Firebolt's array manipulation functionality comes from incorporating Lambda functions. These are expressions passed to functions applied to arrays and in turn, are being applied element by element. The results of the Lambda function interpreted according to the array function in use. The general form of array functions taking a Lambda as an argument is ARRAY_FUNC(<lambda-expression>, arr[, arr1, arr2...]) , where the lambda expression is in the form a1[, a2, a3...] -> <lambda-body> , and arr , arr1 , arr2 ... are expressions evaluating to arrays. The variables a1, a2, ... are called the lambda arguments. The number of the lambda arguments must be equal to the number of arrays passed starting from the second argument of the array function. In addition, the lengths of all the arrays should be equal. We'll start with a simple example using a single array argument. SELECT TRANSFORM(t -> UPPER(t), tags) as up_tags FROM visits Here, the function TRANSFORM will apply the lambda body - that is convert the element to upper-case - on each of the array elements and will result in: up_tags [\"SUMMER_SALE\", \"SPORTS\"] [\"GADGETS\", \"AUDIO\"] A common use case where multiple array arguments are provided is used in the context of two arrays representing a map. The function ARRAY_FIRST returns the first element for which the lambda expression returns a result other than 0. The return value will always be taken from the first array argument provided, however, the lambda expression can compute its result based on all of the lambda arguments corresponding to elements of the array arguments. So if we want to find the value in agent_props_vals corresponding to the key \"platform\" in agent_props_keys the following query: SELECT ARRAY_FIRST(v, k -> k = 'platform', agent_props_vals, agent_props_keys) as platform FROM visits Returns the desired results: platform \"Windows NT 6.1\" \"iOS 14\"","title":"Manipulating arrays with Lambda functions"},{"location":"working-with-semi-structured-data/working-with-arrays/#unnest","text":"Sometimes it is desirable to transform the nested array structure to a standard tabular format. This can be used to expose views to BI tools that cannot handle Firebolt array syntax, or the tabular format is more natural to query using standard SQL idioms. UNNEST serves these purposes. UNNEST is part of the FROM clause and it resembles a JOIN sub-clause. Given an array typed column, it unfolds the element of the array and duplicates all other columns found in the SELECT clause per each array element. For example the following query: SELECT id, tags FROM visits UNNEST(tags) Will result in: id tags 1 \"summer-sale\" 1 \"sports\" 2 \"gadgets\" 2 \"audio\"","title":"UNNEST"}]}